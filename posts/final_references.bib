
@article{mcdowell_clustering_2018,
	title = {Clustering gene expression time series data using an infinite {Gaussian} process mixture model},
	volume = {14},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005896},
	doi = {10.1371/journal.pcbi.1005896},
	abstract = {Transcriptome-wide time series expression profiling is used to characterize the cellular response to environmental perturbations. The first step to analyzing transcriptional response data is often to cluster genes with similar responses. Here, we present a nonparametric model-based method, Dirichlet process Gaussian process mixture model (DPGP), which jointly models data clusters with a Dirichlet process and temporal dependencies with Gaussian processes. We demonstrate the accuracy of DPGP in comparison to state-of-the-art approaches using hundreds of simulated data sets. To further test our method, we apply DPGP to published microarray data from a microbial model organism exposed to stress and to novel RNA-seq data from a human cell line exposed to the glucocorticoid dexamethasone. We validate our clusters by examining local transcription factor binding and histone modifications. Our results demonstrate that jointly modeling cluster number and temporal dependencies can reveal shared regulatory mechanisms. DPGP software is freely available online at https://github.com/PrincetonUniversity/DP\_GP\_cluster.},
	language = {en},
	number = {1},
	urldate = {2025-07-13},
	journal = {PLOS Computational Biology},
	author = {McDowell, Ian C. and Manandhar, Dinesh and Vockley, Christopher M. and Schmid, Amy K. and Reddy, Timothy E. and Engelhardt, Barbara E.},
	month = jan,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Cell cycle and cell division, Covariance, DNA transcription, Gene expression, Gene regulation, Hierarchical clustering, Histone modification, Simulation and modeling},
	pages = {e1005896},
}

@article{zhang_gaussian_2021,
	title = {Gaussian {Mixture} {Model} {Clustering} with {Incomplete} {Data}},
	volume = {17},
	issn = {1551-6857},
	url = {https://doi.org/10.1145/3408318},
	doi = {10.1145/3408318},
	abstract = {Gaussian mixture model (GMM) clustering has been extensively studied due to its effectiveness and efficiency. Though demonstrating promising performance in various applications, it cannot effectively address the absent features among data, which is not uncommon in practical applications. In this article, different from existing approaches that first impute the absence and then perform GMM clustering tasks on the imputed data, we propose to integrate the imputation and GMM clustering into a unified learning procedure. Specifically, the missing data is filled by the result of GMM clustering, and the imputed data is then taken for GMM clustering. These two steps alternatively negotiate with each other to achieve optimum. By this way, the imputed data can best serve for GMM clustering. A two-step alternative algorithm with proved convergence is carefully designed to solve the resultant optimization problem. Extensive experiments have been conducted on eight UCI benchmark datasets, and the results have validated the effectiveness of the proposed algorithm.},
	number = {1s},
	journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
	author = {Zhang, Yi and Li, Miaomiao and Wang, Siwei and Dai, Sisi and Luo, Lei and Zhu, En and Xu, Huiying and Zhu, Xinzhong and Yao, Chaoyun and Zhou, Haoran},
	month = mar,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {EM, GMM, clustering, incomplete data},
}

@article{tapak_comparative_2019,
	title = {Comparative evaluation of time series models for predicting influenza outbreaks: application of influenza-like illness data from sentinel sites of healthcare centers in {Iran}},
	volume = {12},
	issn = {1756-0500},
	url = {https://doi.org/10.1186/s13104-019-4393-y},
	doi = {10.1186/s13104-019-4393-y},
	abstract = {Forecasting the time of future outbreaks would minimize the impact of diseases by taking preventive steps including public health messaging and raising awareness of clinicians for timely treatment and diagnosis. The present study investigated the accuracy of support vector machine, artificial neural-network, and random-forest time series models in influenza like illness (ILI) modeling and outbreaks detection. The models were applied to a data set of weekly ILI frequencies in Iran. The root mean square errors (RMSE), mean absolute errors (MAE), and intra-class correlation coefficient (ICC) statistics were employed as evaluation criteria.},
	number = {1},
	journal = {BMC Research Notes},
	author = {Tapak, Leili and Hamidi, Omid and Fathian, Mohsen and Karami, Manoochehr},
	month = jun,
	year = {2019},
	pages = {353},
}

@inproceedings{chen_probabilistic_2019,
	address = {Anchorage, Alaska, USA},
	title = {Probabilistic forecasting with temporal convolutional neural network},
	doi = {https://doi.org/10.1145/1122445.1122456},
	abstract = {We present a probabilistic forecasting framework based on convolutional neural network (CNN) for multiple related time series forecasting. The framework can be applied to estimate probability density under both parametric and non-parametric settings. More specifically, stacked residual blocks based on dilated causal convolutional nets are constructed to capture the temporal dependencies of the series. Combined with representation learning, our approach is able to learn complex patterns such as seasonality, holiday effects within and across series, and to leverage those patterns for more accurate forecasts, especially when historical data is sparse or unavailable. Extensive empirical studies are performed on several real-world datasets, including datasets from JD.com, China’s largest online retailer. The results show that our framework compares favorably to the state-of-the-art in both point and probabilistic forecasting.},
	booktitle = {{MileTS} ’19: 5th {KDD} {Workshop} on {Mining} and {Learning} from {Time} {Series}},
	publisher = {ACM},
	author = {Chen, Yitian and Kang, Yanfei and Chen, Yixiong and Wang, Zizhuo},
	month = aug,
	year = {2019},
	keywords = {Convolutional neural network, Demand forecasting, Dilated causal convolution, High-dimensional time series, Probabilistic forecasting, zu lesen},
	pages = {11},
}

@inproceedings{gong_patchmixer_2024,
	title = {{PatchMixer}: {A} {Patch}-{Mixing} {Architecture} for {Long}-{Term} {Time} {Series} {Forecasting}},
	abstract = {Although the Transformer has been the dominant architecture for time series forecasting tasks in recent years, a fundamental challenge remains: the permutation-invariant self-attention mechanism within Transformers leads to a loss of temporal information. To tackle these challenges, we propose PatchMixer, a novel CNN-based model. It introduces a permutation-variant convolutional structure to preserve temporal information. Diverging from conventional CNNs in this field, which often employ multiple scales or numerous branches, our method relies exclusively on depthwise separable convolutions. This allows us to extract both local features and global correlations using a single-scale architecture. Furthermore, we employ dual forecasting heads encompassing linear and nonlinear components to better model future curve trends and details. Our experimental results on seven time-series forecasting benchmarks indicate that compared with the state-of-the-art method and the best-performing CNN, PatchMixer yields \$3.9{\textbackslash}\%\$ and \$21.2{\textbackslash}\%\$ relative improvements, respectively, while being 2-3x faster than the most advanced method.},
	booktitle = {Proceedings of the 6th {Data} {Science} {Meets} {Optimisation} {Workshop} at the {Thirty}-{Third} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Gong, Zeying and Tang, Yujin and Liang, Junwei},
	month = oct,
	year = {2024},
	note = {Workshop Paper},
	keywords = {Computer Science - Machine Learning, zu lesen},
}

@article{albahli_lstm_2025,
	title = {{LSTM} vs. {Prophet}: {Achieving} {Superior} {Accuracy} in {Dynamic} {Electricity} {Demand} {Forecasting}},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1996-1073},
	shorttitle = {{LSTM} vs. {Prophet}},
	url = {https://www.mdpi.com/1996-1073/18/2/278},
	doi = {10.3390/en18020278},
	abstract = {Accurate electricity demand forecasting is critical for improving energy efficiency, maintaining grid stability, reducing operational costs, and promoting sustainability. This study presents a novel hybrid forecasting model that integrates Long Short-Term Memory (LSTM) networks and Prophet models, leveraging their complementary strengths through a dynamic weighted ensemble methodology. The LSTM component captures nonlinear dependencies and long-term temporal patterns, while Prophet models seasonal trends and event-driven fluctuations. The hybrid model was evaluated using a comprehensive dataset of hourly electricity consumption from Ontario, Canada, achieving a Root Mean Square Error (RMSE) of 65.34, Mean Absolute Percentage Error (MAPE) of 7.3\%, and an R2 of 0.98. These results demonstrate significant improvements over standalone LSTM, Prophet, and other State-of-the-Art methods, highlighting the hybrid model’s adaptability and superior accuracy. This study underscores the practical implications of the hybrid approach, particularly in energy grid management and resource optimization, setting a new benchmark for time series forecasting in the energy sector.},
	language = {en},
	number = {2},
	urldate = {2025-05-29},
	journal = {Energies},
	author = {Albahli, Saleh},
	month = jan,
	year = {2025},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, electricity, forecasting, smart cities, smart environments, zu lesen},
	pages = {278},
}

@inproceedings{fan_dish-ts_2023,
	title = {Dish-{TS}: {A} {General} {Paradigm} for {Alleviating} {Distribution} {Shift} in {Time} {Series} {Forecasting}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	shorttitle = {Dish-{TS}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/25914},
	doi = {10.1609/aaai.v37i6.25914},
	abstract = {The distribution shift in Time Series Forecasting (TSF), indicating series distribution changes over time, largely hinders the performance of TSF models. Existing works towards distribution shift in time series are mostly limited in the quantification of distribution and, more importantly, overlook the potential shift between lookback and horizon windows. To address above challenges, we systematically summarize the distribution shift in TSF into two categories. Regarding lookback windows as input-space and horizon windows as output-space, there exist (i) intra-space shift, that the distribution within the input-space keeps shifted over time, and (ii) inter-space shift, that the distribution is shifted between input-space and output-space. Then we introduce, Dish-TS, a general neural paradigm for alleviating distribution shift in TSF. Specifically, for better distribution estimation, we propose the coefficient net (Conet), which can be any neural architectures, to map input sequences into learnable distribution coefficients. To relieve intra-space and inter-space shift, we organize Dish-TS as a Dual-Conet framework to separately learn the distribution of input- and output-space, which naturally captures the distribution difference of two spaces. In addition, we introduce a more effective training strategy for intractable Conet learning. Finally, we conduct extensive experiments on several datasets coupled with different state-of-the-art forecasting models. Experimental results show Dish-TS consistently boosts them with a more than 20\% average improvement. Code is available at https://github.com/weifantt/Dish-TS.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Fan, Wei and Wang, Pengyang and Wang, Dongkun and Wang, Dongjie and Zhou, Yuanchun and Fu, Yanjie},
	month = jun,
	year = {2023},
	keywords = {DMKM: Mining of Spatial, Temporal or Spatio-Temporal Data, zu lesen},
	pages = {7522--7529},
}

@inproceedings{feng_latent_2024,
	title = {Latent {Diffusion} {Transformer} for {Probabilistic} {Time} {Series} {Forecasting}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29085},
	doi = {10.1609/aaai.v38i11.29085},
	abstract = {The probability prediction of multivariate time series is a notoriously challenging but practical task. This research proposes to condense high-dimensional multivariate time series forecasting into a problem of latent space time series generation, to improve the expressiveness of each timestamp and make forecasting more manageable. To solve the problem that the existing work is hard to extend to high-dimensional multivariate time series, we present a latent multivariate time series diffusion framework called Latent Diffusion Transformer (LDT), which consists of a symmetric statistics-aware autoencoder and a diffusion-based conditional generator, to implement this idea. Through careful design, the time series autoencoder can compress multivariate timestamp patterns into a concise latent representation by considering dynamic statistics. Then, the diffusion-based conditional generator is able to efficiently generate realistic multivariate timestamp values on a continuous latent space under a novel self-conditioning guidance which is modeled in a non-autoregressive way. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular high-dimensional multivariate time series datasets.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Feng, Shibo and Miao, Chunyan and Zhang, Zhong and Zhao, Peilin},
	month = mar,
	year = {2024},
	keywords = {KRR: Applications, zu lesen},
	pages = {11979--11987},
}

@inproceedings{huang_hdmixer_2024,
	title = {{HDMixer}: {Hierarchical} {Dependency} with {Extendable} {Patch} for {Multivariate} {Time} {Series} {Forecasting}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	shorttitle = {{HDMixer}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29155},
	doi = {10.1609/aaai.v38i11.29155},
	abstract = {Multivariate time series (MTS) prediction has been widely adopted in various scenarios. Recently, some methods have employed patching to enhance local semantics and improve model performance. However, length-fixed patch are prone to losing temporal boundary information, such as complete peaks and periods. Moreover, existing methods mainly focus on modeling long-term dependencies across patches, while paying little attention to other dimensions (e.g., short-term dependencies within patches and complex interactions among cross-variavle patches). To address these challenges, we propose a pure MLP-based HDMixer, aiming to acquire patches with richer semantic information and efficiently modeling hierarchical interactions. Specifically, we design a Length-Extendable Patcher (LEP) tailored to MTS, which enriches the boundary information of patches and alleviates semantic incoherence in series. Subsequently, we devise a Hierarchical Dependency Explorer (HDE) based on pure MLPs. This explorer effectively models short-term dependencies within patches, long-term dependencies across patches, and complex interactions among variables. Extensive experiments on 9 real-world datasets demonstrate the superiority of our approach. The code is available at https://github.com/hqh0728/HDMixer.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Huang, Qihe and Shen, Lei and Zhang, Ruixin and Cheng, Jiahuan and Ding, Shouhong and Zhou, Zhengyang and Wang, Yang},
	month = mar,
	year = {2024},
	keywords = {ML: Classification and Regression, zu lesen},
	pages = {12608--12616},
}

@inproceedings{li_synergetic_2021,
	title = {Synergetic {Learning} of {Heterogeneous} {Temporal} {Sequences} for {Multi}-{Horizon} {Probabilistic} {Forecasting}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17023},
	doi = {10.1609/aaai.v35i10.17023},
	abstract = {Time-series is ubiquitous across applications, such as transportation, finance and healthcare. Time-series is often influenced by external factors, especially in the form of asynchronous events, making forecasting difficult. However, existing models are mainly designated for either synchronous time-series or asynchronous event sequence, and can hardly provide a synthetic way to capture the relation between them. We propose Variational Synergetic Multi-Horizon Network (VSMHN), a novel deep conditional generative model. To learn complex correlations across heterogeneous sequences, a tailored encoder is devised to combine the advances in deep point processes models and variational recurrent neural networks. In addition, an aligned time coding and an auxiliary transition scheme are carefully devised for batched training on unaligned sequences. Our model can be trained effectively using stochastic variational inference and generates probabilistic predictions with Monte-Carlo simulation. Furthermore, our model produces accurate, sharp and more realistic probabilistic forecasts. We also show that modeling asynchronous event sequences is crucial for multi-horizon time-series forecasting.},
	language = {en},
	urldate = {2024-12-24},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Longyuan and Zhang, Jihai and Yan, Junchi and Jin, Yaohui and Zhang, Yunhao and Duan, Yanjie and Tian, Guangjian},
	month = may,
	year = {2021},
	keywords = {Mining of Spatial, Temporal or Spatio-Temporal Da, zu lesen},
	pages = {8420--8428},
}

@inproceedings{nguyen_temporal_2021,
	title = {Temporal {Latent} {Auto}-{Encoder}: {A} {Method} for {Probabilistic} {Multivariate} {Time} {Series} {Forecasting}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	shorttitle = {Temporal {Latent} {Auto}-{Encoder}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17101},
	doi = {10.1609/aaai.v35i10.17101},
	abstract = {Probabilistic forecasting of high dimensional multivariate time series is a notoriously challenging task, both in terms of computational burden and distribution modeling. Most previous work either makes simple distribution assumptions or abandons modeling cross-series correlations.  A promising line of work exploits scalable matrix factorization for latent-space forecasting, but is limited to linear embeddings, unable to model distributions, and not trainable end-to-end when using deep learning forecasting. We introduce a novel temporal latent auto-encoder method which enables nonlinear factorization of multivariate time series, learned end-to-end with a temporal deep learning latent space forecast model. By imposing a probabilistic latent space model, complex distributions of the input series are modeled via the decoder.  
 Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multivariate datasets, with gains sometimes as high as 50\% for several standard metrics.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Nguyen, Nam and Quanz, Brian},
	month = may,
	year = {2021},
	keywords = {Scalability of ML Systems, zu lesen},
	pages = {9117--9125},
}

@article{tiao_advances_1994,
	title = {Some advances in non-linear and adaptive modelling in time-series},
	volume = {13},
	copyright = {Copyright © 1994 John Wiley \& Sons, Ltd.},
	issn = {1099-131X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980130206},
	doi = {10.1002/for.3980130206},
	abstract = {This paper considers some recent developments in non-linear and linear time series analysis. It consists of two main components. The first emphasizes the advances in non-linear modelling and in Bayesian inference via the Gibbs sampler. Advantages and the usefulness of these advances are illustrated by real examples. The second component is concerned with adaptive forecasting. This shows that linear models can provide accurate forecasts provided that the parameters involved are estimated adaptively. In particular, we focus on forecasting long-memory time series. Again, a real example is used to illustrate the results.},
	language = {en},
	number = {2},
	urldate = {2025-04-23},
	journal = {Journal of Forecasting},
	author = {Tiao, George C. and Tsay, Ruey S.},
	year = {1994},
	note = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.3980130206},
	keywords = {ARMA model, Adaptive forecasting, Fractional difference, Gibbs sampler, Long-memory, Outlier, Random variance-shift model, Threshold autoregressive model, zu lesen},
	pages = {109--131},
}

@inproceedings{ma_u-mixer_2024,
	title = {U-{Mixer}: {An} {Unet}-{Mixer} {Architecture} with {Stationarity} {Correction} for {Time} {Series} {Forecasting}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	shorttitle = {U-{Mixer}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29337},
	doi = {10.1609/aaai.v38i13.29337},
	abstract = {Time series forecasting is a crucial task in various domains. Caused by factors such as trends, seasonality, or irregular fluctuations, time series often exhibits non-stationary. It obstructs stable feature propagation through deep layers, disrupts feature distributions, and complicates learning data distribution changes. As a result, many existing models struggle to capture the underlying patterns, leading to degraded forecasting performance. In this study, we tackle the challenge of non-stationarity in time series forecasting with our proposed framework called U-Mixer. By combining Unet and Mixer, U-Mixer effectively captures local temporal dependencies between different patches and channels separately to avoid the influence of distribution variations among channels, and merge low- and high-levels features to obtain comprehensive data representations. The key contribution is a novel stationarity correction method, explicitly restoring data distribution by constraining the difference in stationarity between the data before and after model processing to restore the non-stationarity information, while ensuring the temporal dependencies are preserved. Through extensive experiments on various real-world time series datasets, U-Mixer demonstrates its effectiveness and robustness, and achieves 14.5\% and 7.7\% improvements over state-of-the-art (SOTA) methods.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ma, Xiang and Li, Xuemei and Fang, Lexin and Zhao, Tianlong and Zhang, Caiming},
	month = mar,
	year = {2024},
	keywords = {ML: Time-Series/Data Streams, zu lesen},
	pages = {14255--14262},
}

@inproceedings{zhou_informer_2021,
	title = {Informer: {Beyond} {Efficient} {Transformer} for {Long} {Sequence} {Time}-{Series} {Forecasting}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	shorttitle = {Informer},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17325},
	doi = {10.1609/aaai.v35i12.17325},
	abstract = {Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.},
	language = {en},
	urldate = {2025-04-16},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
	month = may,
	year = {2021},
	keywords = {Energy, Environment \& Sustainability, zu lesen},
	pages = {11106--11115},
}

@inproceedings{welling_bayesian_2011,
	address = {Madison, WI, USA},
	series = {{ICML}'11},
	title = {Bayesian learning via stochastic gradient langevin dynamics},
	isbn = {978-1-4503-0619-5},
	abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
	urldate = {2025-06-09},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Welling, Max and Teh, Yee Whye},
	month = jun,
	year = {2011},
	keywords = {zu lesen},
	pages = {681--688},
}

@inproceedings{bilos_modeling_2023,
	address = {Honolulu, Hawaii, USA},
	series = {{ICML}'23},
	title = {Modeling temporal data as continuous functions with stochastic process diffusion},
	volume = {202},
	abstract = {Temporal data such as time series can be viewed as discretized measurements of the underlying function. To build a generative model for such data we have to model the stochastic process that governs it. We propose a solution by defining the denoising diffusion model in the function space which also allows us to naturally handle irregularly-sampled observations. The forward process gradually adds noise to functions, preserving their continuity, while the learned reverse process removes the noise and returns functions as new samples. To this end, we define suitable noise sources and introduce novel denoising and score-matching models. We show how our method can be used for multivariate probabilistic forecasting and imputation, and how our model can be interpreted as a neural process.},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Biloš, Marin and Rasul, Kashif and Schneider, Anderson and Nevmyvaka, Yuriy and Günnemann, Stephan},
	month = jul,
	year = {2023},
	keywords = {zu lesen},
	pages = {2452--2470},
}

@inproceedings{graikos_diffusion_2022,
	title = {Diffusion {Models} as {Plug}-and-{Play} {Priors}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/5e6cec2a9520708381fe520246018e8b-Abstract-Conference.html},
	language = {en},
	urldate = {2025-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Graikos, Alexandros and Malkin, Nikolay and Jojic, Nebojsa and Samaras, Dimitris},
	month = dec,
	year = {2022},
	keywords = {zu lesen},
	pages = {14715--14728},
}

@inproceedings{han_softs_2024,
	title = {{SOFTS}: {Efficient} {Multivariate} {Time} {Series} {Forecasting} with {Series}-{Core} {Fusion}},
	volume = {37},
	shorttitle = {{SOFTS}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/754612bde73a8b65ad8743f1f6d8ddf6-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Han, Lu and Chen, Xu-Yang and Ye, Han-Jia and Zhan, De-Chuan},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {64145--64175},
}

@inproceedings{hu_attractor_2024,
	title = {Attractor {Memory} for {Long}-{Term} {Time} {Series} {Forecasting}: {A} {Chaos} {Perspective}},
	volume = {37},
	shorttitle = {Attractor {Memory} for {Long}-{Term} {Time} {Series} {Forecasting}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/24ef004f733548db6b3197d9f68dcb85-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hu, Jiaxi and Hu, Yuehong and Chen, Wei and Jin, Ming and Pan, Shirui and Wen, Qingsong and Liang, Yuxuan},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {20786--20818},
}

@inproceedings{huang_crossgnn_2023,
	title = {{CrossGNN}: {Confronting} {Noisy} {Multivariate} {Time} {Series} {Via} {Cross} {Interaction} {Refinement}},
	volume = {36},
	shorttitle = {{CrossGNN}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/9278abf072b58caf21d48dd670b4c721-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Huang, Qihe and Shen, Lei and Zhang, Ruixin and Ding, Shouhong and Wang, Binwu and Zhou, Zhengyang and Wang, Yang},
	month = dec,
	year = {2023},
	keywords = {zu lesen},
	pages = {46885--46902},
}

@inproceedings{jeon_gt-gan_2022,
	title = {{GT}-{GAN}: {General} {Purpose} {Time} {Series} {Synthesis} with {Generative} {Adversarial} {Networks}},
	volume = {35},
	shorttitle = {{GT}-{GAN}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/f03ce573aa8bce26f77b76f1cb9ee979-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jeon, Jinsung and Kim, Jeonghak and Song, Haryong and Cho, Seunghyeon and Park, Noseong},
	month = dec,
	year = {2022},
	keywords = {zu lesen},
	pages = {36999--37010},
}

@inproceedings{jia_witran_2023,
	title = {{WITRAN}: {Water}-wave {Information} {Transmission} and {Recurrent} {Acceleration} {Network} for {Long}-range {Time} {Series} {Forecasting}},
	volume = {36},
	shorttitle = {{WITRAN}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/2938ad0434a6506b125d8adaff084a4a-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jia, Yuxin and Lin, Youfang and Hao, Xinyan and Lin, Yan and Guo, Shengnan and Wan, Huaiyu},
	month = dec,
	year = {2023},
	keywords = {zu lesen},
	pages = {12389--12456},
}

@inproceedings{jia_pgn_2024,
	title = {{PGN}: {The} {RNN}'s {New} {Successor} is {Effective} for {Long}-{Range} {Time} {Series} {Forecasting}},
	volume = {37},
	shorttitle = {{PGN}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/990641d09f71bcee0060a8f1704ab8e2-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jia, Yuxin and Lin, Youfang and Yu, Jing and Wang, Shuo and Liu, Tianhao and Wan, Huaiyu},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {84139--84168},
}

@inproceedings{kang_introducing_2024,
	title = {Introducing {Spectral} {Attention} for {Long}-{Range} {Dependency} in {Time} {Series} {Forecasting}},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/f6adf61977467560f79b95485d1f3a79-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kang, Bong G. and Lee, Dongjun and Kim, HyunGi and Chung, DoHyun and Yoon, Sungroh},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {136509--136544},
}

@inproceedings{kim_are_2024,
	title = {Are {Self}-{Attentions} {Effective} for {Time} {Series} {Forecasting}?},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/cf66f995883298c4db2f0dcba28fb211-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kim, Dongbin and Park, Jinseong and Lee, Jaewook and Kim, Hoki},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {114180--114209},
}

@inproceedings{kim_squeezeformer_2022,
	title = {Squeezeformer: {An} {Efficient} {Transformer} for {Automatic} {Speech} {Recognition}},
	volume = {35},
	shorttitle = {Squeezeformer},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/3ccf6da39eeb8fefc8bbb1b0124adbd1-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kim, Sehoon and Gholami, Amir and Shaw, Albert and Lee, Nicholas and Mangalam, Karttikeya and Malik, Jitendra and Mahoney, Michael W. and Keutzer, Kurt},
	month = dec,
	year = {2022},
	keywords = {zu lesen},
	pages = {9361--9373},
}

@inproceedings{kollovieh_predict_2023,
	title = {Predict, {Refine}, {Synthesize}: {Self}-{Guiding} {Diffusion} {Models} for {Probabilistic} {Time} {Series} {Forecasting}},
	volume = {36},
	shorttitle = {Predict, {Refine}, {Synthesize}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/5a1a10c2c2c9b9af1514687bc24b8f3d-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kollovieh, Marcel and Ansari, Abdul Fatir and Bohlke-Schneider, Michael and Zschiegner, Jasper and Wang, Hao and Wang, Yuyang (Bernie)},
	month = dec,
	year = {2023},
	keywords = {zu lesen},
	pages = {28341--28364},
}

@inproceedings{li_diffusion-lm_2022,
	title = {Diffusion-{LM} {Improves} {Controllable} {Text} {Generation}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/1be5bc25d50895ee656b8c2d9eb89d6a-Abstract-Conference.html},
	language = {en},
	urldate = {2025-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Xiang and Thickstun, John and Gulrajani, Ishaan and Liang, Percy S. and Hashimoto, Tatsunori B.},
	month = dec,
	year = {2022},
	keywords = {zu lesen},
	pages = {4328--4343},
}

@inproceedings{li_generative_2022,
	title = {Generative {Time} {Series} {Forecasting} with {Diffusion}, {Denoise}, and {Disentanglement}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/91a85f3fb8f570e6be52b333b5ab017a-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Yan and Lu, Xinjiang and Wang, Yaqing and Dou, Dejing},
	month = dec,
	year = {2022},
	keywords = {zu lesen},
	pages = {23009--23022},
}

@inproceedings{lin_cyclenet_2024,
	title = {{CycleNet}: {Enhancing} {Time} {Series} {Forecasting} through {Modeling} {Periodic} {Patterns}},
	volume = {37},
	shorttitle = {{CycleNet}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/bfe7998398779dde03cad7a73b1f81b6-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lin, Shengsheng and Lin, Weiwei and Hu, Xinyi and Wu, Wentai and Mo, Ruichao and Zhong, Haocheng},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {106315--106345},
}

@inproceedings{liu_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Diffusion} {Models} for {Time} {Series} {Forecasting}},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/053ee34c0971568bfa5c773015c10502-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Jingwei and Yang, Ling and Li, Hongyan and Hong, Shenda},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {2766--2786},
}

@inproceedings{liu_koopa_2023,
	title = {Koopa: {Learning} {Non}-stationary {Time} {Series} {Dynamics} with {Koopman} {Predictors}},
	volume = {36},
	shorttitle = {Koopa},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/28b3dc0970fa4624a63278a4268de997-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Yong and Li, Chenyu and Wang, Jianmin and Long, Mingsheng},
	month = dec,
	year = {2023},
	keywords = {zu lesen},
	pages = {12271--12290},
}

@inproceedings{liu_autotimes_2024,
	title = {{AutoTimes}: {Autoregressive} {Time} {Series} {Forecasters} via {Large} {Language} {Models}},
	volume = {37},
	shorttitle = {{AutoTimes}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/dcf88cbc8d01ce7309b83d0ebaeb9d29-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Yong and Qin, Guo and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {122154--122184},
}

@inproceedings{liu_non-stationary_2022,
	title = {Non-stationary {Transformers}: {Exploring} the {Stationarity} in {Time} {Series} {Forecasting}},
	volume = {35},
	shorttitle = {Non-stationary {Transformers}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/4054556fcaa934b0bf76da52cf4f92cb-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Yong and Wu, Haixu and Wang, Jianmin and Long, Mingsheng},
	month = dec,
	year = {2022},
	keywords = {zu lesen},
	pages = {9881--9893},
}

@inproceedings{luo_deformabletst_2024,
	title = {{DeformableTST}: {Transformer} for {Time} {Series} {Forecasting} without {Over}-reliance on {Patching}},
	volume = {37},
	shorttitle = {{DeformableTST}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/a0b1082fc7823c4c68abcab4fa850e9c-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Luo, Donghao and Wang, Xue},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {88003--88044},
}

@inproceedings{nikitin_tsgm_2024,
	title = {{TSGM}: {A} {Flexible} {Framework} for {Generative} {Modeling} of {Synthetic} {Time} {Series}},
	volume = {37},
	shorttitle = {{TSGM}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/e92bb8c76d1aa420171f51495ce56eaf-Abstract-Datasets_and_Benchmarks_Track.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Nikitin, Alexander and Iannucci, Letizia and Kaski, Samuel},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {129042--129061},
}

@inproceedings{shang_ada-mshyper_2024,
	title = {Ada-{MSHyper}: {Adaptive} {Multi}-{Scale} {Hypergraph} {Transformer} for {Time} {Series} {Forecasting}},
	volume = {37},
	shorttitle = {Ada-{MSHyper}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/3a6935d11910d6f9142b0a1e36fc6753-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Shang, Zongjiang and Chen, Ling and Wu, Binqing and Cui, Dongliang},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {33310--33337},
}

@inproceedings{yi_frequency-domain_2023,
	title = {Frequency-domain {MLPs} are {More} {Effective} {Learners} in {Time} {Series} {Forecasting}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/f1d16af76939f476b5f040fd1398c0a3-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yi, Kun and Zhang, Qi and Fan, Wei and Wang, Shoujin and Wang, Pengyang and He, Hui and An, Ning and Lian, Defu and Cao, Longbing and Niu, Zhendong},
	month = dec,
	year = {2023},
	keywords = {zu lesen},
	pages = {76656--76679},
}

@inproceedings{zheng_multivariate_2024,
	title = {Multivariate {Probabilistic} {Time} {Series} {Forecasting} with {Correlated} {Errors}},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/619b8e3ead58dce90bc615f2a7d5d102-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zheng, Vincent Zhihao and Sun, Lijun},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {54288--54329},
}

@book{scott_multivariate_2015,
	title = {Multivariate {Density} {Estimation}: {Theory}, {Practice}, and {Visualization}},
	isbn = {978-0-471-69755-8},
	shorttitle = {Multivariate {Density} {Estimation}},
	abstract = {Clarifies modern data analysis through nonparametric density estimation for a complete working knowledge of the theory and methods Featuring a thoroughly revised presentation, Multivariate Density Estimation: Theory, Practice, and Visualization, Second Edition maintains an intuitive approach to the underlying methodology and supporting theory of density estimation. Including new material and updated research in each chapter, the Second Edition presents additional clarification of theoretical opportunities, new algorithms, and up-to-date coverage of the unique challenges presented in the field of data analysis. The new edition focuses on the various density estimation techniques and methods that can be used in the field of big data. Defining optimal nonparametric estimators, the Second Edition demonstrates the density estimation tools to use when dealing with various multivariate structures in univariate, bivariate, trivariate, and quadrivariate data analysis. Continuing to illustrate the major concepts in the context of the classical histogram, Multivariate Density Estimation: Theory, Practice, and Visualization, Second Edition also features:  Over 150 updated figures to clarify theoretical results and to show analyses of real data sets An updated presentation of graphic visualization using computer software such as R A clear discussion of selections of important research during the past decade, including mixture estimation, robust parametric modeling algorithms, and clustering More than 130 problems to help readers reinforce the main concepts and ideas presented Boxed theorems and results allowing easy identification of crucial ideas Figures in color in the digital versions of the book A website with related data sets  Multivariate Density Estimation: Theory, Practice, and Visualization, Second Edition is an ideal reference for theoretical and applied statisticians, practicing engineers, as well as readers interested in the theoretical aspects of nonparametric estimation and the application of these methods to multivariate data. The Second Edition is also useful as a textbook for introductory courses in kernel statistics, smoothing, advanced computational statistics, and general forms of statistical distributions.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Scott, David W.},
	month = mar,
	year = {2015},
	note = {ISBN: 978-0-429-13850-8},
	keywords = {Mathematics / General, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes, zu lesen},
}

@incollection{neal_mcmc_2011,
	title = {{MCMC} {Using} {Hamiltonian} {Dynamics}},
	isbn = {978-0-429-13850-8},
	abstract = {Markov chain Monte Carlo (MCMC) originated with the classic paper of Metropolis et al.
(1953), where it was used to simulate the distribution of states for a system of idealized
molecules. Not long after, another approach tomolecular simulationwas introduced (Alder
and Wainwright, 1959), in which the motion of the molecules was deterministic, following
Newton’s laws ofmotion,which have an elegant formalization asHamiltonian dynamics. For
ﬁnding the properties of bulk materials, these approaches are asymptotically equivalent,
since even in a deterministic simulation, each local region of the material experiences
effectively random inﬂuences from distant regions. Despite the large overlap in their application areas, the MCMC and molecular dynamics approaches have continued to coexist in
the following decades (see Frenkel and Smit, 1996).
In 1987, a landmark paper by Duane, Kennedy, Pendleton, and Roweth united theMCMC and molecular dynamics approaches. They called their method “hybrid Monte
Carlo,” which abbreviates to “HMC,” but the phrase “Hamiltonian Monte Carlo,” retaining the abbreviation, is more speciﬁc and descriptive, and I will use it here. Duane et al.
applied HMC not to molecular simulation, but to lattice ﬁeld theory simulations of quantum chromodynamics. Statistical applications of HMC began with my use of it for neural
network models (Neal, 1996a). I also provided a statistically-oriented tutorial on HMC in a
review of MCMC methods (Neal, 1993, Chapter 5). There have been other applications
of HMC to statistical problems (e.g. Ishwaran, 1999; Schmidt, 2009) and statisticallyoriented reviews (e.g. Liu, 2001, Chapter 9), but HMC still seems to be underappreciated
by statisticians, and perhaps also by physicists outside the lattice ﬁeld theory community.
This review begins by describing Hamiltonian dynamics. Despite terminology that maybe unfamiliar outside physics, the features of Hamiltonian dynamics that are needed for
HMC are elementary. The differential equations of Hamiltonian dynamics must be discretized for computer implementation. The “leapfrog” scheme that is typically used is
quite simple.
Following this introduction to Hamiltonian dynamics, I describe how to use it to con-struct an MCMCmethod. The ﬁrst step is to deﬁne a Hamiltonian function in terms of the
probability distribution we wish to sample from. In addition to the variables we are interested in (the “position” variables), we must introduce auxiliary “momentum” variables,
which typically have independent Gaussian distributions. The HMC method alternates
simple updates for these momentum variables with Metropolis updates in which a new
state is proposed by computing a trajectory according to Hamiltonian dynamics, implemented with the leapfrog method. A state proposed in this way can be distant from thethe 
exploration of the state space that occurs whenMetropolis updates are done using a simple
random-walkproposal distribution. (Analternativewayof avoiding randomwalks is touse
short trajectories but only partially replace the momentum variables between trajectories,
so that successive trajectories tend to move in the same direction.)
After presenting the basic HMCmethod, I discuss practical issues of tuning the leapfrogstepsize and number of leapfrog steps, as well as theoretical results on the scaling of HMC
with dimensionality. I then present a number of variations on HMC. The acceptance rate
for HMC can be increased for many problems by looking at “windows” of states at the
beginning and end of the trajectory. For many statistical problems, approximate computation of trajectories (e.g. using subsets of the data) may be beneﬁcial. Tuning of HMC can
be made easier using a “short-cut” in which trajectories computed with a bad choice of
stepsize take little computation time. Finally, “tempering” methods may be useful when
multiple isolated modes exist.},
	booktitle = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	publisher = {Chapman and Hall/CRC},
	author = {Neal, Radford M.},
	year = {2011},
	note = {ISBN: 978-0-429-13850-8},
	keywords = {zu lesen},
	pages = {50},
}

@book{koenker_quantile_2005,
	title = {Quantile {Regression}},
	isbn = {978-1-139-44471-2},
	abstract = {Quantile regression is gradually emerging as a unified statistical methodology for estimating models of conditional quantile functions. By complementing the exclusive focus of classical least squares regression on the conditional mean, quantile regression offers a systematic strategy for examining how covariates influence the location, scale and shape of the entire response distribution. This monograph is the first comprehensive treatment of the subject, encompassing models that are linear and nonlinear, parametric and nonparametric. The author has devoted more than 25 years of research to this topic. The methods in the analysis are illustrated with a variety of applications from economics, biology, ecology and finance. The treatment will find its core audiences in econometrics, statistics, and applied mathematics in addition to the disciplines cited above.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Koenker, Roger},
	month = may,
	year = {2005},
	note = {ISBN: 978-1-139-44471-2},
	keywords = {Business \& Economics / Econometrics, Business \& Economics / Statistics, Mathematics / Applied, Psychology / General, Social Science / Sociology / General, Technology \& Engineering / Engineering (General), zu lesen},
}

@book{hamilton_time_1994,
	title = {Time {Series} {Analysis}},
	isbn = {978-0-691-21863-2},
	abstract = {An authoritative, self-contained overview of time series analysis for students and researchersThe past decade has brought dramatic changes in the way that researchers analyze economic and financial time series. This textbook synthesizes these advances and makes them accessible to first-year graduate students. James Hamilton provides comprehensive treatments of important innovations such as vector autoregressions, generalized method of moments, the economic and statistical consequences of unit roots, time-varying variances, and nonlinear time series models. In addition, he presents basic tools for analyzing dynamic systems—including linear representations, autocovariance generating functions, spectral analysis, and the Kalman filter—in a way that integrates economic theory with the practical difficulties of analyzing and interpreting real-world data. Time Series Analysis fills an important need for a textbook that integrates economic theory, econometrics, and new results.This invaluable book starts from first principles and should be readily accessible to any beginning graduate student, while it is also intended to serve as a reference book for researchers.},
	language = {en},
	publisher = {Princeton University Press},
	author = {Hamilton, James D.},
	year = {1994},
	note = {ISBN: 978-0-691-21863-2},
	keywords = {Business \& Economics / Investments \& Securities / General, zu lesen},
}

@book{durbin_time_2012,
	edition = {2},
	title = {Time {Series} {Analysis} by {State} {Space} {Methods}},
	isbn = {978-0-19-964117-8},
	url = {https://doi.org/10.1093/acprof:oso/9780199641178.001.0001},
	abstract = {This book presents a comprehensive treatment of the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as being made up of distinct components such as trend, seasonal, regression elements and disturbance elements, each of which is modelled separately. The techniques that emerge from this approach are very flexible. Part I presents a full treatment of the construction and analysis of linear Gaussian state space models. The methods are based on the Kalman filter and are appropriate for a wide range of problems in practical time series analysis. The analysis can be carried out from both classical and Bayesian perspectives. Part I then presents illustrations to real series and exercises are provided for a selection of chapters. Part II discusses approximate and exact approaches for handling broad classes of non-Gaussian and nonlinear state space models. Approximate methods include the extended Kalman filter and the more recently developed unscented Kalman filter. The book shows that exact treatments become feasible when simulation-based methods such as importance sampling and particle filtering are adopted. Bayesian treatments based on simulation methods are also explored.},
	publisher = {Oxford University Press},
	author = {Durbin, James and Koopman, Siem Jan},
	month = may,
	year = {2012},
	note = {ISBN: 978-0-19-964117-8},
	keywords = {zu lesen},
}

@incollection{box_box_2013,
	address = {London},
	title = {Box and {Jenkins}: {Time} {Series} {Analysis}, {Forecasting} and {Control}},
	isbn = {978-1-137-29126-4},
	shorttitle = {Box and {Jenkins}},
	url = {https://doi.org/10.1057/9781137291264_6},
	abstract = {George Box was born in Gravesend, Kent on 18 October 1919 and, after being educated at grammar school, went to the local polytechnic to study chemistry. When the war intervened he was posted to the British Army Engineers to work as a laboratory assistant in a chemical defence experiment station investigating the effects of poison gas. His job was to carry out tests on small animals and determine the effects of gassing and subsequent treatment but, as the test results varied considerably, Box realized that statistical analysis was required and that any such analysis would have to be done by himself! Being 1942, all that he could do was to purchase some books and teach himself enough statistics to analyze the data. This he certainly did ‘beyond the call of duty’ and his work on experimental design in this area of pathology was recognized with the award of a British Empire Medal at the end of the war.},
	language = {en},
	urldate = {2025-04-16},
	booktitle = {A {Very} {British} {Affair}: {Six} {Britons} and the {Development} of {Time} {Series} {Analysis} {During} the 20th {Century}},
	publisher = {Palgrave Macmillan UK},
	author = {Box, George},
	editor = {Mills, Terence C.},
	year = {2013},
	note = {ISBN: 978-1-137-29126-4},
	keywords = {zu lesen},
	pages = {161--215},
}

@incollection{cervera_proper_1996,
	title = {Proper {Scoring} {Rules} for {Fractiles}},
	isbn = {978-0-19-852356-7},
	url = {https://doi.org/10.1093/oso/9780198523567.003.0029},
	abstract = {In this paper we consider the problem of how an expert should be paid when he/she states the information about the distribution of a random quantity in terms of fractiles, and an observation is subsequently taken. The reward, as a function of the stated fractiles and the actual observation, should encourage the expert to be honest. We introduce a family of such functions, discuss their properties and consider some extensions. The ideas are illustrated with an example about the number of delegates to Valencia-5.},
	urldate = {2025-03-18},
	booktitle = {Bayesian {Statistics} 5: {Proceedings} of the {Fifth} {Valencia} {International} {Meeting}},
	publisher = {Oxford University Press},
	author = {Cervera, José L. and Muñoz, J},
	editor = {Bernardo, J M and Berger, J O and Dawid, A P and Smith, A F M},
	month = may,
	year = {1996},
	note = {ISBN: 978-0-19-852356-7},
	keywords = {zu lesen},
}

@inproceedings{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	url = {https://openreview.net/forum?id=YicbFdNTTy},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Learning} {Representations} ({ICLR} 2021)},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2021},
	keywords = {zu lesen},
}

@book{hyndman_forecasting_2008,
	address = {Switzerland},
	title = {Forecasting with {Exponential} {Smoothing}: {The} {State} {Space} {Approach}},
	isbn = {978-3-540-71916-8},
	language = {English},
	publisher = {Springer},
	author = {Hyndman, Robin John and Koehler, Anne B and Ord, J Keith and Snyder, Ralph David},
	year = {2008},
	keywords = {zu lesen},
}

@inproceedings{zhou_sdformer_2024,
	title = {{SDformer}: {Transformer} with {Spectral} {Filter} and {Dynamic} {Attention} for {Multivariate} {Time} {Series} {Long}-term {Forecasting}},
	volume = {6},
	shorttitle = {{SDformer}},
	url = {https://www.ijcai.org/proceedings/2024/629},
	doi = {10.24963/ijcai.2024/629},
	abstract = {Electronic proceedings of IJCAI 2024},
	language = {en},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the {Thirty}-{Third} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Zhou, Ziyu and Lyu, Gengyu and Huang, Yiming and Wang, Zihao and Jia, Ziyu and Yang, Zhen},
	month = aug,
	year = {2024},
	note = {ISSN: 1045-0823},
	keywords = {zu lesen},
	pages = {5689--5697},
}

@inproceedings{zhou_ptse_2023,
	title = {{pTSE}: {A} {Multi}-model {Ensemble} {Method} for {Probabilistic} {Time} {Series} {Forecasting}},
	volume = {4},
	shorttitle = {{pTSE}},
	url = {https://www.ijcai.org/proceedings/2023/521},
	doi = {10.24963/ijcai.2023/521},
	abstract = {Electronic proceedings of IJCAI 2023},
	language = {en},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Zhou, Yunyi and Chu, Zhixuan and Ruan, Yijia and Jin, Ge and Huang, Yuchen and Li, Sheng},
	month = aug,
	year = {2023},
	note = {ISSN: 1045-0823},
	keywords = {zu lesen},
	pages = {4684--4692},
}

@misc{ziel_multivariate_2019,
	title = {Multivariate {Forecasting} {Evaluation}: {On} {Sensitive} and {Strictly} {Proper} {Scoring} {Rules}},
	shorttitle = {Multivariate {Forecasting} {Evaluation}},
	url = {http://arxiv.org/abs/1910.07325},
	doi = {10.48550/arXiv.1910.07325},
	abstract = {In recent years, probabilistic forecasting is an emerging topic, which is why there is a growing need of suitable methods for the evaluation of multivariate predictions. We analyze the sensitivity of the most common scoring rules, especially regarding quality of the forecasted dependency structures. Additionally, we propose scoring rules based on the copula, which uniquely describes the dependency structure for every probability distribution with continuous marginal distributions. Efficient estimation of the considered scoring rules and evaluation methods such as the Diebold-Mariano test are discussed. In detailed simulation studies, we compare the performance of the renowned scoring rules and the ones we propose. Besides extended synthetic studies based on recently published results we also consider a real data example. We find that the energy score, which is probably the most widely used multivariate scoring rule, performs comparably well in detecting forecast errors, also regarding dependencies. This contradicts other studies. The results also show that a proposed copula score provides very strong distinction between models with correct and incorrect dependency structure. We close with a comprehensive discussion on the proposed methodology.},
	urldate = {2025-03-20},
	publisher = {arXiv},
	author = {Ziel, Florian and Berk, Kevin},
	month = oct,
	year = {2019},
	note = {arXiv:1910.07325},
	keywords = {Economics - Econometrics, Statistics - Machine Learning, Statistics - Methodology, Statistics - Other Statistics, zu lesen},
}

@inproceedings{zhou_scat_2024,
	title = {{SCAT}: {A} {Time} {Series} {Forecasting} with {Spectral} {Central} {Alternating} {Transformers}},
	volume = {6},
	shorttitle = {{SCAT}},
	url = {https://www.ijcai.org/proceedings/2024/622},
	doi = {10.24963/ijcai.2024/622},
	abstract = {Electronic proceedings of IJCAI 2024},
	language = {en},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the {Thirty}-{Third} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Zhou, Chengjie and Che, Chao and Wang, Pengfei and Zhang, Qiang},
	month = aug,
	year = {2024},
	note = {ISSN: 1045-0823},
	keywords = {zu lesen},
	pages = {5626--5634},
}

@article{zhao_analysis_2022,
	title = {Analysis of {KNN} {Density} {Estimation}},
	volume = {68},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/9847268},
	doi = {10.1109/TIT.2022.3195870},
	abstract = {We analyze the convergence rates of k nearest neighbor density estimation method, under {\textbackslash}ell \_{\textbackslash}alpha norm with {\textbackslash}alpha ın [1,ınfty] . Our analysis includes two different cases depending on whether the support set is bounded or not. In the first case, the probability density function has a bounded support. We show that if the support set is known, then the kNN density estimator is minimax optimal under {\textbackslash}ell \_{\textbackslash}alpha with both {\textbackslash}alpha ın {\textbackslash}big[1,ınfty{\textbackslash}big) and {\textbackslash}alpha =ınfty . If the support is unknown, the kNN density estimator is still minimax optimal under {\textbackslash}ell ₁ , but is suboptimal under {\textbackslash}ell \_{\textbackslash}alpha for {\textbackslash}alpha {\textgreater}1 , and not consistent under {\textbackslash}ell \_ınfty . In the second case, the support is unbounded and the probability density function is smooth everywhere. Moreover, the Hessian is assumed to decay with the density values. For this case, our result shows that the {\textbackslash}ell \_ınfty error of kNN density estimation is nearly minimax optimal. The {\textbackslash}ell \_{\textbackslash}alpha error for the original kNN density estimator is not consistent. To address this issue, we design a new adaptive kNN estimator, which can select different k for different samples. Using this adaptive estimator, the {\textbackslash}ell \_{\textbackslash}alpha bound is minimax optimal. For comparison, we show that the popular kernel density estimator is not minimax optimal for this case.},
	number = {12},
	urldate = {2025-06-29},
	journal = {IEEE Transactions on Information Theory},
	author = {Zhao, Puning and Lai, Lifeng},
	month = dec,
	year = {2022},
	keywords = {Convergence, Density estimation, Estimation error, KNN, Kernel, Probability density function, Shape, Tail, Upper bound, functional approximation, zu lesen},
	pages = {7971--7995},
}

@inproceedings{zhou_fedformer_2022,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{FEDformer}: {Frequency} {Enhanced} {Decomposed} {Transformer} for {Long}-term {Series} {Forecasting}},
	volume = {162},
	url = {https://proceedings.mlr.press/v162/zhou22g.html},
	abstract = {Long-term time series forecasting is challenging since prediction accuracy tends to decrease dramatically with the increasing horizon. Although Transformer-based methods have significantly improved state-of-the-art results for long-term forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for long-term prediction, we exploit the fact that most time series tend to have a sparse representation in a well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer (FEDformer), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, Fedformer can reduce prediction error by 14.8\% and 22.6\% for multivariate and univariate time series, respectively. Code is publicly available at https://github.com/MAZiqing/FEDformer.},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
	editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	month = jul,
	year = {2022},
	keywords = {zu lesen},
	pages = {27268--27286},
}

@inproceedings{zhao_sustainable_2023,
	series = {{SoCC} ’23},
	title = {Sustainable {Supercomputing} for {AI}: {GPU} {Power} {Capping} at {HPC} {Scale}},
	url = {http://dx.doi.org/10.1145/3620678.3624793},
	doi = {10.1145/3620678.3624793},
	booktitle = {Proceedings of the 2023 {ACM} {Symposium} on {Cloud} {Computing}},
	publisher = {ACM},
	author = {Zhao, Dan and Samsi, Siddharth and McDonald, Joseph and Li, Baolin and Bestor, David and Jones, Michael and Tiwari, Devesh and Gadepally, Vijay},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
	pages = {588--596},
}

@misc{zhang_less_2022,
	title = {Less {Is} {More}: {Fast} {Multivariate} {Time} {Series} {Forecasting} with {Light} {Sampling}-oriented {MLP} {Structures}},
	shorttitle = {Less {Is} {More}},
	url = {http://arxiv.org/abs/2207.01186},
	doi = {10.48550/arXiv.2207.01186},
	abstract = {Multivariate time series forecasting has seen widely ranging applications in various domains, including finance, traffic, energy, and healthcare. To capture the sophisticated temporal patterns, plenty of research studies designed complex neural network architectures based on many variants of RNNs, GNNs, and Transformers. However, complex models are often computationally expensive and thus face a severe challenge in training and inference efficiency when applied to large-scale real-world datasets. In this paper, we introduce LightTS, a light deep learning architecture merely based on simple MLP-based structures. The key idea of LightTS is to apply an MLP-based structure on top of two delicate down-sampling strategies, including interval sampling and continuous sampling, inspired by a crucial fact that down-sampling time series often preserves the majority of its information. We conduct extensive experiments on eight widely used benchmark datasets. Compared with the existing state-of-the-art methods, LightTS demonstrates better performance on five of them and comparable performance on the rest. Moreover, LightTS is highly efficient. It uses less than 5\% FLOPS compared with previous SOTA methods on the largest benchmark dataset. In addition, LightTS is robust and has a much smaller variance in forecasting accuracy than previous SOTA methods in long sequence forecasting tasks.},
	urldate = {2025-04-25},
	publisher = {arXiv},
	author = {Zhang, Tianping and Zhang, Yizhuo and Cao, Wei and Bian, Jiang and Yi, Xiaohan and Zheng, Shun and Li, Jian},
	month = jul,
	year = {2022},
	note = {arXiv:2207.01186},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, zu lesen},
}

@inproceedings{zhao_convergence_2021,
	title = {On the {Convergence} {Rates} of {KNN} {Density} {Estimation}},
	url = {https://ieeexplore.ieee.org/abstract/document/9518025},
	doi = {10.1109/ISIT45174.2021.9518025},
	abstract = {We analyze the {\textbackslash}ell₁ and {\textbackslash}ell$_{\textrm{ınfty}}$ convergence rates of k nearest neighbor density estimation method. Our analysis includes two different cases depending on whether the support set is bounded or not. In the first case, the probability density function has a bounded support and is bounded away from zero. We show that kNN density estimation is minimax optimal under both {\textbackslash}ell₁ and {\textbackslash}ell$_{\textrm{ınfty}}$ criteria, if the support set is known. If the support set is unknown, then the convergence rate of {\textbackslash}ell₁ error is not affected, while {\textbackslash}ell$_{\textrm{ınfty}}$ error does not converge. In the second case, the probability density function can approach zero and is smooth everywhere. Moreover, the Hessian is assumed to decay with the density values. For this case, our result shows that the {\textbackslash}ell$_{\textrm{ınfty}}$ error of kNN density estimation is nearly minimax optimal.},
	urldate = {2025-06-29},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Information} {Theory} ({ISIT})},
	author = {Zhao, Puning and Lai, Lifeng},
	month = jul,
	year = {2021},
	keywords = {Convergence, Estimation error, Information theory, Probability density function, zu lesen},
	pages = {2840--2845},
}

@inproceedings{zhang_crossformer_2022,
	title = {Crossformer: {Transformer} {Utilizing} {Cross}-{Dimension} {Dependency} for {Multivariate} {Time} {Series} {Forecasting}},
	shorttitle = {Crossformer},
	url = {https://openreview.net/forum?id=vSVLM2j9eie},
	abstract = {Recently many deep models have been proposed for multivariate time series (MTS) forecasting. In particular, Transformer-based models have shown great potential because they can capture long-term dependency. However, existing Transformer-based models mainly focus on modeling the temporal dependency (cross-time dependency) yet often omit the dependency among different variables (cross-dimension dependency), which is critical for MTS forecasting. To fill the gap, we propose Crossformer, a Transformer-based model utilizing cross-dimension dependency for MTS forecasting. In Crossformer, the input MTS is embedded into a 2D vector array through the Dimension-Segment-Wise (DSW) embedding to preserve time and dimension information. Then the Two-Stage Attention (TSA) layer is proposed to efficiently capture the cross-time and cross-dimension dependency. Utilizing DSW embedding and TSA layer, Crossformer establishes a Hierarchical Encoder-Decoder (HED) to use the information at different scales for the final forecasting. Extensive experimental results on six real-world datasets show the effectiveness of Crossformer against previous state-of-the-arts.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Zhang, Yunhao and Yan, Junchi},
	month = sep,
	year = {2022},
	keywords = {zu lesen},
}

@inproceedings{zhang_probts_2024,
	title = {{ProbTS}: {Benchmarking} {Point} and {Distributional} {Forecasting} across {Diverse} {Prediction} {Horizons}},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/55f2a27b1ac39dbfdd0fc83742dc87d7-Paper-Datasets_and_Benchmarks_Track.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Jiawen and Wen, Xumeng and Zhang, Zhenwei and Zheng, Shun and Li, Jia and Bian, Jiang},
	editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
	year = {2024},
	keywords = {zu lesen},
	pages = {48045--48082},
}

@inproceedings{yuan_diverse_2019,
	title = {Diverse {Trajectory} {Forecasting} with {Determinantal} {Point} {Processes}},
	url = {https://openreview.net/forum?id=ryxnY3NYPS},
	abstract = {The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.},
	language = {en},
	urldate = {2025-06-18},
	booktitle = {Proceedings of the {Eighth} {International} {Conference} on {Learning} {Representations}},
	author = {Yuan, Ye and Kitani, Kris M.},
	month = sep,
	year = {2019},
	keywords = {zu lesen},
}

@article{zhang_forecasting_1998,
	title = {Forecasting with artificial neural networks:: {The} state of the art},
	volume = {14},
	issn = {0169-2070},
	shorttitle = {Forecasting with artificial neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207097000447},
	doi = {10.1016/S0169-2070(97)00044-7},
	abstract = {Interest in using artificial neural networks (ANNs) for forecasting has led to a tremendous surge in research activities in the past decade. While ANNs provide a great deal of promise, they also embody much uncertainty. Researchers to date are still not certain about the effect of key factors on forecasting performance of ANNs. This paper presents a state-of-the-art survey of ANN applications in forecasting. Our purpose is to provide (1) a synthesis of published research in this area, (2) insights on ANN modeling issues, and (3) the future research directions.},
	number = {1},
	urldate = {2025-05-28},
	journal = {International Journal of Forecasting},
	author = {Zhang, Guoqiang and Eddy Patuwo, B. and Y. Hu, Michael},
	month = mar,
	year = {1998},
	keywords = {Forecasting, Neural networks, zu lesen},
	pages = {35--62},
}

@inproceedings{zeng_are_2023,
	title = {Are {Transformers} {Effective} for {Time} {Series} {Forecasting}?},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26317},
	doi = {10.1609/aaai.v37i9.26317},
	abstract = {Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. 
To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
	month = jun,
	year = {2023},
	keywords = {ML: Time-Series/Data Streams, zu lesen},
	pages = {11121--11128},
}

@inproceedings{yoon_time-series_2019,
	title = {Time-series {Generative} {Adversarial} {Networks}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html},
	abstract = {A good generative model for time-series data should preserve temporal dynamics, in the sense that new sequences respect the original relationships between variables across time. Existing methods that bring generative adversarial networks (GANs) into the sequential setting do not adequately attend to the temporal correlations unique to time-series data. At the same time, supervised models for sequence prediction - which allow finer control over network dynamics - are inherently deterministic. We propose a novel framework for generating realistic time-series data that combines the flexibility of the unsupervised paradigm with the control afforded by supervised training. Through a learned embedding space jointly optimized with both supervised and adversarial objectives, we encourage the network to adhere to the dynamics of the training data during sampling. Empirically, we evaluate the ability of our method to generate realistic samples using a variety of real and synthetic time-series datasets. Qualitatively and quantitatively, we find that the proposed framework consistently and significantly outperforms state-of-the-art benchmarks with respect to measures of similarity and predictive ability.},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yoon, Jinsung and Jarrett, Daniel and van der Schaar, Mihaela},
	year = {2019},
	keywords = {zu lesen},
}

@article{yegin_generative_2024,
	title = {Generative diffusion models: {A} survey of current theoretical developments},
	volume = {608},
	issn = {0925-2312},
	shorttitle = {Generative diffusion models},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231224011445},
	doi = {10.1016/j.neucom.2024.128373},
	abstract = {Generative diffusion models showed high success in many fields with a powerful theoretical background. They convert the data distribution to noise and remove the noise back to obtain a similar distribution. Many existing reviews focused on the specific application areas without concentrating on the developments about the algorithm. Unlike them we investigated the theoretical developments of the generative diffusion models. These approaches mainly divide into two: training-based and training-free. Awakening to this allowed us a clear and understandable categorization for the researchers who will make new developments in the future.},
	urldate = {2025-06-11},
	journal = {Neurocomputing},
	author = {Yeğin, Melike Nur and Amasyalı, Mehmet Fatih},
	month = dec,
	year = {2024},
	keywords = {Denoising diffusion probabilistic models, Generative diffusion models, Image generation, Noise-conditional score networks, Score-based models, zu lesen},
	pages = {128373},
}

@misc{yang_survey_2024,
	title = {A {Survey} on {Diffusion} {Models} for {Time} {Series} and {Spatio}-{Temporal} {Data}},
	url = {http://arxiv.org/abs/2404.18886},
	doi = {10.48550/arXiv.2404.18886},
	abstract = {The study of time series is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain. In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series and spatio-temporal data separately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data. Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Yang, Yiyuan and Jin, Ming and Wen, Haomin and Zhang, Chaoli and Liang, Yuxuan and Ma, Lintao and Wang, Yi and Liu, Chenghao and Yang, Bin and Xu, Zenglin and Bian, Jiang and Pan, Shirui and Wen, Qingsong},
	month = jun,
	year = {2024},
	note = {arXiv:2404.18886},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, zu lesen},
}

@inproceedings{yan_probabilistic_2024,
	title = {Probabilistic {Time} {Series} {Modeling} with {Decomposable} {Denoising} {Diffusion} {Model}},
	url = {https://proceedings.mlr.press/v235/yan24b.html},
	abstract = {Probabilistic time series modeling based on generative models has attracted lots of attention because of its wide applications and excellent performance. However, existing state-of-the-art models, based on stochastic differential equation, not only struggle to determine the drift and diffusion coefficients during the design process but also have slow generation speed. To tackle this challenge, we firstly propose decomposable denoising diffusion model (D3MD3M{\textbackslash}text\{D\}{\textasciicircum}3{\textbackslash}text\{M\}) and prove it is a general framework unifying denoising diffusion models and continuous flow models. Based on the new framework, we propose some simple but efficient probability paths with high generation speed. Furthermore, we design a module that combines a special state space model with linear gated attention modules for sequence modeling. It preserves inductive bias and simultaneously models both local and global dependencies. Experimental results on 8 real-world datasets show that D3MD3M{\textbackslash}text\{D\}{\textasciicircum}3{\textbackslash}text\{M\} reduces RMSE and CRPS by up to 4.6\% and 4.3\% compared with state-of-the-arts on imputation tasks, and achieves comparable results with state-of-the-arts on forecasting tasks with only 10 steps.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yan, Tijin and Gong, Hengheng and Yongping, He and Zhan, Yufeng and Xia, Yuanqing},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {55759--55777},
}

@inproceedings{xu_fits_2023,
	title = {{FITS}: {Modeling} {Time} {Series} with \$10k\$ {Parameters}},
	shorttitle = {{FITS}},
	url = {https://openreview.net/forum?id=bWcnvZ3qMb},
	abstract = {In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain, achieving performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks. Notably, FITS accomplishes this with a svelte profile of just about \$10k\$ parameters, making it ideally suited for edge devices and paving the way for a wide range of applications. The code is available for review at: {\textbackslash}url\{https://anonymous.4open.science/r/FITS\}.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Xu, Zhijian and Zeng, Ailing and Xu, Qiang},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
}

@article{xie_overview_2023,
	title = {An overview of deterministic and probabilistic forecasting methods of wind energy},
	volume = {26},
	issn = {2589-0042},
	url = {https://www.cell.com/iscience/abstract/S2589-0042(22)02077-6},
	doi = {10.1016/j.isci.2022.105804},
	language = {English},
	number = {1},
	urldate = {2024-12-22},
	journal = {iScience},
	author = {Xie, Yuying and Li, Chaoshun and Li, Mengying and Liu, Fangjie and Taukenova, Meruyert},
	month = jan,
	year = {2023},
	keywords = {Energy modeling, energy engineering, energy systems, zu lesen},
}

@inproceedings{wu_deep_2018,
	address = {Ann Arbor MI USA},
	title = {Deep {Learning} for {Epidemiological} {Predictions}},
	isbn = {978-1-4503-5657-2},
	url = {https://dl.acm.org/doi/10.1145/3209978.3210077},
	doi = {10.1145/3209978.3210077},
	abstract = {Predicting new and urgent trends in epidemiological data is an important problem for public health, and has attracted increasing attention in the data mining and machine learning communities. The temporal nature of epidemiology data and the need for real-time prediction by the system makes the problem residing in the category of time-series forecasting or prediction. While traditional autoregressive (AR) methods and Gaussian Process Regression (GPR) have been actively studied for solving this problem, deep learning techniques have not been explored in this domain. In this paper, we develop a deep learning framework, for the first time, to predict epidemiology profiles in the time-series perspective. We adopt Recurrent Neural Networks (RNNs) to capture the long-term correlation in the data and Convolutional Neural Networks (CNNs) to fuse information from data of different sources. A residual structure is also applied to prevent overfitting issues in the training process. We compared our model with the most widely used AR models on USA and Japan datasets. Our approach provides consistently better results than these baseline methods.},
	language = {en},
	urldate = {2023-06-01},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Wu, Yuexin and Yang, Yiming and Nishiura, Hiroshi and Saitoh, Masaya},
	month = jun,
	year = {2018},
	keywords = {zu lesen},
	pages = {1085--1088},
}

@inproceedings{wu_dynamic_2013,
	title = {Dynamic {Covariance} {Models} for {Multivariate} {Financial} {Time} {Series}},
	url = {https://proceedings.mlr.press/v28/wu13.html},
	abstract = {The accurate prediction of time-changing covariances is an important problem in the modeling of multivariate financial data. However, some of the most popular models suffer from a) overfitting problems and multiple local optima, b) failure to capture shifts in market conditions and c) large computational costs. To address these problems we introduce a novel dynamic model for time-changing covariances. Over-fitting and local optima are avoided by following a Bayesian approach instead of computing point estimates. Changes in market conditions are captured by assuming a diffusion process in parameter values, and finally computationally efficient and scalable inference is performed using particle filters. Experiments with financial data show excellent performance of the proposed method with respect to current standard models.},
	language = {en},
	urldate = {2025-06-30},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wu, Yue and Hernandez-Lobato, Jose Miguel and Zoubin, Ghahramani},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	keywords = {zu lesen},
	pages = {558--566},
}

@article{wu_high-dimensional_2020,
	title = {High-dimensional covariance matrix estimation using a low-rank and diagonal decomposition},
	volume = {48},
	copyright = {© 2019 Statistical Society of Canada / Société statistique du Canada},
	issn = {1708-945X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cjs.11532},
	doi = {10.1002/cjs.11532},
	abstract = {We study high-dimensional covariance/precision matrix estimation under the assumption that the covariance/precision matrix can be decomposed into a low-rank component L and a diagonal component D. The rank of L can either be chosen to be small or controlled by a penalty function. Under moderate conditions on the population covariance/precision matrix itself and on the penalty function, we prove some consistency results for our estimators. A block-wise coordinate descent algorithm, which iteratively updates L and D, is then proposed to obtain the estimator in practice. Finally, various numerical experiments are presented; using simulated data, we show that our estimator performs quite well in terms of the Kullback–Leibler loss; using stock return data, we show that our method can be applied to obtain enhanced solutions to the Markowitz portfolio selection problem. The Canadian Journal of Statistics 48: 308–337; 2020 © 2019 Statistical Society of Canada},
	language = {en},
	number = {2},
	urldate = {2025-06-13},
	journal = {Canadian Journal of Statistics},
	author = {Wu, Yilei and Qin, Yingli and Zhu, Mu},
	year = {2020},
	note = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cjs.11532},
	keywords = {Akaike information criterion, Kullback–Leibler loss, Markowitz portfolio selection, consistency, coordinate descent, eigen-decomposition, log-determinant semi-definite programming, zu lesen},
	pages = {308--337},
}

@inproceedings{wu_autoformer_2021,
	title = {Autoformer: {Decomposition} {Transformers} with {Auto}-{Correlation} for {Long}-{Term} {Series} {Forecasting}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	keywords = {zu lesen},
	pages = {22419--22430},
}

@inproceedings{wu_adversarial_2020,
	title = {Adversarial {Sparse} {Transformer} for {Time} {Series} {Forecasting}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/c6b8c8d762da15fa8dbbdfb6baf9e260-Abstract.html},
	abstract = {Many approaches have been proposed for time series forecasting, in light of its significance in wide applications including business demand prediction. 
However, the existing methods suffer from two key limitations. Firstly, most point prediction models only predict an exact value of each time step without flexibility, which can hardly capture the stochasticity of data. Even probabilistic prediction using the likelihood estimation suffers these problems in the same way. Besides, most of them use the auto-regressive generative mode, where ground-truth is provided during training and replaced by the network’s own one-step ahead output during inference, causing the error accumulation in inference. Thus they may fail to forecast time series for long time horizon due to the error accumulation. To solve these issues, in this paper, we propose a new time series forecasting model -- Adversarial Sparse Transformer (AST), based on Generated Adversarial Networks (GANs). Specifically, AST adopts a Sparse Transformer as the generator to learn a sparse attention map for time series forecasting, and uses a discriminator to improve the prediction performance from sequence level. Extensive experiments on several real-world datasets show the effectiveness and efficiency of our method.},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Sifan and Xiao, Xi and Ding, Qianggang and Zhao, Peilin and Wei, Ying and Huang, Junzhou},
	year = {2020},
	keywords = {zu lesen},
	pages = {17105--17115},
}

@article{winkler_decision-theoretic_1972,
	title = {A {Decision}-{Theoretic} {Approach} to {Interval} {Estimation}},
	volume = {67},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1972.10481224},
	doi = {10.1080/01621459.1972.10481224},
	abstract = {Under an appropriate loss function, interval estimation may be regarded as a Bayesian decision-making procedure in which the objective is to find an interval that minimizes expected loss. For various loss functions, the behavior of the optimal interval is investigated, a comparison is made with the usual non-decision-theoretic interval estimates, and applications and examples are discussed.},
	number = {337},
	urldate = {2025-04-01},
	journal = {Journal of the American Statistical Association},
	author = {Winkler, Robert L.},
	month = mar,
	year = {1972},
	note = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1972.10481224},
	keywords = {zu lesen},
	pages = {187--191},
}

@article{winkler_scoring_1996,
	title = {Scoring rules and the evaluation of probabilities},
	volume = {5},
	copyright = {http://www.springer.com/tdm},
	issn = {1133-0686, 1863-8260},
	url = {http://link.springer.com/10.1007/BF02562681},
	doi = {10.1007/BF02562681},
	abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for "good" probabilities. This paper reviews scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of "goodness" of probabilities, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems.},
	language = {en},
	number = {1},
	urldate = {2025-03-18},
	journal = {Test},
	author = {Winkler, R. L. and Muñoz, Javier and Cervera, José L. and Bernardo, José M. and Blattenberger, Gail and Kadane, Joseph B. and Lindley, Dennis V. and Murphy, Allan H. and Oliver, Robert M and Ríos-Insua, David},
	month = jun,
	year = {1996},
	keywords = {zu lesen},
	pages = {1--60},
}

@inproceedings{wu_timesnet_2022,
	title = {{TimesNet}: {Temporal} {2D}-{Variation} {Modeling} for {General} {Time} {Series} {Analysis}},
	shorttitle = {{TimesNet}},
	url = {https://openreview.net/forum?id=ju_Uqw384Oq},
	abstract = {Time series analysis is of immense importance in extensive applications, such as weather forecasting, anomaly detection, and action recognition. This paper focuses on temporal variation modeling, which is the common key problem of extensive analysis tasks. Previous methods attempt to accomplish this directly from the 1D time series, which is extremely challenging due to the intricate temporal patterns. Based on the observation of multi-periodicity in time series, we ravel out the complex temporal variations into the multiple intraperiod- and interperiod-variations. To tackle the limitations of 1D time series in representation capability, we extend the analysis of temporal variations into the 2D space by transforming the 1D time series into a set of 2D tensors based on multiple periods. This transformation can embed the intraperiod- and interperiod-variations into the columns and rows of the 2D tensors respectively, making the 2D-variations to be easily modeled by 2D kernels. Technically, we propose the TimesNet with TimesBlock as a task-general backbone for time series analysis. TimesBlock can discover the multi-periodicity adaptively and extract the complex temporal variations from transformed 2D tensors by a parameter-efficient inception block. Our proposed TimesNet achieves consistent state-of-the-art in five mainstream time series analysis tasks, including short- and long-term forecasting, imputation, classification, and anomaly detection. Code is available at this repository: https://github.com/thuml/TimesNet.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
	month = sep,
	year = {2022},
	keywords = {zu lesen},
}

@article{winkler_good_1968,
	title = {“{Good}” {Probability} {Assessors}},
	volume = {7},
	issn = {1520-0450},
	url = {https://journals.ametsoc.org/view/journals/apme/7/5/1520-0450_1968_007_0751_pa_2_0_co_2.xml},
	doi = {10.1175/1520-0450(1968)007<0751:PA>2.0.CO;2},
	abstract = {Since a meteorologist's predictions are subjective, a framework for the evaluation of meteorological probability assessors must be consistent with the theory of subjective probability. Such a framework is described in this paper. First, two standards of “goodness,” one based upon normative considerations and one based upon substantive considerations, are proposed. Specific properties which a meteorologist's assessments should possess are identified for each standard. Then, several measures of “goodness,” or scoring rules, which indicate the extent to which such assessments possess certain properties, are described. Finally, several important uses of these scoring rules are considered.},
	language = {EN},
	number = {5},
	urldate = {2025-03-20},
	journal = {Journal of Applied Meteorology and Climatology},
	author = {Winkler, Robert L. and Murphy, Allan H.},
	month = oct,
	year = {1968},
	note = {Publisher: American Meteorological Society
Section: Journal of Applied Meteorology and Climatology},
	keywords = {zu lesen},
	pages = {751--758},
}

@article{wilk_probability_1968,
	title = {Probability {Plotting} {Methods} for the {Analysis} of {Data}},
	volume = {55},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2334448},
	doi = {10.2307/2334448},
	abstract = {This paper describes and discusses graphical techniques, based on the primitive empirical cumulative distribution function and on quantile (Q - Q) plots, percent (P - P) plots and hybrids of these, which are useful in assessing a one-dimensional sample, either from original data or resulting from analysis. Areas of application include: the comparison of samples; the comparison of distributions; the presentation of results on sensitivities of statistical methods; the analysis of collections of contrasts and of collections of sample variances; the assessment of multivariate contrasts; and the structuring of analysis of variance mean squares. Many of the objectives and techniques are illustrated by examples.},
	number = {1},
	urldate = {2025-06-16},
	journal = {Biometrika},
	author = {Wilk, M. B. and Gnanadesikan, R.},
	year = {1968},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	keywords = {zu lesen},
	pages = {1--17},
}

@inproceedings{wen_multi-horizon_2018,
	address = {Long Beach, California},
	title = {A {Multi}-{Horizon} {Quantile} {Recurrent} {Forecaster}},
	abstract = {We propose a framework for general probabilistic multi-step time series regression. Specifically, we exploit the expressiveness and temporal nature of Sequence-to-Sequence Neural Networks (e.g. recurrent and convolutional structures), the nonparametric nature of Quantile Regression and the efficiency of Direct Multi-Horizon Forecasting. A new training scheme, *forking-sequences*, is designed for sequential nets to boost stability and performance. We show that the approach accommodates both temporal and static covariates, learning across multiple related series, shifting seasonality, future planned event spikes and cold-starts in real life large-scale forecasting. The performance of the framework is demonstrated in an application to predict the future demand of items sold on Amazon.com, and in a public probabilistic forecasting competition to predict electricity price and load.},
	booktitle = {Time {Series} {Workshop} at 31st {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wen, Ruofeng and Torkkola, Kari and Narayanaswamy, Balakrishnan and Madeka, Dhruv},
	year = {2018},
	note = {Workshop Paper},
	keywords = {Statistics - Machine Learning, zu lesen},
}

@inproceedings{wilson_copula_2010,
	title = {Copula {Processes}},
	volume = {23},
	url = {https://papers.nips.cc/paper_files/paper/2010/hash/fc8001f834f6a5f0561080d134d53d29-Abstract.html},
	abstract = {We define a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions. As an example, we develop a stochastic volatility model, Gaussian Copula Process Volatility (GCPV), to predict the latent standard deviations of a sequence of random variables. To make predictions we use Bayesian inference, with the Laplace approximation, and with Markov chain Monte Carlo as an alternative. We find our model can outperform GARCH on simulated and financial data. And unlike GARCH, GCPV can easily handle missing data, incorporate covariates other than time, and model a rich class of covariance structures.},
	urldate = {2025-06-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wilson, Andrew G and Ghahramani, Zoubin},
	year = {2010},
	keywords = {zu lesen},
}

@article{williams_learning_1989,
	title = {A learning algorithm for continually running fully recurrent neural networks},
	volume = {1},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1989.1.2.270},
	doi = {10.1162/neco.1989.1.2.270},
	abstract = {The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.},
	number = {2},
	urldate = {2025-05-23},
	journal = {Neural Comput.},
	author = {Williams, Ronald J. and Zipser, David},
	month = jun,
	year = {1989},
	keywords = {zu lesen},
	pages = {270--280},
}

@inproceedings{wen_deep_2019,
	address = {Long Beach, California},
	title = {Deep {Generative} {Quantile}-{Copula} {Models} for {Probabilistic} {Forecasting}},
	abstract = {We introduce a new category of multivariate conditional generative models and demonstrate its performance and versatility in probabilistic time series forecasting and simulation. Specifically, the output of quantile regression networks is expanded from a set of fixed quantiles to the whole Quantile Function by a univariate mapping from a latent uniform distribution to the target distribution. Then the multivariate case is solved by learning such quantile functions for each dimension's marginal distribution, followed by estimating a conditional Copula to associate these latent uniform random variables. The quantile functions and copula, together defining the joint predictive distribution, can be parameterized by a single implicit generative Deep Neural Network.},
	booktitle = {Proceedings of the {Time} {Series} {Workshop} at 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wen, Ruofeng and Torkkola, Kari},
	month = jul,
	year = {2019},
	note = {Workshop Paper},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, zu lesen},
}

@inproceedings{wen_transformers_2023,
	title = {Transformers in {Time} {Series}: {A} {Survey}},
	volume = {6},
	shorttitle = {Transformers in {Time} {Series}},
	url = {https://www.ijcai.org/proceedings/2023/759},
	doi = {10.24963/ijcai.2023/759},
	abstract = {Electronic proceedings of IJCAI 2023},
	language = {en},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
	month = aug,
	year = {2023},
	note = {ISSN: 1045-0823},
	keywords = {zu lesen},
	pages = {6778--6786},
}

@article{weiss_multi-step_1991,
	title = {Multi-step estimation and forecasting in dynamic models},
	volume = {48},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/030440769190035C},
	doi = {10.1016/0304-4076(91)90035-C},
	abstract = {We consider the situation in which a researcher uses a misspecified model for forecasting. If the interest is in multi-step forecasting and the accuracy of the forecasts is measured through the sum of squared multi-step forecast errors, then in large samples it is preferable to minimize corresponding sum of squared in-sample multi-step forecast errors. We derive the asymptotic properties of the resulting estimator. To asses the behavior of the estimator in small samples, we perform a Monte Carlo experiment. In most of the cases considered, OLS outperforms the estimator. We conjecture that this occurs because the estimator is still defined on the basis of squared errors.},
	number = {1},
	urldate = {2025-04-23},
	journal = {Journal of Econometrics},
	author = {Weiss, Andrew A.},
	month = apr,
	year = {1991},
	keywords = {zu lesen},
	pages = {135--149},
}

@misc{wang_easytorch_2020,
	title = {{EasyTorch}: {Simple} and powerful pytorch framework.},
	url = {https://github.com/cnstark/easytorch},
	author = {Wang, Yuhao},
	year = {2020},
	note = {URL: https://github.com/cnstark/easytorch},
	keywords = {zu lesen},
}

@inproceedings{wang_card_2023,
	title = {{CARD}: {Channel} {Aligned} {Robust} {Blend} {Transformer} for {Time} {Series} {Forecasting}},
	shorttitle = {{CARD}},
	url = {https://openreview.net/forum?id=MJksrOhurE},
	abstract = {Recent studies have demonstrated the great power of Transformer models for time series forecasting. One of the key elements that lead to the transformer's success is the channel-independent (CI) strategy to improve the training robustness. However, the ignorance of the correlation among different channels in CI would limit the model's forecasting capacity. In this work, we design a special Transformer, i.e., **C**hannel **A**ligned **R**obust Blen**d** Transformer (CARD for short), that addresses key shortcomings of CI type Transformer in time series forecasting. First, CARD introduces a channel-aligned attention structure that allows it to capture both temporal correlations among signals and dynamical dependence among multiple variables over time. Second, in order to efficiently utilize the multi-scale knowledge, we design a token blend module to generate tokens with different resolutions. Third, we introduce a robust loss function for time series forecasting to alleviate the potential overfitting issue. This new loss function weights the importance of forecasting over a finite horizon based on prediction uncertainties. Our evaluation of multiple long-term and short-term forecasting datasets demonstrates that CARD significantly outperforms state-of-the-art time series forecasting methods. The code is available at the following repository: https://github.com/wxie9/CARD.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Wang, Xue and Zhou, Tian and Wen, Qingsong and Gao, Jinyang and Ding, Bolin and Jin, Rong},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
}

@inproceedings{wang_timemixer_2023,
	title = {{TimeMixer}: {Decomposable} {Multiscale} {Mixing} for {Time} {Series} {Forecasting}},
	shorttitle = {{TimeMixer}},
	url = {https://openreview.net/forum?id=7oLshfEIC2},
	abstract = {Time series forecasting is widely used in extensive applications, such as traffic planning and weather forecasting. However, real-world time series usually present intricate temporal variations, making forecasting extremely challenging. Going beyond the mainstream paradigms of plain decomposition and multiperiodicity analysis, we analyze temporal variations in a novel view of multiscale-mixing, where time series present distinct patterns in different sampling scales. Specifically, the microscopic and the macroscopic information are reflected in fine and coarse scales, respectively, and thereby complex variations are inherently disentangled. Based on this observation, we propose TimeMixer as a fully MLP-based architecture with Past-Decomposable-Mixing (PDM) and Future-Multipredictor-Mixing (FMM) blocks to take full advantage of disentangled multiscale series in both past extraction and future prediction phases. Concretely, PDM applies the decomposition to multiscale series and further mixes the decomposed seasonal and trend components in fine-to-coarse and coarse-to-fine directions separately, which successively aggregates the microscopic seasonal and macroscopic trend information. FMM further ensembles multiple predictors to utilize complementary forecasting capabilities in multiscale observations. Consequently, our proposed TimeMixer is able to achieve consistent state-of-the-art performances in both long-term and short-term forecasting tasks with favorable run-time efficiency.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Wang, Shiyu and Wu, Haixu and Shi, Xiaoming and Hu, Tengge and Luo, Huakun and Ma, Lintao and Zhang, James Y. and Zhou, Jun},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
}

@inproceedings{wang_generative_2024,
	title = {Generative {Probabilistic} {Time} {Series} {Forecasting} and {Applications} in {Grid} {Operations}},
	doi = {10.1109/CISS59072.2024.10480214},
	booktitle = {2024 58th {Annual} {Conference} on {Information} {Sciences} and {Systems} ({CISS})},
	author = {Wang, Xinyi and Tong, Lang and Zhao, Qing},
	year = {2024},
	keywords = {Decision making, Electricity, Probabilistic time series forecasting, Recurrent neural networks, Technological innovation, Time series analysis, Training, Uncertainty, autoencoder, generative adversarial networks, innovation representation, zu lesen},
	pages = {1--6},
}

@article{wang_gaussian_2013,
	title = {From {Gaussian} kernel density estimation to kernel methods},
	volume = {4},
	issn = {1868-808X},
	url = {https://doi.org/10.1007/s13042-012-0078-8},
	doi = {10.1007/s13042-012-0078-8},
	abstract = {This paper explores how a kind of probabilistic systems, namely, Gaussian kernel density estimation (GKDE), can be used to interpret several classical kernel methods, including the well-known support vector machine (SVM), support vector regression (SVR), one-class kernel classifier, i.e., support vector data description (SVDD) or equivalently minimal enclosing ball (MEB), and the fuzzy systems (FS). For the SVM, we reveal that the classical SVM with Gaussian density kernel attempts to find a noisy GKDE based Bayesian classifier with equal prior probabilities for each class. For the SVR, the classification based ε-SVR attempts to obtain two noisy GKDEs for each class in the constructed binary classification dataset, and the decision boundary just corresponds to the mapping function of the original regression problem. For the MEB or SVDD, we reveal the equivalence between it and the integrated-squared-errors (ISE) criterion based GKDE and by using this equivalence a MEB based classifier with privacy-preserving function is proposed for one kind of classification tasks where the datasets contain privacy-preserving clouds. For the FS, we show that the GKDE for a regression dataset is equivalent to the construction of a zero-order Takagi–Sugeno–Kang (TSK) fuzzy system based on the same dataset. Our extensive experiments confirm the obtained conclusions and demonstrated the effectiveness of the proposed new machine learning and modeling methods.},
	language = {en},
	number = {2},
	urldate = {2025-06-26},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Wang, Shitong and Deng, Zhaohong and Chung, Fu-lai and Hu, Wenjun},
	month = apr,
	year = {2013},
	keywords = {Bayesian classifier, Categorization, Functional clustering, Fuzzy system, Gaussian kernel density function, Kernel methods, Knowledge Based Systems, Machine Learning, Minimal enclosing ball, Privacy-preserving classifier, Statistical Learning, Stochastic Calculus, Support vector machine, zu lesen},
	pages = {119--137},
}

@article{wang_forecasting_2018,
	title = {Forecasting energy demand in {China} and {India}: {Using} single-linear, hybrid-linear, and non-linear time series forecast techniques},
	volume = {161},
	issn = {0360-5442},
	shorttitle = {Forecasting energy demand in {China} and {India}},
	url = {https://www.sciencedirect.com/science/article/pii/S0360544218314658},
	doi = {10.1016/j.energy.2018.07.168},
	abstract = {Better forecasting energy demand in China and India can help those countries meet future challenges caused by the changes in that demand, as well as inform future global energy needs. In this study, the single-linear, hybrid-linear, and non-linear forecasting techniques based on grey theory are developed to more accurately forecasting energy demand in China and India. These prosed techniques were applied to simulate China’s and India’s energy consumption of China and India between 1990 and 2016. Three standards (trend map, error measure, and fit method) of analyzing quality of forecast technique are used to quantify the quality of these proposed technique. The results show these proposed techniques have a very high degree of fit, a low error rate, and high fitting precision. For example, the mean absolute percent error of single-linear, hybrid-linear, and non-linear techniques are 1.30–3.08\%, 0.80–2.57\%, and 2.06–2.19\%, respectively. The results of optimality analysis show these proposed models can produce reliable forecasting results in China and India, which might be used to forecasting energy demand in other countries/regions. Our forecasting results show the annual growth rate of India’s energy demand from 2017 to 2026 will be 4.49\%–5.21\% (single-linear), 2.42\%–7.04\% (hybrid-linear), 0.58\%–4.02\% (non-linear), respectively. The annual growth rate of China’s energy demand from 2017 to 2026 will be 1.36\%–1.70\% (single-linear), 1.04\%–1.49\% (hybrid-linear), 1.80\%–2.34\% (non-linear), respectively. The growth rate of India’s energy consumption is expected to be 2–4 times that of China from 2017 to 2026, indicating India will become even more important in the global energy market.},
	urldate = {2024-04-27},
	journal = {Energy},
	author = {Wang, Qiang and Li, Shuyu and Li, Rongrong},
	month = oct,
	year = {2018},
	keywords = {China and India, Energy security, Forecasting, Hybrid linear and nonlinear model, zu lesen},
	pages = {821--831},
}

@misc{wang_combination_2018,
	title = {Combination of {Hyperband} and {Bayesian} {Optimization} for {Hyperparameter} {Optimization} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1801.01596},
	doi = {10.48550/arXiv.1801.01596},
	abstract = {Deep learning has achieved impressive results on many problems. However, it requires high degree of expertise or a lot of experience to tune well the hyperparameters, and such manual tuning process is likely to be biased. Moreover, it is not practical to try out as many different hyperparameter configurations in deep learning as in other machine learning scenarios, because evaluating each single hyperparameter configuration in deep learning would mean training a deep neural network, which usually takes quite long time. Hyperband algorithm achieves state-of-the-art performance on various hyperparameter optimization problems in the field of deep learning. However, Hyperband algorithm does not utilize history information of previous explored hyperparameter configurations, thus the solution found is suboptimal. We propose to combine Hyperband algorithm with Bayesian optimization (which does not ignore history when sampling next trial configuration). Experimental results show that our combination approach is superior to other hyperparameter optimization approaches including Hyperband algorithm.},
	urldate = {2025-07-04},
	publisher = {arXiv},
	author = {Wang, Jiazhuo and Xu, Jason and Wang, Xuejun},
	month = jan,
	year = {2018},
	note = {arXiv:1801.01596},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, zu lesen},
}

@inproceedings{wang_micn_2022,
	title = {{MICN}: {Multi}-scale {Local} and {Global} {Context} {Modeling} for {Long}-term {Series} {Forecasting}},
	shorttitle = {{MICN}},
	url = {https://openreview.net/forum?id=zt53IDUR1U},
	abstract = {Recently, Transformer-based methods have achieved surprising performance in the field of long-term series forecasting, but the attention mechanism for computing global correlations entails high complexity. And they do not allow for targeted modeling of local features as CNN structures do. To solve the above problems, we propose to combine local features and global correlations to capture the overall view of time series (e.g., fluctuations, trends). To fully exploit the underlying information in the time series, a multi-scale branch structure is adopted to model different potential patterns separately. Each pattern is extracted with down-sampled convolution and isometric convolution for local features and global correlations, respectively. In addition to being more effective, our proposed method, termed as Multi-scale Isometric Convolution Network (MICN), is more efficient with linear complexity about the sequence length with suitable convolution kernels. Our experiments on six benchmark datasets show that compared with state-of-the-art methods, MICN yields 17.2\% and 21.6\% relative improvements for multivariate and univariate time series, respectively.},
	language = {en},
	urldate = {2025-04-25},
	booktitle = {Proceedings of the {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Wang, Huiqiang and Peng, Jian and Huang, Feihu and Wang, Jince and Chen, Junhui and Xiao, Yifei},
	month = sep,
	year = {2022},
	keywords = {zu lesen},
}

@inproceedings{wang_scheduled_2024,
	title = {Scheduled {Sampling} for {Recursive} {Multi}-{Step} {GPU} {Temperature} {Forecasting}},
	doi = {10.1109/ICMLA61862.2024.00147},
	booktitle = {2024 {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Wang, Harold and Jiang, Xunfei and Ebrahimi, Mahdi},
	year = {2024},
	keywords = {Costs, Data centers, Forecasting, Graphics processing units, Load modeling, Machine Learning, Predictive models, Scheduled Sampling, Temperature Prediction, Temperature distribution, Thermal loading, Thermal management, Training, zu lesen},
	pages = {991--996},
}

@article{wan_direct_2017,
	title = {Direct {Quantile} {Regression} for {Nonparametric} {Probabilistic} {Forecasting} of {Wind} {Power} {Generation}},
	volume = {32},
	doi = {10.1109/TPWRS.2016.2625101},
	number = {4},
	journal = {IEEE Transactions on Power Systems},
	author = {Wan, Can and Lin, Jin and Wang, Jianhui and Song, Yonghua and Dong, Zhao Yang},
	year = {2017},
	keywords = {Extreme learning machine, Forecasting, Predictive models, Probabilistic logic, Reliability, Uncertainty, Wind forecasting, Wind power generation, linear programming, probabilistic forecasting, quantile regression, uncertainty, wind power, zu lesen},
	pages = {2767--2778},
}

@inproceedings{van_den_oord_neural_2017,
	title = {Neural {Discrete} {Representation} {Learning}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of posterior collapse'' -— where the latents are ignored when they are paired with a powerful autoregressive decoder -— typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2025-06-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
	year = {2017},
	keywords = {zu lesen},
}

@misc{van_den_oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	doi = {10.48550/arXiv.1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2025-04-17},
	publisher = {arXiv},
	author = {Van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv:1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, zu lesen},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2024-03-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {zu lesen},
}

@book{van_rossum_python_1995,
	title = {Python tutorial},
	volume = {620},
	publisher = {Centrum voor Wiskunde en Informatica Amsterdam, The Netherlands},
	author = {Van Rossum, Guido and Drake Jr, Fred L},
	year = {1995},
	keywords = {zu lesen},
}

@article{vaghefi_modeling_2014,
	title = {Modeling and forecasting of cooling and electricity load demand},
	volume = {136},
	issn = {0306-2619},
	url = {https://www.sciencedirect.com/science/article/pii/S030626191400943X},
	doi = {https://doi.org/10.1016/j.apenergy.2014.09.004},
	abstract = {The objective of this paper is to extend a statistical approach to effectively provide look-ahead forecasts for cooling and electricity demand load. Our proposed model is a generalized form of a Cochrane–Orcutt estimation technique that combines a multiple linear regression model and a seasonal autoregressive moving average model. The proposed model is adaptive so that it updates forecast values every time that new information on cooling and electricity load is received. Therefore, the model can simultaneously take advantage of two statistical methods, time series, and linear regression in an adaptive way. The effectiveness of the proposed forecast model is shown through a use case. The example utilizes the proposed approach for economic dispatching of a combined cooling, heating and power (CCHP) plant at the University of California, Irvine. The results reveal the effectiveness of the proposed forecast model.},
	journal = {Applied Energy},
	author = {Vaghefi, A. and Jafari, M. A. and Bisse, Emmanuel and Lu, Y. and Brouwer, J.},
	year = {2014},
	keywords = {Cochrane–Orcutt estimation, Heat recovery steam generator, Load forecasting, Thermal energy storage, Time series-regression model, zu lesen},
	pages = {186--196},
}

@inproceedings{uria_rnade_2013,
	title = {{RNADE}: {The} real-valued neural autoregressive density-estimator},
	volume = {26},
	shorttitle = {{RNADE}},
	url = {https://proceedings.neurips.cc/paper/2013/hash/53adaf494dc89ef7196d73636eb2451b-Abstract.html},
	abstract = {We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradient-based optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.},
	urldate = {2025-06-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
	year = {2013},
	keywords = {zu lesen},
}

@inproceedings{vahdat_nvae_2020,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	volume = {33},
	shorttitle = {{NVAE}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/e3b21256183cf7c2c7a66be163579d37-Abstract.html},
	abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256x256 pixels. The source code is publicly available.},
	urldate = {2025-06-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vahdat, Arash and Kautz, Jan},
	year = {2020},
	keywords = {zu lesen},
	pages = {19667--19679},
}

@article{tyralis_review_2024,
	title = {A review of predictive uncertainty estimation with machine learning},
	volume = {57},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-023-10698-8},
	doi = {10.1007/s10462-023-10698-8},
	abstract = {Predictions and forecasts of machine learning models should take the form of probability distributions, aiming to increase the quantity of information communicated to end users. Although applications of probabilistic prediction and forecasting with machine learning models in academia and industry are becoming more frequent, related concepts and methods have not been formalized and structured under a holistic view of the entire field. Here, we review the topic of predictive uncertainty estimation with machine learning algorithms, as well as the related metrics (consistent scoring functions and proper scoring rules) for assessing probabilistic predictions. The review covers a time period spanning from the introduction of early statistical (linear regression and time series models, based on Bayesian statistics or quantile regression) to recent machine learning algorithms (including generalized additive models for location, scale and shape, random forests, boosting and deep learning algorithms) that are more flexible by nature. The review of the progress in the field, expedites our understanding on how to develop new algorithms tailored to users’ needs, since the latest advancements are based on some fundamental concepts applied to more complex algorithms. We conclude by classifying the material and discussing challenges that are becoming a hot topic of research.},
	language = {en},
	number = {4},
	urldate = {2025-03-18},
	journal = {Artificial Intelligence Review},
	author = {Tyralis, Hristos and Papacharalampous, Georgia},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Boosting, Deep learning, Distributional regression, Ensemble learning, Machine learning, Probabilistic forecasting, Quantile regression, Random forests, zu lesen},
	pages = {94},
}

@article{touvron_resmlp_2023,
	title = {{ResMLP}: {Feedforward} {Networks} for {Image} {Classification} {With} {Data}-{Efficient} {Training}},
	volume = {45},
	issn = {1939-3539},
	shorttitle = {{ResMLP}},
	doi = {10.1109/TPAMI.2022.3206148},
	abstract = {We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.},
	language = {eng},
	number = {4},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Touvron, Hugo and Bojanowski, Piotr and Caron, Mathilde and Cord, Matthieu and El-Nouby, Alaaeldin and Grave, Edouard and Izacard, Gautier and Joulin, Armand and Synnaeve, Gabriel and Verbeek, Jakob and Jegou, Herve},
	month = apr,
	year = {2023},
	pmid = {36094972},
	keywords = {zu lesen},
	pages = {5314--5321},
}

@article{tipping_probabilistic_1999,
	title = {Probabilistic {Principal} {Component} {Analysis}},
	volume = {61},
	issn = {1369-7412},
	url = {https://doi.org/10.1111/1467-9868.00196},
	doi = {10.1111/1467-9868.00196},
	abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
	number = {3},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Tipping, Michael E. and Bishop, Christopher M.},
	year = {1999},
	note = {https://academic.oup.com/jrsssb/article-pdf/61/3/611/49589634/jrsssb\_61\_3\_611.pdf},
	keywords = {zu lesen},
	pages = {611--622},
}

@inproceedings{theis_note_2016,
	title = {A note on the evaluation of generative models},
	url = {http://www.iclr.cc/doku.php?id=iclr2016:main},
	urldate = {2025-06-05},
	booktitle = {Proceedings of the {Fourth} {International} {Conference} on {Learning} {Representations}},
	author = {Theis, L. and van den Oord, A. and Bethge, M.},
	month = may,
	year = {2016},
	keywords = {zu lesen},
	pages = {1--10},
}

@article{taylor_forecasting_2018,
	title = {Forecasting at {Scale}},
	volume = {72},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2017.1380080},
	doi = {10.1080/00031305.2017.1380080},
	abstract = {Forecasting is a common data science task that helps organizations with capacity planning, goal setting, and anomaly detection. Despite its importance, there are serious challenges associated with producing reliable and high-quality forecasts—especially when there are a variety of time series and analysts with expertise in time series modeling are relatively rare. To address these challenges, we describe a practical approach to forecasting “at scale” that combines configurable models with analyst-in-the-loop performance analysis. We propose a modular regression model with interpretable parameters that can be intuitively adjusted by analysts with domain knowledge about the time series. We describe performance analyses to compare and evaluate forecasting procedures, and automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable, practical forecasting of business time series.},
	number = {1},
	urldate = {2024-05-24},
	journal = {The American Statistician},
	author = {Taylor, Sean J. and Letham, Benjamin},
	month = jan,
	year = {2018},
	keywords = {Nonlinear regression, Statistical practice, Time series, zu lesen},
	pages = {37--45},
}

@inproceedings{tong_probabilistic_2022,
	title = {Probabilistic {Decomposition} {Transformer} for {Time} {Series} {Forecasting}},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977653.ch54},
	abstract = {Abstract Time series forecasting is crucial for many fields, such as disaster warning, weather prediction, and energy consumption. The Transformer-based models are considered to have revolutionized the field of time series. However, the autoregressive form of the Transformer introduces cumulative errors in the inference stage. Furthermore, the complex temporal pattern of the time series leads to an increased difficulty for the models in mining reliable temporal dependencies. In this paper, we propose the Probabilistic Decomposition Transformer model, which provides a flexible framework for hierarchical and decomposable forecasts. The hierarchical mechanism utilizes the forecasting results of Transformer as conditional information for the generative model, performing sequence-level forecasts to approximate the ground truth, which can mitigate the cumulative error of the autoregressive Transformer. In addition, the conditional generative model encodes historical and predictive information into the latent space and reconstructs typical patterns from the latent space, such as seasonality and trend terms. The process provides a flexible framework for the separation of complex patterns through the interaction of information in the latent space. Extensive experiments on several datasets demonstrate the effectiveness and robustness of the model, indicating that it compares favorably with the state-of-the-art.},
	booktitle = {Proceedings of the 2023 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	author = {Tong, Junlong and Xie, Liping and Zhang, Kanjian},
	year = {2022},
	note = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch54},
	keywords = {zu lesen},
	pages = {478--486},
}

@inproceedings{tolstikhin_mlp-mixer_2021,
	title = {{MLP}-{Mixer}: {An} all-{MLP} {Architecture} for {Vision}},
	volume = {34},
	shorttitle = {{MLP}-{Mixer}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html},
	abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
	urldate = {2025-05-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
	year = {2021},
	keywords = {zu lesen},
	pages = {24261--24272},
}

@article{thomson_eliciting_1979,
	title = {Eliciting production possibilities from a well-informed manager},
	volume = {20},
	issn = {0022-0531},
	url = {https://www.sciencedirect.com/science/article/pii/0022053179900425},
	doi = {https://doi.org/10.1016/0022-0531(79)90042-5},
	number = {3},
	journal = {Journal of Economic Theory},
	author = {Thomson, William},
	year = {1979},
	keywords = {zu lesen},
	pages = {360--380},
}

@article{tavenard_tslearn_2020,
	title = {Tslearn, {A} {Machine} {Learning} {Toolkit} for {Time} {Series} {Data}},
	volume = {21},
	url = {http://jmlr.org/papers/v21/20-091.html},
	number = {118},
	journal = {Journal of Machine Learning Research},
	author = {Tavenard, Romain and Faouzi, Johann and Vandewiele, Gilles and Divo, Felix and Androz, Guillaume and Holtz, Chester and Payne, Marie and Yurchak, Roman and Rußwurm, Marc and Kolar, Kushal and Woods, Eli},
	year = {2020},
	keywords = {zu lesen},
	pages = {1--6},
}

@inproceedings{tang_probabilistic_2021,
	title = {Probabilistic {Transformer} {For} {Time} {Series} {Analysis}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/c68bd9055776bf38d8fc43c0ed283678-Abstract.html},
	abstract = {Generative modeling of multivariate time series has remained challenging partly due to the complex, non-deterministic dynamics across long-distance timesteps. In this paper, we propose deep probabilistic methods that combine state-space models (SSMs) with transformer architectures. In contrast to previously proposed SSMs, our approaches use attention mechanism to model non-Markovian dynamics in the latent space and avoid recurrent neural networks entirely. We also extend our models to include several layers of stochastic variables organized in a hierarchy for further expressiveness. Compared to transformer models, ours are probabilistic, non-autoregressive, and capable of generating diverse long-term forecasts with uncertainty estimates. Extensive experiments show that our models consistently outperform competitive baselines on various tasks and datasets, including time series forecasting and human motion prediction.},
	urldate = {2024-12-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tang, Binh and Matteson, David S},
	year = {2021},
	keywords = {zu lesen},
	pages = {23592--23608},
}

@article{taieb_recursive_2012,
	title = {Recursive and direct multi-step forecasting: the best of both worlds},
	abstract = {We propose a new forecasting strategy, called rectify, that seeks to combine the best properties of both the recursive and direct forecasting strategies. The rationale behind the rectify strategy is to begin with biased recursive forecasts and adjust them so they are unbiased and have smaller error. We use linear and nonlinear simulated time series to investigate the performance of the rectify strategy and compare the results with those from the recursive and the direct strategies. We also carry out some experiments using real world time series from the M3 and the NN5 forecasting competitions. We ﬁnd that the rectify strategy is always better than, or at least has comparable performance to, the best of the recursive and the direct strategies. This ﬁnding makes the rectify strategy very attractive as it avoids making a choice between the recursive and the direct strategies which can be a diﬃcult task in real-world applications.},
	language = {en},
	author = {Taieb, Souhaib Ben and Hyndman, Rob J},
	year = {2012},
	keywords = {zu lesen},
}

@inproceedings{tashiro_csdi_2021,
	title = {{CSDI}: {Conditional} {Score}-based {Diffusion} {Models} for {Probabilistic} {Time} {Series} {Imputation}},
	volume = {34},
	shorttitle = {{CSDI}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/cfe8504bda37b575c70ee1a8276f3486-Abstract.html},
	abstract = {The imputation of missing values in time series has many applications in healthcare and finance. While autoregressive models are natural candidates for time series imputation, score-based diffusion models have recently outperformed existing counterparts including autoregressive models in many tasks such as image generation and audio synthesis, and would be promising for time series imputation. In this paper, we propose Conditional Score-based Diffusion model (CSDI), a novel time series imputation method that utilizes score-based diffusion models conditioned on observed data. Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. On healthcare and environmental data, CSDI improves by 40-65\% over existing probabilistic imputation methods on popular performance metrics. In addition, deterministic imputation by CSDI reduces the error by 5-20\% compared to the state-of-the-art deterministic imputation methods. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines. The code is available at https://github.com/ermongroup/CSDI.},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tashiro, Yusuke and Song, Jiaming and Song, Yang and Ermon, Stefano},
	year = {2021},
	keywords = {zu lesen},
	pages = {24804--24816},
}

@inproceedings{tarima_use_2020,
	title = {Use of {Uncertain} {Additional} {Information} in {Newsvendor} {Models}},
	url = {https://ieeexplore.ieee.org/abstract/document/9314751},
	doi = {10.1109/GOL49479.2020.9314751},
	abstract = {The newsvendor problem is a popular inventory management problem in supply chain management and logistics. Solutions to the newsvendor problem determine optimal inventory levels. This model is typically fully determined by a purchase and sale prices and a distribution of random market demand. From a statistical point of view, this problem is often considered as a quantile estimation of a critical fractile which maximizes anticipated profit. The distribution of demand is a random variable and is often estimated on historic data. In an ideal situation, when the probability distribution of demand is known, one can determine the quantile of a critical fractile minimizing a particular loss function. When a parametric family is known, maximum likelihood estimation is asymptotically efficient under certain regularity assumptions and the maximum likelihood estimators (MLEs) are used for estimating quantiles. Then, the Cramer-Rao lower bound determines the lowest possible asymptotic variance for the MLEs. Can one find a quantile estimator with a smaller variance then the Cramer-Rao lower bound? If a relevant additional information is available then the answer is yes. This manuscript considers minimum variance and mean squared error estimation which incorporate additional information for estimating optimal inventory levels.},
	urldate = {2025-06-19},
	booktitle = {2020 5th {International} {Conference} on {Logistics} {Operations} {Management} ({GOL})},
	author = {Tarima, Sergey and Zenkova, Zhanna},
	month = oct,
	year = {2020},
	keywords = {Bayes methods, Correlation, Gaussian distribution, History, Maximum likelihood estimation, Newsvendor model, Standards, Uncertainty, additional estimation, minimum mean squared error, minimum variance, quantile estimation, zu lesen},
	pages = {1--6},
}

@article{taieb_bias_2016,
	title = {A {Bias} and {Variance} {Analysis} for {Multistep}-{Ahead} {Time} {Series} {Forecasting}},
	volume = {27},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/7064712},
	doi = {10.1109/TNNLS.2015.2411629},
	abstract = {Multistep-ahead forecasts can either be produced recursively by iterating a one-step-ahead time series model or directly by estimating a separate model for each forecast horizon. In addition, there are other strategies; some of them combine aspects of both aforementioned concepts. In this paper, we present a comprehensive investigation into the bias and variance behavior of multistep-ahead forecasting strategies. We provide a detailed review of the different multistep-ahead strategies. Subsequently, we perform a theoretical study that derives the bias and variance for a number of forecasting strategies. Finally, we conduct a Monte Carlo experimental study that compares and evaluates the bias and variance performance of the different strategies. From the theoretical and the simulation studies, we analyze the effect of different factors, such as the forecast horizon and the time series length, on the bias and variance components, and on the different multistep-ahead strategies. Several lessons are learned, and recommendations are given concerning the advantages, disadvantages, and best conditions of use of each strategy.},
	number = {1},
	urldate = {2025-04-23},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Taieb, Souhaib Ben and Atiya, Amir F.},
	month = jan,
	year = {2016},
	keywords = {Analytical models, Artificial neural networks, Bias, Computational modeling, Forecasting, Joints, Monte Carlo simulation, Predictive models, Time series analysis, machine learning, multistep-ahead forecasting, nearest neighbors, neural networks (NNs), time series, variance, variance., zu lesen},
	pages = {62--76},
}

@article{taieb_review_2012,
	title = {A review and comparison of strategies for multi-step ahead time series forecasting based on the {NN5} forecasting competition},
	volume = {39},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417412000528},
	doi = {10.1016/j.eswa.2012.01.039},
	abstract = {Multi-step ahead forecasting is still an open challenge in time series forecasting. Several approaches that deal with this complex problem have been proposed in the literature but an extensive comparison on a large number of tasks is still missing. This paper aims to fill this gap by reviewing existing strategies for multi-step ahead forecasting and comparing them in theoretical and practical terms. To attain such an objective, we performed a large scale comparison of these different strategies using a large experimental benchmark (namely the 111 series from the NN5 forecasting competition). In addition, we considered the effects of deseasonalization, input variable selection, and forecast combination on these strategies and on multi-step ahead forecasting at large. The following three findings appear to be consistently supported by the experimental results: Multiple-Output strategies are the best performing approaches, deseasonalization leads to uniformly improved forecast accuracy, and input selection is more effective when performed in conjunction with deseasonalization.},
	number = {8},
	urldate = {2025-06-12},
	journal = {Expert Systems with Applications},
	author = {Taieb, Souhaib Ben and Bontempi, Gianluca and Atiya, Amir F. and Sorjamaa, Antti},
	month = jun,
	year = {2012},
	keywords = {Friedman test, Lazy Learning, Long-term forecasting, Machine learning, Multi-step ahead forecasting, NN5 forecasting competition, Strategies of forecasting, Time series forecasting, zu lesen},
	pages = {7067--7083},
}

@article{tabak_family_2013,
	title = {A {Family} of {Nonparametric} {Density} {Estimation} {Algorithms}},
	volume = {66},
	copyright = {Copyright © 2012 Wiley Periodicals, Inc.},
	issn = {1097-0312},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423},
	doi = {10.1002/cpa.21423},
	abstract = {A new methodology for density estimation is proposed. The methodology, which builds on the one developed by Tabak and Vanden-Eijnden, normalizes the data points through the composition of simple maps. The parameters of each map are determined through the maximization of a local quadratic approximation to the log-likelihood. Various candidates for the elementary maps of each step are proposed; criteria for choosing one includes robustness, computational simplicity, and good behavior in high-dimensional settings. A good choice is that of localized radial expansions, which depend on a single parameter: all the complexity of arbitrary, possibly convoluted probability densities can be built through the composition of such simple maps. © 2012 Wiley Periodicals, Inc.},
	language = {en},
	number = {2},
	urldate = {2025-06-05},
	journal = {Communications on Pure and Applied Mathematics},
	author = {Tabak, E. G. and Turner, Cristina V.},
	year = {2013},
	note = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21423},
	keywords = {zu lesen},
	pages = {145--164},
}

@inproceedings{szegedy_going_2015,
	title = {Going deeper with convolutions},
	url = {https://ieeexplore.ieee.org/document/7298594},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	urldate = {2025-04-25},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Computer architecture, Computer vision, Convolutional codes, Neural networks, Object detection, Sparse matrices, Visualization, zu lesen},
	pages = {1--9},
}

@inproceedings{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	volume = {27},
	url = {https://papers.nips.cc/paper_files/paper/2014/hash/5a18e133cbf9f257297f410bb7eca942-Abstract.html},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2025-04-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	year = {2014},
	keywords = {zu lesen},
}

@inproceedings{sun_memory_2022,
	title = {Memory {Augmented} {State} {Space} {Model} for {Time} {Series} {Forecasting}},
	volume = {4},
	url = {https://www.ijcai.org/proceedings/2022/479},
	doi = {10.24963/ijcai.2022/479},
	abstract = {Electronic proceedings of IJCAI 2022},
	language = {en},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Sun, Yinbo and Ma, Lintao and Liu, Yu and Wang, Shijun and Zhang, James and Zheng, YangFei and Yun, Hu and Lei, Lei and Kang, Yulin and Ye, Llinbao},
	month = jul,
	year = {2022},
	note = {ISSN: 1045-0823},
	keywords = {zu lesen},
	pages = {3451--3457},
}

@misc{sun_fredo_2022,
	title = {{FreDo}: {Frequency} {Domain}-based {Long}-{Term} {Time} {Series} {Forecasting}},
	shorttitle = {{FreDo}},
	url = {http://arxiv.org/abs/2205.12301},
	doi = {10.48550/arXiv.2205.12301},
	abstract = {The ability to forecast far into the future is highly beneficial to many applications, including but not limited to climatology, energy consumption, and logistics. However, due to noise or measurement error, it is questionable how far into the future one can reasonably predict. In this paper, we first mathematically show that due to error accumulation, sophisticated models might not outperform baseline models for long-term forecasting. To demonstrate, we show that a non-parametric baseline model based on periodicity can actually achieve comparable performance to a state-of-the-art Transformer-based model on various datasets. We further propose FreDo, a frequency domain-based neural network model that is built on top of the baseline model to enhance its performance and which greatly outperforms the state-of-the-art model. Finally, we validate that the frequency domain is indeed better by comparing univariate models trained in the frequency v.s. time domain.},
	urldate = {2025-07-04},
	publisher = {arXiv},
	author = {Sun, Fan-Keng and Boning, Duane S.},
	month = may,
	year = {2022},
	note = {arXiv:2205.12301},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, zu lesen},
}

@article{sprangers_parameter-efficient_2023,
	title = {Parameter-efficient deep probabilistic forecasting},
	volume = {39},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207021001850},
	doi = {10.1016/j.ijforecast.2021.11.011},
	abstract = {Probabilistic time series forecasting is crucial in many application domains, such as retail, ecommerce, finance, and biology. With the increasing availability of large volumes of data, a number of neural architectures have been proposed for this problem. In particular, Transformer-based methods achieve state-of-the-art performance on real-world benchmarks. However, these methods require a large number of parameters to be learned, which imposes high memory requirements on the computational resources for training such models. To address this problem, we introduce a novel bidirectional temporal convolutional network that requires an order of magnitude fewer parameters than a common Transformer-based approach. Our model combines two temporal convolutional networks: the first network encodes future covariates of the time series, whereas the second network encodes past observations and covariates. We jointly estimate the parameters of an output distribution via these two networks. Experiments on four real-world datasets show that our method performs on par with four state-of-the-art probabilistic forecasting methods, including a Transformer-based approach and WaveNet, on two point metrics (sMAPE and NRMSE) as well as on a set of range metrics (quantile loss percentiles) in the majority of cases. We also demonstrate that our method requires significantly fewer parameters than Transformer-based methods, which means that the model can be trained faster with significantly lower memory requirements, which as a consequence reduces the infrastructure cost for deploying these models.},
	number = {1},
	urldate = {2025-06-02},
	journal = {International Journal of Forecasting},
	author = {Sprangers, Olivier and Schelter, Sebastian and de Rijke, Maarten},
	month = jan,
	year = {2023},
	keywords = {Efficiency in forecasting methods, Large-scale forecasting Forecasting with neural networks, Probabilistic forecasting, Temporal convolutional network, zu lesen},
	pages = {332--345},
}

@inproceedings{song_maximum_2021,
	title = {Maximum {Likelihood} {Training} of {Score}-{Based} {Diffusion} {Models}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/0a9fdbb17feb6ccb7ec405cfb85222c4-Abstract.html},
	urldate = {2025-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
	year = {2021},
	keywords = {zu lesen},
	pages = {1415--1428},
}

@inproceedings{sohn_learning_2015,
	title = {Learning {Structured} {Output} {Representation} using {Deep} {Conditional} {Generative} {Models}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html},
	abstract = {Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision. Although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations. In this work, we develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows a fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference. Furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
	urldate = {2025-06-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
	year = {2015},
	keywords = {zu lesen},
}

@inproceedings{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	language = {en},
	urldate = {2025-06-09},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	keywords = {zu lesen},
	pages = {2256--2265},
}

@inproceedings{shen_non-autoregressive_2023,
	title = {Non-autoregressive {Conditional} {Diffusion} {Models} for {Time} {Series} {Prediction}},
	url = {https://proceedings.mlr.press/v202/shen23d.html},
	abstract = {Recently, denoising diffusion models have led to significant breakthroughs in the generation of images, audio and text. However, it is still an open question on how to adapt their strong modeling ability to model time series. In this paper, we propose TimeDiff, a non-autoregressive diffusion model that achieves high-quality time series prediction with the introduction of two novel conditioning mechanisms: future mixup and autoregressive initialization. Similar to teacher forcing, future mixup allows parts of the ground-truth future predictions for conditioning, while autoregressive initialization helps better initialize the model with basic time series patterns such as short-term trends. Extensive experiments are performed on nine real-world datasets. Results show that TimeDiff consistently outperforms existing time series diffusion models, and also achieves the best overall performance across a variety of the existing strong baselines (including transformers and FiLM).},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shen, Lifeng and Kwok, James},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {31016--31029},
}

@inproceedings{shen_multi-resolution_2023,
	title = {Multi-{Resolution} {Diffusion} {Models} for {Time} {Series} {Forecasting}},
	url = {https://openreview.net/forum?id=mmjnr0G8ZY},
	abstract = {The diffusion model has been successfully used in many computer vision applications, such as text-guided image generation and image-to-image translation. Recently, there have been attempts on extending the diffusion model for time series data. However, these extensions are fairly straightforward and do not utilize the unique properties of time series data. As different patterns are usually exhibited at multiple scales of a time series, we in this paper leverage this multi-resolution temporal structure and propose the multi-resolution diffusion model (mr-Diff). By using the seasonal-trend decomposition, we sequentially extract fine-to-coarse trends from the time series for forward diffusion. The denoising process then proceeds in an easy-to-hard non-autoregressive manner. The coarsest trend is generated first. Finer details are progressively added, using the predicted coarser trends as condition variables. Experimental results on nine real-world time series datasets demonstrate that mr-Diff outperforms state-of-the-art time series diffusion models. It is also better than or comparable across a wide variety of advanced time series prediction models.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Shen, Lifeng and Chen, Weiyu and Kwok, James},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
}

@inproceedings{sklar_fonctions_1959,
	title = {Fonctions de répartition à n dimensions et leurs marges},
	volume = {8},
	booktitle = {Annales de l'{ISUP}},
	author = {Sklar, M},
	year = {1959},
	keywords = {zu lesen},
	pages = {229--231},
}

@article{shen_take_2024,
	title = {Take an {Irregular} {Route}: {Enhance} the {Decoder} of {Time}-{Series} {Forecasting} {Transformer}},
	volume = {11},
	issn = {2327-4662},
	shorttitle = {Take an {Irregular} {Route}},
	url = {https://ieeexplore.ieee.org/abstract/document/10352988},
	doi = {10.1109/JIOT.2023.3341099},
	abstract = {With the development of Internet of Things (IoT) systems, precise long-term forecasting method is requisite for decision makers to evaluate current statuses and formulate future policies. Currently, transformer and MLP are two paradigms for deep time-series forecasting and the former one is more prevailing in virtue of its exquisite attention mechanism and encoder–decoder architecture. However, data scientists seem to be more willing to dive into the research of encoder, leaving decoder unconcerned. Some researchers even adopt linear projections in lieu of the decoder to reduce the complexity. We argue that both extracting the features of input sequence and seeking the relations of input and prediction sequence, which are respective functions of encoder and decoder, are of paramount significance. Motivated from the success of FPN in CV field, we propose FPPformer to utilize bottom-up and top-down architectures, respectively, in encoder and decoder to build the full and rational hierarchy. The cutting-edge patchwise attention is exploited and further developed with the combination, whose format is also different in encoder and decoder, of revamped elementwise attention in this work. Extensive experiments with six state-of-the-art baselines on twelve benchmarks verify the promising performances of FPPformer and the importance of elaborately devising decoder in time-series forecasting transformer. The source code is released in https://github.com/OrigamiSL/FPPformer.},
	number = {8},
	urldate = {2025-05-08},
	journal = {IEEE Internet of Things Journal},
	author = {Shen, Li and Wei, Yuning and Wang, Yangzhu and Qiu, Huaxin},
	month = apr,
	year = {2024},
	keywords = {Decoding, Deep learning, Feature extraction, Forecasting, Internet of Things, Predictive models, Time series analysis, Transformers, neural network, time-series forecasting, transformer, zu lesen},
	pages = {14344--14356},
}

@article{shao_exploring_2025,
	title = {Exploring {Progress} in {Multivariate} {Time} {Series} {Forecasting}: {Comprehensive} {Benchmarking} and {Heterogeneity} {Analysis}},
	volume = {37},
	issn = {1558-2191},
	shorttitle = {Exploring {Progress} in {Multivariate} {Time} {Series} {Forecasting}},
	url = {https://ieeexplore.ieee.org/document/10726722},
	doi = {10.1109/TKDE.2024.3484454},
	abstract = {Multivariate Time Series (MTS) analysis is crucial to understanding and managing complex systems, such as traffic and energy systems, and a variety of approaches to MTS forecasting have been proposed recently. However, we often observe inconsistent or seemingly contradictory performance findings across different studies. This hinders our understanding of the merits of different approaches and slows down progress. We address the need for means of assessing MTS forecasting proposals reliably and fairly, in turn enabling better exploitation of MTS as seen in different applications. Specifically, we first propose BasicTS+, a benchmark designed to enable fair, comprehensive, and reproducible comparison of MTS forecasting solutions. BasicTS+ establishes a unified training pipeline and reasonable settings, enabling an unbiased evaluation. Second, we identify the heterogeneity across different MTS as an important consideration and enable classification of MTS based on their temporal and spatial characteristics. Disregarding this heterogeneity is a prime reason for difficulties in selecting the most promising technical directions. Third, we apply BasicTS+ along with rich datasets to assess the capabilities of more than 30 MTS forecasting solutions. This provides readers with an overall picture of the cutting-edge research on MTS forecasting.},
	number = {1},
	urldate = {2024-12-21},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Shao, Zezhi and Wang, Fei and Xu, Yongjun and Wei, Wei and Yu, Chengqing and Zhang, Zhao and Yao, Di and Sun, Tao and Jin, Guangyin and Cao, Xin and Cong, Gao and Jensen, Christian S. and Cheng, Xueqi},
	month = jan,
	year = {2025},
	keywords = {Benchmark testing, Benchmarking, Computer science, Data models, Electricity, Forecasting, Predictive models, Proposals, Reliability, Time series analysis, Transformers, long-term time series forecasting, multivariate time series, spatial-temporal forecasting, zu lesen},
	pages = {291--305},
}

@inproceedings{shahroudi_evaluation_2024,
	title = {Evaluation of {Trajectory} {Distribution} {Predictions} with {Energy} {Score}},
	url = {https://proceedings.mlr.press/v235/shahroudi24a.html},
	abstract = {Predicting the future trajectory of surrounding objects is inherently uncertain and vital in the safe and reliable planning of autonomous systems such as in self-driving cars. Although trajectory prediction models have become increasingly sophisticated in dealing with the complexities of spatiotemporal data, the evaluation methods used to assess these models have not kept pace. "Minimum of N" is a common family of metrics used to assess the rich outputs of such models. We critically examine the Minimum of N within the proper scoring rules framework to show that it is not strictly proper and demonstrate how that could lead to a misleading assessment of multimodal trajectory predictions. As an alternative, we propose using Energy Score-based evaluation measures, leveraging their proven propriety for a more reliable evaluation of trajectory distribution predictions.},
	language = {en},
	urldate = {2025-07-04},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shahroudi, Novin and Lepson, Mihkel and Kull, Meelis},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {44322--44341},
}

@inproceedings{sen_think_2019,
	title = {Think {Globally}, {Act} {Locally}: {A} {Deep} {Neural} {Network} {Approach} to {High}-{Dimensional} {Time} {Series} {Forecasting}},
	volume = {32},
	shorttitle = {Think {Globally}, {Act} {Locally}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html},
	abstract = {Forecasting high-dimensional time series plays a crucial role in many applications such as demand forecasting and financial predictions. Modern datasets can have millions of correlated time-series that evolve together, i.e they are extremely high dimensional (one dimension for each individual time-series). There is a need for exploiting global patterns and coupling them with local calibration for better prediction. However, most recent deep learning approaches in the literature are one-dimensional, i.e, even though they are trained on the whole dataset, during prediction, the future forecast for a single dimension mainly depends on past values from the same dimension. In this paper, we seek to correct this deficiency and propose DeepGLO, a deep forecasting model which thinks globally and acts locally. In particular, DeepGLO is a hybrid model that combines a global matrix factorization model regularized by a temporal convolution network, along with another temporal network that can capture local properties of each time-series and associated covariates. Our model can be trained effectively on high-dimensional but diverse time series, where different time series can have vastly different scales, without a priori normalization or rescaling. Empirical results demonstrate that DeepGLO can outperform state-of-the-art approaches; for example, we see more than 25\% improvement in WAPE over other methods on a public dataset that contains more than 100K-dimensional time series.},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sen, Rajat and Yu, Hsiang-Fu and Dhillon, Inderjit S},
	year = {2019},
	keywords = {zu lesen},
}

@article{semenoglou_investigating_2021,
	title = {Investigating the accuracy of cross-learning time series forecasting methods},
	volume = {37},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207020301850},
	doi = {10.1016/j.ijforecast.2020.11.009},
	abstract = {The M4 competition identified innovative forecasting methods, advancing the theory and practice of forecasting. One of the most promising innovations of M4 was the utilization of cross-learning approaches that allow models to learn from multiple series how to accurately predict individual ones. In this paper, we investigate the potential of cross-learning by developing various neural network models that adopt such an approach, and we compare their accuracy to that of traditional models that are trained in a series-by-series fashion. Our empirical evaluation, which is based on the M4 monthly data, confirms that cross-learning is a promising alternative to traditional forecasting, at least when appropriate strategies for extracting information from large, diverse time series data sets are considered. Ways of combining traditional with cross-learning methods are also examined in order to initiate further research in the field.},
	number = {3},
	urldate = {2025-05-28},
	journal = {International Journal of Forecasting},
	author = {Semenoglou, Artemios-Anargyros and Spiliotis, Evangelos and Makridakis, Spyros and Assimakopoulos, Vassilios},
	month = jul,
	year = {2021},
	keywords = {Cross-learning, Features, M4 competition, Neural networks, Time series, zu lesen},
	pages = {1072--1084},
}

@article{scheuerer_variogram-based_2015,
	title = {Variogram-{Based} {Proper} {Scoring} {Rules} for {Probabilistic} {Forecasts} of {Multivariate} {Quantities}},
	volume = {143},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/143/4/mwr-d-14-00269.1.xml},
	doi = {10.1175/MWR-D-14-00269.1},
	abstract = {Proper scoring rules provide a theoretically principled framework for the quantitative assessment of the predictive performance of probabilistic forecasts. While a wide selection of such scoring rules for univariate quantities exists, there are only few scoring rules for multivariate quantities, and many of them require that forecasts are given in the form of a probability density function. The energy score, a multivariate generalization of the continuous ranked probability score, is the only commonly used score that is applicable in the important case of ensemble forecasts, where the multivariate predictive distribution is represented by a finite sample. Unfortunately, its ability to detect incorrectly specified correlations between the components of the multivariate quantity is somewhat limited. In this paper the authors present an alternative class of proper scoring rules based on the geostatistical concept of variograms. The sensitivity of these variogram-based scoring rules to incorrectly predicted means, variances, and correlations is studied in a number of examples with simulated observations and forecasts; they are shown to be distinctly more discriminative with respect to the correlation structure. This conclusion is confirmed in a case study with postprocessed wind speed forecasts at five wind park locations in Colorado.},
	language = {EN},
	number = {4},
	urldate = {2025-03-21},
	journal = {Monthly Weather Review},
	author = {Scheuerer, Michael and Hamill, Thomas M.},
	month = apr,
	year = {2015},
	note = {Publisher: American Meteorological Society},
	keywords = {Ensembles, Forecast verification/skill, zu lesen},
	pages = {1321--1334},
}

@inproceedings{seeger_bayesian_2016,
	title = {Bayesian {Intermittent} {Demand} {Forecasting} for {Large} {Inventories}},
	volume = {29},
	url = {https://papers.nips.cc/paper/2016/hash/03255088ed63354a54e0e5ed957e9008-Abstract.html},
	abstract = {We present a scalable and robust Bayesian method for demand forecasting in the context of a large e-commerce platform, paying special attention to intermittent and bursty target statistics. Inference is approximated by the Newton-Raphson algorithm, reduced to linear-time Kalman smoothing, which allows us to operate on several orders of magnitude larger problems than previous related work. In a study on large real-world sales datasets, our method outperforms competing approaches on fast and medium moving items.},
	urldate = {2025-06-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Seeger, Matthias W and Salinas, David and Flunkert, Valentin},
	year = {2016},
	keywords = {zu lesen},
}

@article{salinas_deepar_2020,
	title = {{DeepAR}: {Probabilistic} forecasting with autoregressive recurrent networks},
	volume = {36},
	issn = {0169-2070},
	shorttitle = {{DeepAR}},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207019301888},
	doi = {10.1016/j.ijforecast.2019.07.001},
	abstract = {Probabilistic forecasting, i.e., estimating a time series’ future probability distribution given its past, is a key enabler for optimizing business processes. In retail businesses, for example, probabilistic demand forecasts are crucial for having the right inventory available at the right time and in the right place. This paper proposes DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an autoregressive recurrent neural network model on a large number of related time series. We demonstrate how the application of deep learning techniques to forecasting can overcome many of the challenges that are faced by widely-used classical approaches to the problem. By means of extensive empirical evaluations on several real-world forecasting datasets, we show that our methodology produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.},
	number = {3},
	urldate = {2024-12-24},
	journal = {International Journal of Forecasting},
	author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
	month = jul,
	year = {2020},
	keywords = {Big data, Deep learning, Demand forecasting, Neural networks, Probabilistic forecasting, zu lesen},
	pages = {1181--1191},
}

@article{sakoe_dynamic_1978,
	title = {Dynamic programming algorithm optimization for spoken word recognition},
	volume = {26},
	doi = {10.1109/TASSP.1978.1163055},
	number = {1},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Sakoe, H. and Chiba, S.},
	year = {1978},
	keywords = {Acoustics, Constraint optimization, Dynamic programming, Feature extraction, Fluctuations, Heuristic algorithms, Pattern matching, Signal processing algorithms, Speech processing, Timing, zu lesen},
	pages = {43--49},
}

@article{roweis_unifying_1999,
	title = {A {Unifying} {Review} of {Linear} {Gaussian} {Models}},
	volume = {11},
	issn = {0899-7667},
	url = {https://ieeexplore.ieee.org/abstract/document/6790691},
	doi = {10.1162/089976699300016674},
	abstract = {Factor analysis, principal component analysis, mixtures of gaussian clusters, vector quantization, Kalman filter models, and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model. This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity. Through the use of other nonlinearities, we show how independent component analysis is also a variation of the same basic generative model. We show that factor analysis and mixtures of gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term. We introduce a new model for static data, known as sensible principal component analysis, as well as a novel concept of spatially adaptive observation noise. We also review some of the literature involving global and local mixtures of the basic models and provide pseudocode for inference and learning for all the basic models.},
	number = {2},
	urldate = {2025-06-11},
	journal = {Neural Computation},
	author = {Roweis, Sam and Ghahramani, Zoubin},
	month = feb,
	year = {1999},
	keywords = {zu lesen},
	pages = {305--345},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	shorttitle = {U-{Net}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image, zu lesen},
	pages = {234--241},
}

@inproceedings{salinas_high-dimensional_2019,
	title = {High-dimensional multivariate forecasting with low-rank {Gaussian} {Copula} {Processes}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/0b105cf1504c4e241fcc6d519ea962fb-Abstract.html},
	abstract = {Predicting the dependencies between observations from multiple time series is critical for applications such as anomaly detection, financial risk management, causal analysis, or demand forecasting.
However, the computational and numerical difficulties of estimating time-varying and high-dimensional covariance matrices often limits existing methods to handling at most a few hundred dimensions or requires making strong assumptions on the dependence between series.
We propose to combine an RNN-based time series model with a Gaussian copula process output model with a low-rank covariance structure to reduce the computational complexity and handle non-Gaussian marginal distributions.
This permits to drastically reduce the number of parameters and consequently allows the modeling of time-varying correlations of thousands of time series. We show on several real-world datasets that our method provides significant accuracy improvements over state-of-the-art baselines and perform an ablation study analyzing the contributions of the different components of our model.},
	urldate = {2025-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Salinas, David and Bohlke-Schneider, Michael and Callot, Laurent and Medico, Roberto and Gasthaus, Jan},
	year = {2019},
	keywords = {zu lesen},
}

@article{rosenblatt_estimation_1956,
	title = {Estimation of a probability density-function and mode},
	volume = {27},
	issn = {0003-4851},
	journal = {The Annals of Mathematical Statistics},
	author = {Rosenblatt, M.},
	year = {1956},
	note = {Place: Baltimore, Md.
Publisher: Waverly Press},
	keywords = {zu lesen},
	pages = {832},
}

@inproceedings{rasul_multivariate_2020,
	title = {Multivariate {Probabilistic} {Time} {Series} {Forecasting} via {Conditioned} {Normalizing} {Flows}},
	url = {https://openreview.net/forum?id=WiGQBFuVRv},
	abstract = {Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multi-variate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multi-variate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series.},
	language = {en},
	urldate = {2025-01-16},
	booktitle = {Proceedings of the {Ninth} {International} {Conference} on {Learning} {Representations}},
	author = {Rasul, Kashif and Sheikh, Abdul-Saboor and Schuster, Ingmar and Bergmann, Urs M. and Vollgraf, Roland},
	month = oct,
	year = {2020},
	keywords = {zu lesen},
}

@inproceedings{rasul_autoregressive_2021,
	title = {Autoregressive {Denoising} {Diffusion} {Models} for {Multivariate} {Probabilistic} {Time} {Series} {Forecasting}},
	url = {https://proceedings.mlr.press/v139/rasul21a.html},
	abstract = {In this work, we propose TimeGrad, an autoregressive model for multivariate probabilistic time series forecasting which samples from the data distribution at each time step by estimating its gradient. To this end, we use diffusion probabilistic models, a class of latent variable models closely connected to score matching and energy-based methods. Our model learns gradients by optimizing a variational bound on the data likelihood and at inference time converts white noise into a sample of the distribution of interest through a Markov chain using Langevin sampling. We demonstrate experimentally that the proposed autoregressive denoising diffusion model is the new state-of-the-art multivariate probabilistic forecasting method on real-world data sets with thousands of correlated dimensions. We hope that this method is a useful tool for practitioners and lays the foundation for future research in this area.},
	language = {en},
	urldate = {2024-12-21},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rasul, Kashif and Seward, Calvin and Schuster, Ingmar and Vollgraf, Roland},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {8857--8868},
}

@misc{rasul_vq-ar_2022,
	title = {{VQ}-{AR}: {Vector} {Quantized} {Autoregressive} {Probabilistic} {Time} {Series} {Forecasting}},
	shorttitle = {{VQ}-{AR}},
	url = {http://arxiv.org/abs/2205.15894},
	doi = {10.48550/arXiv.2205.15894},
	abstract = {Time series models aim for accurate predictions of the future given the past, where the forecasts are used for important downstream tasks like business decision making. In practice, deep learning based time series models come in many forms, but at a high level learn some continuous representation of the past and use it to output point or probabilistic forecasts. In this paper, we introduce a novel autoregressive architecture, VQ-AR, which instead learns a {\textbackslash}emph\{discrete\} set of representations that are used to predict the future. Extensive empirical comparison with other competitive deep learning models shows that surprisingly such a discrete set of representations gives state-of-the-art or equivalent results on a wide variety of time series datasets. We also highlight the shortcomings of this approach, explore its zero-shot generalization capabilities, and present an ablation study on the number of representations. The full source code of the method will be available at the time of publication with the hope that researchers can further investigate this important but overlooked inductive bias for the time series domain.},
	urldate = {2024-12-24},
	publisher = {arXiv},
	author = {Rasul, Kashif and Park, Young-Jin and Ramström, Max Nihlén and Kim, Kyung-Min},
	month = may,
	year = {2022},
	note = {arXiv:2205.15894},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, zu lesen},
}

@inproceedings{rezende_stochastic_2014,
	title = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},
	url = {https://proceedings.mlr.press/v32/rezende14.html},
	abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
	language = {en},
	urldate = {2025-06-11},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	month = jun,
	year = {2014},
	note = {ISSN: 1938-7228},
	keywords = {zu lesen},
	pages = {1278--1286},
}

@article{ray_ensemble_2020,
	title = {Ensemble forecasts of coronavirus disease 2019 ({COVID}-19) in the {US}},
	journal = {MedRXiv},
	author = {Ray, Evan L and Wattanachit, Nutcha and Niemi, Jarad and Kanji, Abdul Hannan and House, Katie and Cramer, Estee Y and Bracher, Johannes and Zheng, Andrew and Yamana, Teresa K and Xiong, Xinyue and {others}},
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory Press},
	keywords = {zu lesen},
	pages = {2020--08},
}

@article{raeesi_traffic_2014,
	title = {Traffic time series forecasting by feedforward neural network: a case study based on traffic data of monroe},
	volume = {XL-2/W3},
	issn = {2194-9034},
	shorttitle = {{TRAFFIC} {TIME} {SERIES} {FORECASTING} {BY} {FEEDFORWARD} {NEURAL} {NETWORK}},
	url = {https://isprs-archives.copernicus.org/articles/XL-2-W3/219/2014/},
	doi = {10.5194/isprsarchives-XL-2-W3-219-2014},
	abstract = {Short time prediction is one of the most important factors in intelligence transportation system (ITS). In this research, the use of feed forward neural network for traffic time-series prediction is presented. In this paper, the traffic in one direction of the road segment is predicted. The input of the neural network is the time delay data exported from the road traffic data of Monroe city. The time delay data is used for training the network. For generating the time delay data, the traffic data related to the first 300 days of 2008 is used. The performance of the feed forward neural network model is validated using the real observation data of the 301st day.},
	language = {en},
	urldate = {2023-06-01},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Raeesi, M. and Mesgari, M. S. and Mahmoudi, P.},
	month = oct,
	year = {2014},
	keywords = {zu lesen},
	pages = {219--223},
}

@article{pourahmadi_covariance_2011,
	title = {Covariance {Estimation}: {The} {GLM} and {Regularization} {Perspectives}},
	volume = {26},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Covariance {Estimation}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-26/issue-3/Covariance-Estimation-The-GLM-and-Regularization-Perspectives/10.1214/11-STS358.full},
	doi = {10.1214/11-STS358},
	abstract = {Finding an unconstrained and statistically interpretable reparameterization of a covariance matrix is still an open problem in statistics. Its solution is of central importance in covariance estimation, particularly in the recent high-dimensional data environment where enforcing the positive-definiteness constraint could be computationally expensive. We provide a survey of the progress made in modeling covariance matrices from two relatively complementary perspectives: (1) generalized linear models (GLM) or parsimony and use of covariates in low dimensions, and (2) regularization or sparsity for high-dimensional data. An emerging, unifying and powerful trend in both perspectives is that of reducing a covariance estimation problem to that of estimating a sequence of regression problems. We point out several instances of the regression-based formulation. A notable case is in sparse estimation of a precision matrix or a Gaussian graphical model leading to the fast graphical LASSO algorithm. Some advantages and limitations of the regression-based Cholesky decomposition relative to the classical spectral (eigenvalue) and variance-correlation decompositions are highlighted. The former provides an unconstrained and statistically interpretable reparameterization, and guarantees the positive-definiteness of the estimated covariance matrix. It reduces the unintuitive task of covariance estimation to that of modeling a sequence of regressions at the cost of imposing an a priori order among the variables. Elementwise regularization of the sample covariance matrix such as banding, tapering and thresholding has desirable asymptotic properties and the sparse estimated covariance matrix is positive definite with probability tending to one for large samples and dimensions.},
	number = {3},
	urldate = {2025-06-18},
	journal = {Statistical Science},
	author = {Pourahmadi, Mohsen},
	month = aug,
	year = {2011},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Bayesian estimation, Cholesky decomposition, Sparsity, dependence and correlation, graphical models, longitudinal data, parsimony, penalized likelihood, precision matrix, spectral decomposition, variance-correlation decomposition, zu lesen},
	pages = {369--387},
}

@inproceedings{rasul_vq-tr_2023,
	title = {{VQ}-{TR}: {Vector} {Quantized} {Attention} for {Time} {Series} {Forecasting}},
	shorttitle = {{VQ}-{TR}},
	url = {https://openreview.net/forum?id=IxpTsFS7mh},
	abstract = {Probabilistic time series forecasting is a challenging problem due to the long sequences involved, the large number of samples needed for accurate probabilistic inference, and the need for real-time inference in many applications. These challenges necessitate methods that are not only accurate but computationally efficient. Unfortunately, most current state-of-the-art methods for time series forecasting are based on Transformers, which scale poorly due to quadratic complexity in sequence length, and are therefore needlessly computationally inefficient. Moreover, with a few exceptions, these methods have only been evaluated for non-probabilistic point estimation. In this work, we address these two shortcomings. For the first, we introduce VQ-TR, which maps large sequences to a discrete set of latent representations as part of the Attention module. This not only allows us to attend over larger context windows with linear complexity in sequence length but also allows for effective regularization to avoid overfitting. For the second, we provide what is to the best of our knowledge the first systematic comparison of modern Transformer-based time series forecasting methods for probabilistic forecasting. In this comparison, we find that VQ-TR performs better or comparably to all other methods while being computationally efficient.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Rasul, Kashif and Bennett, Andrew and Vicente, Pablo and Gupta, Umang and Ghonia, Hena and Schneider, Anderson and Nevmyvaka, Yuriy},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
}

@inproceedings{rangapuram_deep_2018,
	title = {Deep {State} {Space} {Models} for {Time} {Series} {Forecasting}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/5cf68969fb67aa6082363a6d4e6468e2-Abstract.html},
	abstract = {We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning. By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network, our method retains desired properties of state space models such as data efficiency and interpretability, while making use of the ability to learn complex patterns from raw data offered by deep learning approaches. Our method scales gracefully from regimes where little training data is available to regimes where data from millions of time series can be leveraged to learn accurate models. We provide qualitative as well as quantitative results with the proposed method, showing that it compares favorably to the state-of-the-art.},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
	year = {2018},
	keywords = {zu lesen},
}

@inproceedings{qin_dual-stage_2017,
	title = {A {Dual}-{Stage} {Attention}-{Based} {Recurrent} {Neural} {Network} for {Time} {Series} {Prediction}},
	url = {https://doi.org/10.24963/ijcai.2017/366},
	doi = {10.24963/ijcai.2017/366},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI}-17},
	author = {Qin, Yao and Song, Dongjin and Chen, Haifeng and Cheng, Wei and Jiang, Guofei and Cottrell, Garrison W.},
	year = {2017},
	keywords = {zu lesen},
	pages = {2627--2633},
}

@techreport{pinson_discrimination_2013,
	address = {Kgs. Lyngby},
	type = {Technical {Report}},
	title = {Discrimination ability of the {Energy} score},
	abstract = {Research on generating and verification of multivariate probabilistic forecasts has gained increased interest over the last few years. Emphasis is placed here on the evaluation of forecast quality with the Energy score, which is based on a quadratic scoring rule. While this score may be seen as appealing since being proper, we show that its discrimination ability may be limited when focusing on the dependence structure of multivariate probabilistic forecasts. For the case of multivariate Gaussian process, a theoretical upper for such discrimination ability is derived and discussed. This limited discrimination ability may eventually get compromised by computational and sampling issues, as dimension increases.},
	number = {15},
	institution = {Technical University of Denmark},
	author = {Pinson, Pierre and Tastu, Julija},
	year = {2013},
	keywords = {Discrimination, Energy score, Multivariate scenarios, Probabilistic forecasting, Proper score, zu lesen},
}

@inproceedings{piao_fredformer_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Fredformer: {Frequency} {Debiased} {Transformer} for {Time} {Series} {Forecasting}},
	isbn = {979-8-4007-0490-1},
	shorttitle = {Fredformer},
	url = {https://dl.acm.org/doi/10.1145/3637528.3671928},
	doi = {10.1145/3637528.3671928},
	abstract = {The Transformer model has shown leading performance in time series forecasting. Nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high-frequency features, showing a frequency bias. This bias prevents the model from accurately capturing important high-frequency data features. In this paper, we undertake empirical analyses to understand this bias and discover that frequency bias results from the model disproportionately focusing on frequency features with higher energy. Based on our analysis, we formulate this bias and propose Fredformer, a Transformer-based framework designed to mitigate frequency bias by learning features equally across different frequency bands. This approach prevents the model from overlooking lower amplitude features important for accurate forecasting. Extensive experiments show the effectiveness of our proposed approach, which can outperform other baselines in different real-world time-series datasets. Furthermore, we introduce a lightweight variant of the Fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. The code is available at: https://github.com/chenzRG/Fredformer},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Piao, Xihao and Chen, Zheng and Murayama, Taichi and Matsubara, Yasuko and Sakurai, Yasushi},
	month = aug,
	year = {2024},
	keywords = {zu lesen},
	pages = {2400--2410},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
	keywords = {zu lesen},
}

@inproceedings{park_learning_2022,
	title = {Learning {Quantile} {Functions} without {Quantile} {Crossing} for {Distribution}-free {Time} {Series} {Forecasting}},
	url = {https://proceedings.mlr.press/v151/park22a.html},
	abstract = {Quantile regression is an effective technique to quantify uncertainty, fit challenging underlying distributions, and often provide full probabilistic predictions through joint learnings over multiple quantile levels. A common drawback of these joint quantile regressions, however, is quantile crossing, which violates the desirable monotone property of the conditional quantile function. In this work, we propose the Incremental (Spline) Quantile Functions I(S)QF, a flexible and efficient distribution-free quantile estimation framework that resolves quantile crossing with a simple neural network layer. Moreover, I(S)QF inter/extrapolate to predict arbitrary quantile levels that differ from the underlying training ones. Equipped with the analytical evaluation of the continuous ranked probability score of I(S)QF representations, we apply our methods to NN-based times series forecasting cases, where the savings of the expensive re-training costs for non-trained quantile levels is particularly significant. We also provide a generalization error analysis of our proposed approaches under the sequence-to-sequence setting. Lastly, extensive experiments demonstrate the improvement of consistency and accuracy errors over other baselines.},
	language = {en},
	urldate = {2025-06-06},
	booktitle = {Proceedings of {The} 25th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Park, Youngsuk and Maddix, Danielle and Aubet, François-Xavier and Kan, Kelvin and Gasthaus, Jan and Wang, Yuyang},
	month = may,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {8127--8150},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	keywords = {zu lesen},
	pages = {2825--2830},
}

@article{parzen_estimation_1962,
	title = {On {Estimation} of a {Probability} {Density} {Function} and {Mode}},
	volume = {33},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2237880},
	number = {3},
	urldate = {2025-06-29},
	journal = {The Annals of Mathematical Statistics},
	author = {Parzen, Emanuel},
	year = {1962},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {zu lesen},
	pages = {1065--1076},
}

@misc{paparrizos_bridging_2024,
	title = {Bridging the {Gap}: {A} {Decade} {Review} of {Time}-{Series} {Clustering} {Methods}},
	shorttitle = {Bridging the {Gap}},
	url = {http://arxiv.org/abs/2412.20582},
	doi = {10.48550/arXiv.2412.20582},
	abstract = {Time series, as one of the most fundamental representations of sequential data, has been extensively studied across diverse disciplines, including computer science, biology, geology, astronomy, and environmental sciences. The advent of advanced sensing, storage, and networking technologies has resulted in high-dimensional time-series data, however, posing significant challenges for analyzing latent structures over extended temporal scales. Time-series clustering, an established unsupervised learning strategy that groups similar time series together, helps unveil hidden patterns in these complex datasets. In this survey, we trace the evolution of time-series clustering methods from classical approaches to recent advances in neural networks. While previous surveys have focused on specific methodological categories, we bridge the gap between traditional clustering methods and emerging deep learning-based algorithms, presenting a comprehensive, unified taxonomy for this research area. This survey highlights key developments and provides insights to guide future research in time-series clustering.},
	urldate = {2025-06-25},
	publisher = {arXiv},
	author = {Paparrizos, John and Yang, Fan and Li, Haojun},
	month = dec,
	year = {2024},
	note = {arXiv:2412.20582},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning, zu lesen},
}

@inproceedings{paparrizos_k-shape_2015,
	address = {New York, NY, USA},
	series = {{SIGMOD} '15},
	title = {k-{Shape}: {Efficient} and {Accurate} {Clustering} of {Time} {Series}},
	isbn = {978-1-4503-2758-9},
	shorttitle = {k-{Shape}},
	url = {https://dl.acm.org/doi/10.1145/2723372.2737793},
	doi = {10.1145/2723372.2737793},
	abstract = {The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data mining methods, not only due to its exploratory power, but also as a preprocessing step or subroutine for other techniques. In this paper, we present k-Shape, a novel algorithm for time-series clustering. k-Shape relies on a scalable iterative refinement procedure, which creates homogeneous and well-separated clusters. As its distance measure, k-Shape uses a normalized version of the cross-correlation measure in order to consider the shapes of time series while comparing them. Based on the properties of that distance measure, we develop a method to compute cluster centroids, which are used in every iteration to update the assignment of time series to clusters. To demonstrate the robustness of k-Shape, we perform an extensive experimental evaluation of our approach against partitional, hierarchical, and spectral clustering methods, with combinations of the most competitive distance measures. k-Shape outperforms all scalable approaches in terms of accuracy. Furthermore, k-Shape also outperforms all non-scalable (and hence impractical) combinations, with one exception that achieves similar accuracy results. However, unlike k-Shape, this combination requires tuning of its distance measure and is two orders of magnitude slower than k-Shape. Overall, k-Shape emerges as a domain-independent, highly accurate, and highly efficient clustering approach for time series with broad applications.},
	urldate = {2025-07-01},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Paparrizos, John and Gravano, Luis},
	month = may,
	year = {2015},
	keywords = {zu lesen},
	pages = {1855--1870},
}

@article{papamakarios_normalizing_2021,
	title = {Normalizing {Flows} for {Probabilistic} {Modeling} and {Inference}},
	volume = {22},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v22/19-1028.html},
	abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
	number = {57},
	urldate = {2025-06-05},
	journal = {Journal of Machine Learning Research},
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	year = {2021},
	keywords = {zu lesen},
	pages = {1--64},
}

@article{osband_information-eliciting_1985,
	title = {Information-eliciting compensation schemes},
	volume = {46},
	journal = {Journal of Public Economics},
	author = {Osband, Kent and Reichelstein, Stefan},
	year = {1985},
	keywords = {zu lesen},
	pages = {107--115},
}

@inproceedings{oreshkin_n-beats_2019,
	title = {N-{BEATS}: {Neural} basis expansion analysis for interpretable time series forecasting},
	shorttitle = {N-{BEATS}},
	url = {https://openreview.net/forum?id=r1ecqn4YwB},
	abstract = {We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11\% over a statistical benchmark and by 3\% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.},
	language = {en},
	urldate = {2025-07-04},
	booktitle = {Proceedings of the {Eighth} {International} {Conference} on {Learning} {Representations}},
	author = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
	month = sep,
	year = {2019},
	keywords = {zu lesen},
}

@inproceedings{papamakarios_masked_2017,
	title = {Masked {Autoregressive} {Flow} for {Density} {Estimation}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html},
	abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
	urldate = {2025-06-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	year = {2017},
	keywords = {zu lesen},
}

@inproceedings{nie_time_2022,
	title = {A {Time} {Series} is {Worth} 64 {Words}: {Long}-term {Forecasting} with {Transformers}},
	shorttitle = {A {Time} {Series} is {Worth} 64 {Words}},
	url = {https://openreview.net/forum?id=Jbdc0vTOcol},
	abstract = {We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-training performed on one dataset to other datasets also produces SOTA forecasting accuracy.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Nie, Yuqi and Nguyen, Nam H. and Sinthong, Phanwadee and Kalagnanam, Jayant},
	month = sep,
	year = {2022},
	keywords = {zu lesen},
}

@inproceedings{olsen_think_2024,
	title = {Think {Global}, {Adapt} {Local}: {Learning} {Locally} {Adaptive} {K}-{Nearest} {Neighbor} {Kernel} {Density} {Estimators}},
	shorttitle = {Think {Global}, {Adapt} {Local}},
	url = {https://proceedings.mlr.press/v238/olsen24a.html},
	abstract = {Kernel density estimation (KDE) is a powerful technique for non-parametric density estimation, yet practical use of KDE-based methods remains limited by insufficient representational flexibility, especially for higher-dimensional data. Contrary to KDE, K-nearest neighbor (KNN) density estimation procedures locally adapt the density based on the K-nearest neighborhood, but unfortunately only provide asymptotically correct density estimates. We present the KNN-KDE method introducing observation-specific kernels for KDE that are locally adapted through priors defined by the covariance of the K-nearest neighborhood, forming a fully Bayesian model with exact density estimates. We further derive a scalable inference procedure that infers parameters through variational inference by optimizing the predictive likelihood exploiting sparsity, batched optimization, and parallel computation for massive inference speedups. We find that KNN-KDE provides valid density estimates superior to conventional KDE and KNN density estimation on both synthetic and real data sets. We further observe that the bayesian KNN-KDE even outperforms recent neural density estimation procedures on two of the five considered real data sets. The KNN-KDE unifies conventional kernel and KNN density estimation providing a scalable, generic and accurate framework for density estimation.},
	language = {en},
	urldate = {2025-06-29},
	booktitle = {Proceedings of {The} 27th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Olsen, Kenny and Lindrup, Rasmus M. Hoeegh and Mørup, Morten},
	month = apr,
	year = {2024},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {4114--4122},
}

@inproceedings{nix_estimating_1994,
	title = {Estimating the mean and variance of the target probability distribution},
	volume = {1},
	url = {https://ieeexplore.ieee.org/document/374138},
	doi = {10.1109/ICNN.1994.374138},
	abstract = {Introduces a method that estimates the mean and the variance of the probability distribution of the target as a function of the input, given an assumed target error-distribution model. Through the activation of an auxiliary output unit, this method provides a measure of the uncertainty of the usual network output for each input pattern. The authors derive the cost function and weight-update equations for the example of a Gaussian target error distribution, and demonstrate the feasibility of the network on a synthetic problem where the true input-dependent noise level is known.{\textless}{\textgreater}},
	urldate = {2025-06-04},
	booktitle = {Proceedings of 1994 {IEEE} {International} {Conference} on {Neural} {Networks} ({ICNN}'94)},
	author = {Nix, D.A. and Weigend, A.S.},
	month = jun,
	year = {1994},
	keywords = {Cognitive science, Computer errors, Computer science, Cost function, Equations, Error correction, Feedforward systems, Measurement uncertainty, Noise level, Probability distribution, zu lesen},
	pages = {55--60 vol.1},
}

@article{ning_comparative_2022,
	title = {A comparative machine learning study for time series oil production forecasting: {ARIMA}, {LSTM}, and {Prophet}},
	volume = {164},
	issn = {0098-3004},
	url = {https://www.sciencedirect.com/science/article/pii/S009830042200084X},
	doi = {https://doi.org/10.1016/j.cageo.2022.105126},
	abstract = {It is challenging to predict the production performance of unconventional reservoirs because of the sediment heterogeneity, intricate flow channels, and complex fluid phase behavior. The traditional oil production prediction methods (e.g., decline curve analysis and reservoir simulation modeling forecasting) are subjective. This paper presents a machine learning-based time series forecasting method, which considers the existing data as time series and extracts the salient characteristics of historical data to predict values of a future time sequence. We used time series forecasting because of the historical fluctuations in production well and reservoir operations. Three algorithms were studied and compared to address the limitations of traditional production forecasting: Auto-Regressive Integrated Moving Averages (ARIMA), Long-Short-Term Memory (LSTM) network, and Prophet. This study starts with the representative oil production data from a well located in an unconventional reservoir in the Denver-Julesburg (DJ) Basin. 70\% of the data was used for model training, whereas the remaining 30\% of data was used to evaluate the performance of the above-mentioned methods. Then, the decline curve analysis and reservoir simulation modeling forecasting were applied for comparison. The advantages of the machine-learning models include a simple workflow, no prior assumption about the reservoir type, fast prediction, and reliable performance prediction for a typical fluctuating declining curve. More importantly, the ‘Prophet’ model captures production fluctuation caused by winter impact, which can attract the operator's attention and prevent potential failures. This has rarely been explored and discussed by previous studies. The application of ARIMA, LSTM, and Prophet methods to 65 wells in the DJ Basin show that ARIMA and LSTM perform better than Prophet—probably because not all oil production data include seasonal influences. Furthermore, the wells in the nearby pads can be studied using the same parameter values in ARIMA and LSTM for predicting oil prediction in a transferred learning framework. Specifically, we observed that ARIMA is robust in predicting the oil production rate of wells across the DJ Basin.},
	journal = {Computers \& Geosciences},
	author = {Ning, Yanrui and Kazemi, Hossein and Tahmasebi, Pejman},
	year = {2022},
	keywords = {ARIMA, LSTM, Machine learning, Oil production prediction, Time series forecasting, zu lesen},
	pages = {105126},
}

@article{nguyen_estimating_2010,
	title = {Estimating {Divergence} {Functionals} and the {Likelihood} {Ratio} by {Convex} {Risk} {Minimization}},
	volume = {56},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/document/5605355},
	doi = {10.1109/TIT.2010.2068870},
	abstract = {We develop and analyze M-estimation methods for divergence functionals and the likelihood ratios of two probability distributions. Our method is based on a nonasymptotic variational characterization of f -divergences, which allows the problem of estimating divergences to be tackled via convex empirical risk optimization. The resulting estimators are simple to implement, requiring only the solution of standard convex programs. We present an analysis of consistency and convergence for these estimators. Given conditions only on the ratios of densities, we show that our estimators can achieve optimal minimax rates for the likelihood ratio and the divergence functionals in certain regimes. We derive an efficient optimization algorithm for computing our estimates, and illustrate their convergence behavior and practical viability by simulations.},
	number = {11},
	urldate = {2025-06-09},
	journal = {IEEE Transactions on Information Theory},
	author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
	month = nov,
	year = {2010},
	keywords = {Convergence, Convex functions, Convex optimization, Entropy, Estimation, Kernel, Kullback-Leibler (KL) divergence, M-estimation, Measurement, Probability distribution, density ratio estimation, divergence estimation, f-divergence, reproducing kernel Hilbert space (RKHS), surrogate loss functions, zu lesen},
	pages = {5847--5861},
}

@incollection{murphy_forecast_1985,
	title = {Forecast {Evaluation}},
	isbn = {978-0-429-30308-1},
	abstract = {Forecast evaluation can be described as "the process and practice of determining the quality and value of forecasts." As such, it consists of a body of concepts and procedures for comparing forecasts and observations and for assessing the value or utility of forecasts to users. The practice of evaluating weather forecasts is almost as old as routine weather forecasting itself, with the first papers and reports on evaluation methodology and on the results of evaluation studies appearing nearly a century ago. Moreover, the history and development of forecast evaluation in meteorology have been marked by several colorful incidents and by considerable controversy. Unfortunately, a comprehensive review of the literature on the evaluation of weather forecasts is clearly beyond the scope of this chapter. The reader is referred to publications by Muller (1944), Bleeker (1946), Brier and Allen (1951), Gringorten (1951), Johnson (1957), Meglis (1960), Murphy and Allen (1970), Dobryshman (1972), National Center for Atmospheric Research (1976), World Meteorological Organization (1980), and Daan (1984) for reviews, collections of papers, and bibliographies concerning various aspects of this topic.},
	booktitle = {Probability, {Statistics}, {And} {Decision} {Making} {In} {The} {Atmospheric} {Sciences}},
	publisher = {CRC Press},
	author = {Murphy, Allan H. and Daan, Harald},
	year = {1985},
	note = {Num Pages: 59},
	keywords = {zu lesen},
}

@article{mukherjee_ar-mdn_2018,
	title = {{AR}-{MDN}: {Associative} and {Recurrent} {Mixture} {Density} {Networks} for {eRetail} {Demand} {Forecasting}},
	volume = {11},
	number = {5},
	journal = {Proceedings of the VLDB Endowment},
	author = {Mukherjee, Srayanta and Shankar, Devashish and Ghosh, Atin and Tathawadekar, Nilam and Kompalli, Pramod and Sarawagi, Sunita and Chaudhury, Krishnendu},
	year = {2018},
	keywords = {zu lesen},
}

@article{montero-manso_principles_2021,
	title = {Principles and algorithms for forecasting groups of time series: {Locality} and globality},
	volume = {37},
	issn = {0169-2070},
	shorttitle = {Principles and algorithms for forecasting groups of time series},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207021000558},
	doi = {10.1016/j.ijforecast.2021.03.004},
	abstract = {Global methods that fit a single forecasting method to all time series in a set have recently shown surprising accuracy, even when forecasting large groups of heterogeneous time series. We provide the following contributions that help understand the potential and applicability of global methods and how they relate to traditional local methods that fit a separate forecasting method to each series: •Global and local methods can produce the same forecasts without any assumptions about similarity of the series in the set.•The complexity of local methods grows with the size of the set while it remains constant for global methods. This result supports the recent evidence and provides principles for the design of new algorithms.•In an extensive empirical study, we show that purposely naïve algorithms derived from these principles show outstanding accuracy. In particular, global linear models provide competitive accuracy with far fewer parameters than the simplest of local methods.},
	number = {4},
	urldate = {2025-05-28},
	journal = {International Journal of Forecasting},
	author = {Montero-Manso, Pablo and Hyndman, Rob J.},
	month = oct,
	year = {2021},
	keywords = {Cross-learning, Forecasting, Generalization, Global, Local, Pooled regression, Time series, zu lesen},
	pages = {1632--1653},
}

@misc{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	doi = {10.48550/arXiv.1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	urldate = {2025-07-04},
	publisher = {arXiv},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv:1411.1784},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, zu lesen},
}

@article{matheson_scoring_1976,
	title = {Scoring {Rules} for {Continuous} {Probability} {Distributions}},
	volume = {22},
	issn = {0025-1909, 1526-5501},
	url = {https://pubsonline.informs.org/doi/10.1287/mnsc.22.10.1087},
	doi = {10.1287/mnsc.22.10.1087},
	abstract = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.},
	language = {en},
	number = {10},
	urldate = {2025-03-18},
	journal = {Management Science},
	author = {Matheson, James E. and Winkler, Robert L.},
	month = jun,
	year = {1976},
	keywords = {zu lesen},
	pages = {1087--1096},
}

@article{matheron_principles_1963,
	title = {Principles of geostatistics},
	volume = {58},
	issn = {0361-0128},
	url = {https://doi.org/10.2113/gsecongeo.58.8.1246},
	doi = {10.2113/gsecongeo.58.8.1246},
	abstract = {Knowledge of ore grades and ore reserves as well as error estimation of these values, is fundamental for mining engineers and mining geologists. Until now no appropriate scientific approach to those estimation problems has existed: geostatistics, the principles of which are summarized in this paper, constitutes a new science leading to such an approach. The author criticizes classical statistical methods still in use, and shows some of the main results given by geostatistics. Any ore deposit evaluation as well as proper decision of starting mining operations should be preceded by a geostatistical investigation which may avoid economic failures.},
	number = {8},
	urldate = {2025-03-25},
	journal = {Economic Geology},
	author = {Matheron, Georges},
	month = dec,
	year = {1963},
	keywords = {zu lesen},
	pages = {1246--1266},
}

@inproceedings{meszaros_rome_2024,
	title = {{ROME}: {Robust} {Multi}-{Modal} {Density} {Estimator}},
	url = {https://doi.org/10.24963/ijcai.2024/525},
	doi = {10.24963/ijcai.2024/525},
	booktitle = {Proceedings of the {Thirty}-{Third} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI}-24},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Mészáros, Anna and Schumann, Julian F. and Alonso-Mora, Javier and Zgonnikov, Arkady and Kober, Jens},
	editor = {Larson, Kate},
	month = aug,
	year = {2024},
	keywords = {zu lesen},
	pages = {4751--4759},
}

@article{mehtab_time_2020,
	title = {A {Time} {Series} {Analysis}-{Based} {Stock} {Price} {Prediction} {Using} {Machine} {Learning} and {Deep} {Learning} {Models}},
	volume = {6},
	issn = {1744-6635, 1744-6643},
	url = {http://arxiv.org/abs/2004.11697},
	doi = {10.1504/IJBFMI.2020.115691},
	abstract = {Prediction of future movement of stock prices has always been a challenging task for the researchers. While the advocates of the efficient market hypothesis (EMH) believe that it is impossible to design any predictive framework that can accurately predict the movement of stock prices, there are seminal work in the literature that have clearly demonstrated that the seemingly random movement patterns in the time series of a stock price can be predicted with a high level of accuracy. Design of such predictive models requires choice of appropriate variables, right transformation methods of the variables, and tuning of the parameters of the models. In this work, we present a very robust and accurate framework of stock price prediction that consists of an agglomeration of statistical, machine learning and deep learning models. We use the daily stock price data, collected at five minutes interval of time, of a very well known company that is listed in the National Stock Exchange (NSE) of India. The granular data is aggregated into three slots in a day, and the aggregated data is used for building and training the forecasting models. We contend that the agglomerative approach of model building that uses a combination of statistical, machine learning, and deep learning approaches, can very effectively learn from the volatile and random movement patterns in a stock price data. We build eight classification and eight regression models based on statistical and machine learning approaches. In addition to these models, a deep learning regression model using a long-and-short-term memory (LSTM) network is also built. Extensive results have been presented on the performance of these models, and the results are critically analyzed.},
	number = {4},
	urldate = {2024-04-27},
	journal = {International Journal of Business Forecasting and Marketing Intelligence},
	author = {Mehtab, Sidra and Sen, Jaydip},
	year = {2020},
	keywords = {Computer Science - Machine Learning, Quantitative Finance - Statistical Finance, Statistics - Machine Learning, zu lesen},
	pages = {272},
}

@inproceedings{marcotte_regions_2023,
	title = {Regions of {Reliability} in the {Evaluation} of {Multivariate} {Probabilistic} {Forecasts}},
	url = {https://proceedings.mlr.press/v202/marcotte23a.html},
	abstract = {Multivariate probabilistic time series forecasts are commonly evaluated via proper scoring rules, i.e., functions that are minimal in expectation for the ground-truth distribution. However, this property is not sufficient to guarantee good discrimination in the non-asymptotic regime. In this paper, we provide the first systematic finite-sample study of proper scoring rules for time series forecasting evaluation. Through a power analysis, we identify the “region of reliability” of a scoring rule, i.e., the set of practical conditions where it can be relied on to identify forecasting errors. We carry out our analysis on a comprehensive synthetic benchmark, specifically designed to test several key discrepancies between ground-truth and forecast distributions, and we gauge the generalizability of our findings to real-world tasks with an application to an electricity production problem. Our results reveal critical shortcomings in the evaluation of multivariate probabilistic forecasts as commonly performed in the literature.},
	language = {en},
	urldate = {2025-03-19},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Marcotte, Etienne and Zantedeschi, Valentina and Drouin, Alexandre and Chapados, Nicolas},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {23958--24004},
}

@inproceedings{mangalam_goals_2021,
	title = {From {Goals}, {Waypoints} \& {Paths} to {Long} {Term} {Human} {Trajectory} {Forecasting}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Mangalam_From_Goals_Waypoints__Paths_to_Long_Term_Human_Trajectory_ICCV_2021_paper.html},
	language = {en},
	urldate = {2025-06-18},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Mangalam, Karttikeya and An, Yang and Girase, Harshayu and Malik, Jitendra},
	year = {2021},
	keywords = {zu lesen},
	pages = {15233--15242},
}

@inproceedings{macqueen_methods_1967,
	title = {Some methods for classification and analysis of multivariate observations},
	volume = {5},
	booktitle = {Proceedings of the {Fifth} {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}, {Volume} 1: {Statistics}},
	publisher = {University of California press},
	author = {MacQueen, James},
	year = {1967},
	keywords = {zu lesen},
	pages = {281--298},
}

@article{makridakis_m5_2022,
	title = {The {M5} uncertainty competition: {Results}, findings and conclusions},
	volume = {38},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207021001722},
	doi = {https://doi.org/10.1016/j.ijforecast.2021.10.009},
	abstract = {This paper describes the M5 “Uncertainty” competition, the second of two parallel challenges of the latest M competition, aiming to advance the theory and practice of forecasting. The particular objective of the M5 “Uncertainty” competition was to accurately forecast the uncertainty distributions of the realized values of 42,840 time series that represent the hierarchical unit sales of the largest retail company in the world by revenue, Walmart. To do so, the competition required the prediction of nine different quantiles (0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, and 0.995), that can sufficiently describe the complete distributions of future sales. The paper provides details on the implementation and execution of the M5 “Uncertainty” competition, presents its results and the top-performing methods, and summarizes its major findings and conclusions. Finally, it discusses the implications of its findings and suggests directions for future research.},
	number = {4},
	journal = {International Journal of Forecasting},
	author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios and Chen, Zhi and Gaba, Anil and Tsetlin, Ilia and Winkler, Robert L.},
	year = {2022},
	keywords = {Forecasting competitions, M competitions, Machine learning, Probabilistic forecasts, Retail sales forecasting, Time series, Uncertainty, zu lesen},
	pages = {1365--1385},
}

@inproceedings{mahalakshmi_survey_2016,
	title = {A survey on forecasting of time series data},
	url = {https://ieeexplore.ieee.org/abstract/document/7725358},
	doi = {10.1109/ICCTIDE.2016.7725358},
	abstract = {Time series analysis and forecasting future values has been a major research focus since years ago. Time series analysis and forecasting in time series data finds it significance in many applications such as business, stock market and exchange, weather, electricity demand, cost and usage of products such as fuels, electricity, etc. and in any kind of place that has specific seasonal or trendy changes with time. The forecasting of time series data provides the organization with useful information that is necessary for making important decisions. In this paper, a detailed survey of the various techniques applied for forecasting different types of time series dataset is provided. This survey covers the overall forecasting models, the algorithms used within the model and other optimization techniques used for better performance and accuracy. The various performance evaluation parameters used for evaluating the forecasting models are also discussed in this paper. This study gives the reader an idea about the various researches that take place within forecasting using the time series data.},
	urldate = {2025-04-17},
	booktitle = {2016 {International} {Conference} on {Computing} {Technologies} and {Intelligent} {Data} {Engineering} ({ICCTIDE}'16)},
	author = {Mahalakshmi, G. and Sridevi, S. and Rajaram, S.},
	month = jan,
	year = {2016},
	keywords = {Data mining, Data models, Forecasting, Load forecasting, Load modeling, Prediction, Predictive models, Temporal data mining, Time series, Time series analysis, zu lesen},
	pages = {1--8},
}

@book{madsen_time_2007,
	address = {New York},
	title = {Time {Series} {Analysis}},
	isbn = {978-0-429-19583-9},
	abstract = {With a focus on analyzing and modeling linear dynamic systems using statistical methods, Time Series Analysis formulates various linear models, discusses their theoretical characteristics, and explores the connections among stochastic dynamic models. Emphasizing the time domain description, the author presents theorems to highlight the most},
	publisher = {Chapman and Hall/CRC},
	author = {Madsen, Henrik},
	month = nov,
	year = {2007},
	doi = {10.1201/9781420059687},
	keywords = {zu lesen},
}

@incollection{lutkepohl_vector_2005,
	address = {Berlin, Heidelberg},
	title = {Vector {Autoregressive} {Moving} {Average} {Processes}},
	isbn = {978-3-540-27752-1},
	url = {https://doi.org/10.1007/978-3-540-27752-1_11},
	abstract = {In this chapter, we extend our standard finite order VAR model, \% MathType!MTEF!2!1!+-\% feaagaart1ev2aaatCvAUfeBSjuyZL2yd9gzLbvyNv2CaerbuLwBLn\% hiov2DGi1BTfMBaeXatLxBI9gBaerbd9wDYLwzYbItLDharqqtubsr\% 4rNCHbGeaGqiVu0Je9sqqrpepC0xbbL8F4rqqrFfpeea0xe9Lq-Jc9\% vqaqpepm0xbba9pwe9Q8fs0-yqaqpepae9pg0FirpepeKkFr0xfr-x\% fr-xb9adbaqaaeGaciGaaiaabeqaamaabaabaaGcbaGaamyEamaaBa\% aaleaacaWG0baabeaakiabg2da9iabe27aUjabgUcaRiaadgeadaWg\% aaWcbaGaaGymaaqabaGccaWG5bWaaSbaaSqaaiaadshacqGHsislca\% aIXaaabeaakiabgUcaRiablAciljabgUcaRiaadgeadaWgaaWcbaGa\% amiCaaqabaGccaWG5bWaaSbaaSqaaiaadshacqGHsislcaWGWbaabe\% aakiabgUcaRiabew7aLnaaBaaaleaacaWG0baabeaakiaacYcaaaa!4E94!\$\$y\_t  = {\textbackslash}nu  + A\_1 y\_\{t - 1\}  +  {\textbackslash}ldots  + A\_p y\_\{t - p\}  + {\textbackslash}varepsilon \_t ,\$\$by allowing the error terms, here εt, to be autocorrelated rather than white noise. The autocorrelation structure is assumed to be of a relatively simple type so that εt has a finite order moving average (MA) representation, \% MathType!MTEF!2!1!+-\% feaagaart1ev2aaatCvAUfeBSjuyZL2yd9gzLbvyNv2CaerbuLwBLn\% hiov2DGi1BTfMBaeXatLxBI9gBaerbd9wDYLwzYbItLDharqqtubsr\% 4rNCHbGeaGqiVu0Je9sqqrpepC0xbbL8F4rqqrFfpeea0xe9Lq-Jc9\% vqaqpepm0xbba9pwe9Q8fs0-yqaqpepae9pg0FirpepeKkFr0xfr-x\% fr-xb9adbaqaaeGaciGaaiaabeqaamaabaabaaGcbaGaeqyTdu2aaS\% baaSqaaiaadshaaeqaaOGaeyypa0JaamyDamaaBaaaleaacaWG0baa\% beaakiabgUcaRiaad2eadaWgaaWcbaGaaGymaaqabaGccaWG1bWaaS\% baaSqaaiaadshacqGHsislcaaIXaaabeaakiabgUcaRiablAciljab\% gUcaRiaad2eadaWgaaWcbaGaamyCaaqabaGccaWG1bWaaSbaaSqaai\% aadshacqGHsislcaWGXbaabeaakiaacYcaaaa!4C08!\$\${\textbackslash}varepsilon \_t  = u\_t  + M\_1 u\_\{t - 1\}  +  {\textbackslash}ldots  + M\_q u\_\{t - q\} ,\$\$where, as usual, utis zero mean white noise with nonsingular covariance matrix Σu. A finite order VAR process with finite order MA error term is called a VARMA (vector autoregressive moving average) process.},
	language = {en},
	urldate = {2025-05-28},
	booktitle = {New {Introduction} to {Multiple} {Time} {Series} {Analysis}},
	publisher = {Springer},
	author = {Lütkepohl, Helmut},
	editor = {Lütkepohl, Helmut},
	year = {2005},
	doi = {10.1007/978-3-540-27752-1_11},
	keywords = {zu lesen},
	pages = {419--446},
}

@article{machete_contrasting_2013,
	title = {Contrasting probabilistic scoring rules},
	volume = {143},
	issn = {0378-3758},
	url = {https://www.sciencedirect.com/science/article/pii/S0378375813001158},
	doi = {10.1016/j.jspi.2013.05.012},
	abstract = {There are several scoring rules that one can choose from in order to score probabilistic forecasting models or estimate model parameters. Whilst it is generally agreed that proper scoring rules are preferable, there is no clear criterion for preferring one proper scoring rule above another. This manuscript compares and contrasts some commonly used proper scoring rules and provides guidance on scoring rule selection. In particular, it is shown that the logarithmic scoring rule prefers erring with more uncertainty, the spherical scoring rule prefers erring with lower uncertainty, whereas the other scoring rules are indifferent to either option.},
	number = {10},
	urldate = {2025-03-18},
	journal = {Journal of Statistical Planning and Inference},
	author = {Machete, Reason L.},
	month = oct,
	year = {2013},
	keywords = {Estimation, Forecast evaluation, Probabilistic forecasting, Utility function, zu lesen},
	pages = {1781--1790},
}

@inproceedings{luo_energy-calibrated_2025,
	address = {Cham},
	title = {Energy-{Calibrated} {VAE} with {Test} {Time} {Free} {Lunch}},
	isbn = {978-3-031-73013-9},
	doi = {10.1007/978-3-031-73013-9_19},
	abstract = {In this paper, we propose a novel generative model that utilizes a conditional Energy-Based Model (EBM) for enhancing Variational Autoencoder (VAE), termed Energy-Calibrated VAE (EC-VAE). Specifically, VAEs often suffer from blurry generated samples due to the lack of a tailored training on the samples generated in the generative direction. On the other hand, EBMs can generate high-quality samples but require expensive Markov Chain Monte Carlo (MCMC) sampling. To address these issues, we introduce a conditional EBM for calibrating the generative direction of VAE during training, without requiring it for the generation at test time. In particular, we train EC-VAE upon both the input data and the calibrated samples with adaptive weight to enhance efficacy while avoiding MCMC sampling at test time. Furthermore, we extend the calibration idea of EC-VAE to variational learning and normalizing flows, and apply EC-VAE to an additional application of zero-shot image restoration via neural transport prior and range-null theory. We evaluate the proposed method with two applications, including image generation and zero-shot image restoration, and the experimental results show that our method achieves competitive performance over single-step non-adversarial generation.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Luo, Yihong and Qiu, Siya and Tao, Xingjian and Cai, Yujun and Tang, Jing},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	year = {2025},
	keywords = {zu lesen},
	pages = {326--344},
}

@inproceedings{lu_cats_2024,
	title = {{CATS}: {Enhancing} {Multivariate} {Time} {Series} {Forecasting} by {Constructing} {Auxiliary} {Time} {Series} as {Exogenous} {Variables}},
	shorttitle = {{CATS}},
	url = {https://proceedings.mlr.press/v235/lu24d.html},
	abstract = {For Multivariate Time Series Forecasting (MTSF), recent deep learning applications show that univariate models frequently outperform multivariate ones. To address the deficiency in multivariate models, we introduce a method to Construct Auxiliary Time Series (CATS) that functions like a 2D temporal-contextual attention mechanism, which generates Auxiliary Time Series (ATS) from Original Time Series (OTS) to effectively represent and incorporate inter-series relationships for forecasting. Key principles of ATS—continuity, sparsity, and variability—are identified and implemented through different modules. Even with a basic 2-layer MLP as the core predictor, CATS achieves state-of-the-art, significantly reducing complexity and parameters compared to previous multivariate models, marking it as an efficient and transferable MTSF solution.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lu, Jiecheng and Han, Xu and Sun, Yan and Yang, Shihao},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {32990--33006},
}

@article{loftsgaarden_nonparametric_1965,
	title = {A {Nonparametric} {Estimate} of a {Multivariate} {Density} {Function}},
	volume = {36},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-36/issue-3/A-Nonparametric-Estimate-of-a-Multivariate-Density-Function/10.1214/aoms/1177700079.full},
	doi = {10.1214/aoms/1177700079},
	abstract = {Let \$x\_1, {\textbackslash}cdots, x\_n\$ be independent observations on a \$p\$-dimensional random variable \$X = (X\_1, {\textbackslash}cdots, X\_p)\$ with absolutely continuous distribution function \$F(x\_1, {\textbackslash}cdots, x\_p)\$. An observation \$x\_i\$ on \$X\$ is \$x\_i = (x\_\{1i\}, {\textbackslash}cdots, x\_\{pi\})\$. The problem considered here is the estimation of the probability density function \$f(x\_1, {\textbackslash}cdots, x\_p)\$ at a point \$z = (z\_1, {\textbackslash}cdots, z\_p)\$ where \$f\$ is positive and continuous. An estimator is proposed and consistency is shown. The problem of estimating a probability density function has only recently begun to receive attention in the literature. Several authors [Rosenblatt (1956), Whittle (1958), Parzen (1962), and Watson and Leadbetter (1963)] have considered estimating a univariate density function. In addition, Fix and Hodges (1951) were concerned with density estimation in connection with nonparametric discrimination. Cacoullos (1964) generalized Parzen's work to the multivariate case. The work in this paper arose out of work on the nonparametric discrimination problem.},
	number = {3},
	urldate = {2025-06-29},
	journal = {The Annals of Mathematical Statistics},
	author = {Loftsgaarden, D. O. and Quesenberry, C. P.},
	month = jun,
	year = {1965},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {zu lesen},
	pages = {1049--1051},
}

@article{long_forecasting_2023,
	title = {Forecasting the {Monkeypox} {Outbreak} {Using} {ARIMA}, {Prophet}, {NeuralProphet}, and {LSTM} {Models} in the {United} {States}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2571-9394},
	url = {https://www.mdpi.com/2571-9394/5/1/5},
	doi = {10.3390/forecast5010005},
	abstract = {Since May 2022, over 64,000 Monkeypox cases have been confirmed globally up until September 2022. The United States leads the world in cases, with over 25,000 cases nationally. This recent escalation of the Monkeypox outbreak has become a severe and urgent worldwide public health concern. We aimed to develop an efficient forecasting tool that allows health experts to implement effective prevention policies for Monkeypox and shed light on the case development of diseases that share similar characteristics to Monkeypox. This research utilized five machine learning models, namely, ARIMA, LSTM, Prophet, NeuralProphet, and a stacking model, on the Monkeypox datasets from the CDC official website to forecast the next 7-day trend of Monkeypox cases in the United States. The result showed that NeuralProphet achieved the most optimal performance with a RMSE of 49.27 and R2 of 0.76. Further, the final trained NeuralProphet was employed to forecast seven days of out-of-sample cases. On the basis of cases, our model demonstrated 95\% accuracy.},
	language = {en},
	number = {1},
	urldate = {2025-05-29},
	journal = {Forecasting},
	author = {Long, Bowen and Tan, Fangya and Newman, Mark},
	month = mar,
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {ARIMA, LSTM, Monkeypox, NeuralProphet, Prophet, forecasting, zu lesen},
	pages = {127--137},
}

@inproceedings{liu_pay_2021,
	title = {Pay {Attention} to {MLPs}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/4cc05b35c2f937c5bd9e7d41d3686fff-Abstract.html},
	abstract = {Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based solely on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.},
	urldate = {2025-05-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Hanxiao and Dai, Zihang and So, David and Le, Quoc V},
	year = {2021},
	keywords = {zu lesen},
	pages = {9204--9215},
}

@inproceedings{liu_itransformer_2023,
	title = {{iTransformer}: {Inverted} {Transformers} {Are} {Effective} for {Time} {Series} {Forecasting}},
	shorttitle = {{iTransformer}},
	url = {https://openreview.net/forum?id=JePfAI8fah},
	abstract = {The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting. Code is available at this repository: https://github.com/thuml/iTransformer.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
}

@inproceedings{liu_deep_2024,
	title = {Deep {Functional} {Factor} {Models}: {Forecasting} {High}-{Dimensional} {Functional} {Time} {Series} via {Bayesian} {Nonparametric} {Factorization}},
	shorttitle = {Deep {Functional} {Factor} {Models}},
	url = {https://proceedings.mlr.press/v235/liu24aw.html},
	abstract = {This paper introduces the Deep Functional Factor Model (DF2M), a Bayesian nonparametric model designed for analysis of high-dimensional functional time series. DF2M is built upon the Indian Buffet Process and the multi-task Gaussian Process, incorporating a deep kernel function that captures non-Markovian and nonlinear temporal dynamics. Unlike many black-box deep learning models, DF2M offers an explainable approach to utilizing neural networks by constructing a factor model and integrating deep neural networks within the kernel function. Additionally, we develop a computationally efficient variational inference algorithm to infer DF2M. Empirical results from four real-world datasets demonstrate that DF2M provides better explainability and superior predictive accuracy compared to conventional deep learning models for high-dimensional functional time series.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liu, Yirui and Qiao, Xinghao and Pei, Yulong and Wang, Liying},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {31709--31727},
}

@inproceedings{liu_pyraformer_2021,
	title = {Pyraformer: {Low}-{Complexity} {Pyramidal} {Attention} for {Long}-{Range} {Time} {Series} {Modeling} and {Forecasting}},
	shorttitle = {Pyraformer},
	url = {https://openreview.net/forum?id=0EXmFzUn5I},
	abstract = {Accurate prediction of the future given the past based on time series data is of paramount importance, since it opens the door for decision making and risk management ahead of time. In practice, the challenge is to build a flexible but parsimonious model that can capture a wide range of temporal dependencies. In this paper, we propose Pyraformer by exploring the multiresolution representation of the time series. Specifically, we introduce the pyramidal attention module (PAM) in which the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighboring connections model the temporal dependencies of different ranges. Under mild conditions, the maximum length of the signal traversing path in Pyraformer is a constant (i.e., \${\textbackslash}mathcal O(1)\$) with regard to the sequence length \$L\$, while its time and space complexity scale linearly with \$L\$. Extensive numerical results show that Pyraformer typically achieves the highest prediction accuracy in both single-step and long-range forecasting tasks with the least amount of time and memory consumption, especially when the sequence is long.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Tenth} {International} {Conference} on {Learning} {Representations}},
	author = {Liu, Shizhan and Yu, Hang and Liao, Cong and Li, Jianguo and Lin, Weiyao and Liu, Alex X. and Dustdar, Schahram},
	month = oct,
	year = {2021},
	keywords = {zu lesen},
}

@inproceedings{lin_feature_2017,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = jul,
	year = {2017},
	keywords = {zu lesen},
}

@inproceedings{lin_sparsetsf_2024,
	title = {{SparseTSF}: {Modeling} {Long}-term {Time} {Series} {Forecasting} with *1k* {Parameters}},
	shorttitle = {{SparseTSF}},
	url = {https://proceedings.mlr.press/v235/lin24n.html},
	abstract = {This paper introduces SparseTSF, a novel, extremely lightweight model for Long-term Time Series Forecasting (LTSF), designed to address the challenges of modeling complex temporal dependencies over extended horizons with minimal computational resources. At the heart of SparseTSF lies the Cross-Period Sparse Forecasting technique, which simplifies the forecasting task by decoupling the periodicity and trend in time series data. This technique involves downsampling the original sequences to focus on cross-period trend prediction, effectively extracting periodic features while minimizing the model’s complexity and parameter count. Based on this technique, the SparseTSF model uses fewer than 1k parameters to achieve competitive or superior performance compared to state-of-the-art models. Furthermore, SparseTSF showcases remarkable generalization capabilities, making it well-suited for scenarios with limited computational resources, small samples, or low-quality data. The code is publicly available at this repository: https://github.com/lss-1138/SparseTSF.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lin, Shengsheng and Lin, Weiwei and Wu, Wentai and Chen, Haojun and Yang, Junjie},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {30211--30226},
}

@misc{lin_segrnn_2023,
	title = {{SegRNN}: {Segment} {Recurrent} {Neural} {Network} for {Long}-{Term} {Time} {Series} {Forecasting}},
	shorttitle = {{SegRNN}},
	url = {http://arxiv.org/abs/2308.11200},
	doi = {10.48550/arXiv.2308.11200},
	abstract = {RNN-based methods have faced challenges in the Long-term Time Series Forecasting (LTSF) domain when dealing with excessively long look-back windows and forecast horizons. Consequently, the dominance in this domain has shifted towards Transformer, MLP, and CNN approaches. The substantial number of recurrent iterations are the fundamental reasons behind the limitations of RNNs in LTSF. To address these issues, we propose two novel strategies to reduce the number of iterations in RNNs for LTSF tasks: Segment-wise Iterations and Parallel Multi-step Forecasting (PMF). RNNs that combine these strategies, namely SegRNN, significantly reduce the required recurrent iterations for LTSF, resulting in notable improvements in forecast accuracy and inference speed. Extensive experiments demonstrate that SegRNN not only outperforms SOTA Transformer-based models but also reduces runtime and memory usage by more than 78\%. These achievements provide strong evidence that RNNs continue to excel in LTSF tasks and encourage further exploration of this domain with more RNN-based approaches. The source code is coming soon.},
	urldate = {2025-04-15},
	publisher = {arXiv},
	author = {Lin, Shengsheng and Lin, Weiwei and Wu, Wentai and Zhao, Feiyu and Mo, Ruichao and Zhang, Haotong},
	month = aug,
	year = {2023},
	note = {arXiv:2308.11200},
	keywords = {Computer Science - Machine Learning, zu lesen},
}

@article{lim_time-series_2021,
	title = {Time-series forecasting with deep learning: a survey},
	volume = {379},
	shorttitle = {Time-series forecasting with deep learning},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsta.2020.0209},
	doi = {10.1098/rsta.2020.0209},
	abstract = {Numerous deep learning architectures have been developed to accommodate the diversity of time-series datasets across different domains. In this article, we survey common encoder and decoder designs used in both one-step-ahead and multi-horizon time-series forecasting—describing how temporal information is incorporated into predictions by each model. Next, we highlight recent developments in hybrid deep learning models, which combine well-studied statistical models with neural network components to improve pure methods in either category. Lastly, we outline some ways in which deep learning can also facilitate decision support with time-series data.

This article is part of the theme issue ‘Machine learning for weather and climate modelling’.},
	number = {2194},
	urldate = {2024-12-22},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Lim, Bryan and Zohren, Stefan},
	month = feb,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {counterfactual prediction, deep neural networks, hybrid models, interpretability, time-series forecasting, uncertainty estimation, zu lesen},
	pages = {20200209},
}

@article{lim_temporal_2021,
	title = {Temporal {Fusion} {Transformers} for interpretable multi-horizon time series forecasting},
	volume = {37},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207021000637},
	doi = {10.1016/j.ijforecast.2021.03.012},
	abstract = {Multi-horizon forecasting often contains a complex mix of inputs – including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed in the past – without any prior information on how they interact with the target. Several deep learning methods have been proposed, but they are typically ‘black-box’ models that do not shed light on how they use the full range of inputs present in practical scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) – a novel attention-based architecture that combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, TFT uses recurrent layers for local processing and interpretable self-attention layers for long-term dependencies. TFT utilizes specialized components to select relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of scenarios. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and highlight three practical interpretability use cases of TFT.},
	number = {4},
	urldate = {2025-06-02},
	journal = {International Journal of Forecasting},
	author = {Lim, Bryan and Arık, Sercan Ö. and Loeff, Nicolas and Pfister, Tomas},
	month = oct,
	year = {2021},
	keywords = {Attention mechanisms, Deep learning, Explainable AI, Interpretability, Multi-horizon forecasting, Time series, zu lesen},
	pages = {1748--1764},
}

@inproceedings{li_diffusion_2018,
	title = {Diffusion {Convolutional} {Recurrent} {Neural} {Network}: {Data}-{Driven} {Traffic} {Forecasting}},
	url = {https://openreview.net/forum?id=SJiHXGWAZ},
	booktitle = {Proceedings of the {Sixth} {International} {Conference} on {Learning} {Representations}},
	author = {Li, Yaguang and Yu, Rose and Shahabi, Cyrus and Liu, Yan},
	year = {2018},
	keywords = {zu lesen},
}

@inproceedings{li_transformer-modulated_2023,
	title = {Transformer-{Modulated} {Diffusion} {Models} for {Probabilistic} {Multivariate} {Time} {Series} {Forecasting}},
	url = {https://openreview.net/forum?id=qae04YACHs},
	abstract = {Transformers have gained widespread usage in multivariate time series (MTS) forecasting, delivering impressive performance. Nonetheless, these existing transformer-based methods often neglect an essential aspect: the incorporation of uncertainty into the predicted series, which holds significant value in decision-making. In this paper, we introduce a Transformer-Modulated Diffusion Model (TMDM), uniting conditional diffusion generative process with transformers into a unified framework to enable precise distribution forecasting for MTS. TMDM harnesses the power of transformers to extract essential insights from historical time series data. This information is then utilized as prior knowledge, capturing covariate-dependence in both the forward and reverse processes within the diffusion model. Furthermore, we seamlessly integrate well-designed transformer-based forecasting methods into TMDM to enhance its overall performance. Additionally, we introduce two novel metrics for evaluating uncertainty estimation performance. Through extensive experiments on six datasets using four evaluation metrics, we establish the effectiveness of TMDM in probabilistic MTS forecasting.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Ninth} the {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Li, Yuxin and Chen, Wenchao and Hu, Xinyue and Chen, Bo and Sun, Baolin and Zhou, Mingyuan},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
}

@inproceedings{li_smartformer_2023,
	title = {{SMARTformer}: {Semi}-{Autoregressive} {Transformer} with {Efficient} {Integrated} {Window} {Attention} for {Long} {Time} {Series} {Forecasting}},
	volume = {3},
	shorttitle = {{SMARTformer}},
	url = {https://www.ijcai.org/proceedings/2023/241},
	doi = {10.24963/ijcai.2023/241},
	abstract = {Electronic proceedings of IJCAI 2023},
	language = {en},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Yiduo and Qi, Shiyi and Li, Zhe and Rao, Zhongwen and Pan, Lujia and Xu, Zenglin},
	month = aug,
	year = {2023},
	note = {ISSN: 1045-0823},
	keywords = {zu lesen},
	pages = {2169--2177},
}

@inproceedings{li_enhancing_2019,
	title = {Enhancing the {Locality} and {Breaking} the {Memory} {Bottleneck} of {Transformer} on {Time} {Series} {Forecasting}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html},
	abstract = {Time series forecasting is an important problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. In this paper, we propose to tackle such forecasting problem with Transformer. Although impressed by its performance in our preliminary study, we found its two major weaknesses: (1) locality-agnostics: the point-wise dot- product self-attention in canonical Transformer architecture is insensitive to local context, which can make the model prone to anomalies in time series; (2) memory bottleneck: space complexity of canonical Transformer grows quadratically with sequence length L, making directly modeling long time series infeasible. In order to solve these two issues, we first propose convolutional self-attention by producing queries and keys with causal convolution so that local context can be better incorporated into attention mechanism. Then, we propose LogSparse Transformer with only O(L(log L){\textasciicircum}2) memory cost, improving forecasting accuracy for time series with fine granularity and strong long-term dependencies under constrained memory budget. Our experiments on both synthetic data and real- world datasets show that it compares favorably to the state-of-the-art.},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
	year = {2019},
	keywords = {zu lesen},
}

@misc{lecun_mnist_1998,
	title = {The {MNIST} {Database} of {Handwritten} {Digits}},
	url = {http://yann.lecun.com/exdb/mnist/},
	author = {LeCun, Yann and Cortes, Corinna and Burges, Christopher JC},
	year = {1998},
	note = {URL: http://yann.lecun.com/exdb/mnist/},
	keywords = {zu lesen},
}

@article{li_time_2020,
	title = {Time {Series} {Clustering} {Model} based on {DTW} for {Classifying} {Car} {Parks}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-4893},
	url = {https://www.mdpi.com/1999-4893/13/3/57},
	doi = {10.3390/a13030057},
	abstract = {An increasing number of automobiles have led to a serious shortage of parking spaces and a serious imbalance of parking supply and demand. The best way to solve these problems is to achieve the reasonable planning and classify management of car parks, guide the intelligent parking, and then promote its marketization and industrialization. Therefore, we aim to adopt clustering method to classify car parks. Owing to the time series characteristics of car park data, a time series clustering framework, including preprocessing, distance measurement, clustering and evaluation, is first developed for classifying car parks. Then, in view of the randomness of existing clustering models, a new time series clustering model based on dynamic time warping (DTW) is proposed, which contains distance radius calculation, obtaining density of the neighbor area, k centers initialization, and clustering. Finally, some UCR datasets and data of 27 car parks are employed to evaluate the performance of the models and results show that the proposed model performs obviously better results than those clustering models based on Euclidean distance (ED) and traditional clustering models based on DTW.},
	language = {en},
	number = {3},
	urldate = {2025-07-01},
	journal = {Algorithms},
	author = {Li, Taoying and Wu, Xu and Zhang, Junhe},
	month = mar,
	year = {2020},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Euclidean distance, car park, dynamic time warping, time series clustering, zu lesen},
	pages = {57},
}

@article{li_hyperband_2018,
	title = {Hyperband: {A} {Novel} {Bandit}-{Based} {Approach} to {Hyperparameter} {Optimization}},
	volume = {18},
	url = {http://jmlr.org/papers/v18/16-558.html},
	number = {185},
	journal = {Journal of Machine Learning Research},
	author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year = {2018},
	keywords = {zu lesen},
	pages = {1--52},
}

@article{lecun_tutorial_2006,
	title = {A {Tutorial} on {Energy}-{Based} {Learning}},
	abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each conﬁguration of the variables. Inference consists in clamping the value of observed variables and ﬁnding conﬁgurations of the remaining variables that minimize the energy. Learning consists in ﬁnding an energy function in which observed conﬁgurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random ﬁelds, maximum margin Markov networks, and several manifold learning methods.},
	language = {en},
	journal = {Predicting structured data},
	author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc’Aurelio and Huang, Fu Jie},
	editor = {Bakir, G and Hofman, T and Schölkopf, B and Smola, A and Taskar, B},
	month = aug,
	year = {2006},
	keywords = {zu lesen},
}

@article{lara-benitez_experimental_2021,
	title = {An {Experimental} {Review} on {Deep} {Learning} {Architectures} for {Time} {Series} {Forecasting}},
	volume = {31},
	issn = {0129-0657, 1793-6462},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0129065721300011},
	doi = {10.1142/S0129065721300011},
	abstract = {In recent years, deep learning techniques have outperformed traditional models in many machine learning tasks. Deep neural networks have successfully been applied to address time series forecasting problems, which is a very important topic in data mining. They have proved to be an effective solution given their capacity to automatically learn the temporal dependencies present in time series. However, selecting the most convenient type of deep neural network and its parametrization is a complex task that requires considerable expertise. Therefore, there is a need for deeper studies on the suitability of all existing architectures for different forecasting tasks. In this work, we face two main challenges: a comprehensive review of the latest works using deep learning for time series forecasting and an experimental study comparing the performance of the most popular architectures. The comparison involves a thorough analysis of seven types of deep learning models in terms of accuracy and efficiency. We evaluate the rankings and distribution of results obtained with the proposed models under many different architecture configurations and training hyperparameters. The datasets used comprise more than 50,000 time series divided into 12 different forecasting problems. By training more than 38,000 models on these data, we provide the most extensive deep learning study for time series forecasting. Among all studied models, the results show that long short-term memory (LSTM) and convolutional networks (CNN) are the best alternatives, with LSTMs obtaining the most accurate forecasts. CNNs achieve comparable performance with less variability of results under different parameter configurations, while also being more efficient.},
	language = {en},
	number = {03},
	urldate = {2023-06-01},
	journal = {International Journal of Neural Systems},
	author = {Lara-Benítez, Pedro and Carranza-García, Manuel and Riquelme, José C.},
	month = mar,
	year = {2021},
	keywords = {zu lesen},
	pages = {2130001},
}

@inproceedings{lambert_eliciting_2008,
	address = {New York, NY, USA},
	series = {{EC} '08},
	title = {Eliciting properties of probability distributions},
	isbn = {978-1-60558-169-9},
	url = {https://doi.org/10.1145/1386790.1386813},
	doi = {10.1145/1386790.1386813},
	abstract = {We investigate the problem of truthfully eliciting an expert's assessment of a property of a probability distribution, where a property is any real-valued function of the distribution such as mean or variance. We show that not all properties are elicitable; for example, the mean is elicitable and the variance is not. For those that are elicitable, we provide a representation theorem characterizing all payment (or "score") functions that induce truthful revelation. We also consider the elicitation of sets of properties. We then observe that properties can always be inferred from sets of elicitable properties. This naturally suggests the concept of elicitation complexity; the elicitation complexity of property is the minimal size of such a set implying the property. Finally we discuss applications to prediction markets.},
	urldate = {2025-04-03},
	booktitle = {Proceedings of the 9th {ACM} conference on {Electronic} commerce},
	publisher = {Association for Computing Machinery},
	author = {Lambert, Nicolas S. and Pennock, David M. and Shoham, Yoav},
	month = jul,
	year = {2008},
	keywords = {zu lesen},
	pages = {129--138},
}

@inproceedings{lai_modeling_2018,
	address = {New York, NY, USA},
	series = {{SIGIR} '18},
	title = {Modeling {Long}- and {Short}-{Term} {Temporal} {Patterns} with {Deep} {Neural} {Networks}},
	isbn = {978-1-4503-5657-2},
	url = {https://dl.acm.org/doi/10.1145/3209978.3210006},
	doi = {10.1145/3209978.3210006},
	abstract = {Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) and the Recurrent Neural Network (RNN) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. Furthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. All the data and experiment codes are available online.},
	urldate = {2024-05-24},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
	month = jun,
	year = {2018},
	keywords = {autoregressive models, multivariate time series, neural network, zu lesen},
	pages = {95--104},
}

@book{kullback_information_1997,
	address = {New York, NY, USA},
	title = {Information {Theory} and {Statistics}},
	isbn = {978-0-486-69684-3},
	language = {en},
	publisher = {Dover Publications, Inc},
	author = {Kullback, Solomon},
	year = {1997},
	keywords = {zu lesen},
}

@article{kullback_information_1951,
	title = {On {Information} and {Sufficiency}},
	volume = {22},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2236703},
	number = {1},
	urldate = {2025-06-09},
	journal = {The Annals of Mathematical Statistics},
	author = {Kullback, S. and Leibler, R. A.},
	year = {1951},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {zu lesen},
	pages = {79--86},
}

@article{koopman_hamiltonian_1931,
	title = {Hamiltonian {Systems} and {Transformation} in {Hilbert} {Space}},
	volume = {17},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.17.5.315},
	doi = {10.1073/pnas.17.5.315},
	number = {5},
	urldate = {2025-04-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Koopman, B. O.},
	month = may,
	year = {1931},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	keywords = {zu lesen},
	pages = {315--318},
}

@article{koochali_if_2021,
	title = {If {You} {Like} {It}, {GAN} {It}—{Probabilistic} {Multivariate} {Times} {Series} {Forecast} with {GAN}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2673-4591},
	url = {https://www.mdpi.com/2673-4591/5/1/40},
	doi = {10.3390/engproc2021005040},
	abstract = {The contribution of this paper is two-fold. First, we present ProbCast—a novel probabilistic model for multivariate time-series forecasting. We employ a conditional GAN framework to train our model with adversarial training. Second, we propose a framework that lets us transform a deterministic model into a probabilistic one with improved performance. The motivation of the framework is to either transform existing highly accurate point forecast models to their probabilistic counterparts or to train GANs stably by selecting the architecture of GAN’s component carefully and efficiently. We conduct experiments over two publicly available datasets—an electricity consumption dataset and an exchange-rate dataset. The results of the experiments demonstrate the remarkable performance of our model as well as the successful application of our proposed framework.},
	language = {en},
	number = {1},
	urldate = {2025-01-08},
	journal = {Engineering Proceedings},
	author = {Koochali, Alireza and Dengel, Andreas and Ahmed, Sheraz},
	year = {2021},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {forecasting, generative adversarial networks, prediction, probabilistic, time-series, zu lesen},
}

@inproceedings{kong_diffwave_2020,
	title = {{DiffWave}: {A} {Versatile} {Diffusion} {Model} for {Audio} {Synthesis}},
	shorttitle = {{DiffWave}},
	url = {https://openreview.net/forum?id=a-xFK8Ymz5J},
	abstract = {In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
	language = {en},
	urldate = {2025-06-09},
	booktitle = {Proceedings of the {Ninth} {International} {Conference} on {Learning} {Representations}},
	author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
	month = oct,
	year = {2020},
	keywords = {zu lesen},
}

@article{kolassa_why_2020,
	series = {M4 {Competition}},
	title = {Why the “best” point forecast depends on the error or accuracy measure},
	volume = {36},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207019301359},
	doi = {10.1016/j.ijforecast.2019.02.017},
	number = {1},
	urldate = {2025-05-28},
	journal = {International Journal of Forecasting},
	author = {Kolassa, Stephan},
	month = jan,
	year = {2020},
	keywords = {zu lesen},
	pages = {208--211},
}

@article{koochali_random_2022,
	title = {Random {Noise} vs. {State}-of-the-{Art} {Probabilistic} {Forecasting} {Methods}: {A} {Case} {Study} on {CRPS}-{Sum} {Discrimination} {Ability}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {Random {Noise} vs. {State}-of-the-{Art} {Probabilistic} {Forecasting} {Methods}},
	url = {https://www.mdpi.com/2076-3417/12/10/5104},
	doi = {10.3390/app12105104},
	abstract = {The recent developments in the machine-learning domain have enabled the development of complex multivariate probabilistic forecasting models. To evaluate the predictive power of these complex methods, it is pivotal to have a precise evaluation method to gauge the performance and predictability power of these complex methods. To do so, several evaluation metrics have been proposed in the past (such as the energy score, Dawid–Sebastiani score, and variogram score); however, these cannot reliably measure the performance of a probabilistic forecaster. Recently, CRPS-Sum has gained a lot of prominence as a reliable metric for multivariate probabilistic forecasting. This paper presents a systematic evaluation of CRPS-Sum to understand its discrimination ability. We show that the statistical properties of target data affect the discrimination ability of CRPS-Sum. Furthermore, we highlight that CRPS-Sum calculation overlooks the performance of the model on each dimension. These flaws can lead us to an incorrect assessment of model performance. Finally, with experiments on real-world datasets, we demonstrate that the shortcomings of CRPS-Sum provide a misleading indication of the probabilistic forecasting performance method. We illustrate that it is easily possible to have a better CRPS-Sum for a dummy model, which looks like random noise, in comparison to the state-of-the-art method.},
	language = {en},
	number = {10},
	urldate = {2025-03-20},
	journal = {Applied Sciences},
	author = {Koochali, Alireza and Schichtel, Peter and Dengel, Andreas and Ahmed, Sheraz},
	month = jan,
	year = {2022},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {assessment, probabilistic forecasting, time-series analysis, zu lesen},
	pages = {5104},
}

@incollection{kline_methods_2004,
	title = {Methods for {Multi}-{Step} {Time} {Series} {Forecasting} {Neural} {Networks}},
	copyright = {Access limited to members},
	isbn = {978-1-59140-176-6},
	url = {https://www.igi-global.com/chapter/methods-multi-step-time-series/www.igi-global.com/chapter/methods-multi-step-time-series/27253},
	abstract = {In this study, we examine two methods for Multi-Step forecasting with neural networks: the Joint Method and the Independent Method. A subset of the M-3 Competition quarterly data series is used for the study. The methods are compared to each other, to a neural network Iterative Method, and to a base...},
	language = {en},
	urldate = {2025-04-23},
	booktitle = {Neural {Networks} in {Business} {Forecasting}},
	publisher = {IGI Global Scientific Publishing},
	author = {Kline, Douglas M.},
	year = {2004},
	doi = {10.4018/978-1-59140-176-6.ch012},
	keywords = {zu lesen},
	pages = {226--250},
}

@article{koenker_regression_1978,
	title = {Regression {Quantiles}},
	volume = {46},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/1913643},
	doi = {10.2307/1913643},
	abstract = {A simple minimization problem yielding the ordinary sample quantiles in the location model is shown to generalize naturally to the linear model generating a new class of statistics we term "regression quantiles." The estimator which minimizes the sum of absolute residuals is an important special case. Some equivariance properties and the joint asymptotic distribution of regression quantiles are established. These results permit a natural generalization of the linear model of certain well-known robust estimators of location. Estimators are suggested, which have comparable efficiency to least squares for Gaussian linear models while substantially out-performing the least-squares estimator over a wide class of non-Gaussian error distributions.},
	number = {1},
	urldate = {2025-03-18},
	journal = {Econometrica},
	author = {Koenker, Roger and Bassett, Gilbert},
	year = {1978},
	note = {Publisher: [Wiley, Econometric Society]},
	keywords = {zu lesen},
	pages = {33--50},
}

@article{ko_learning_2011,
	title = {Learning {GP}-{BayesFilters} via {Gaussian} process latent variable models},
	volume = {30},
	issn = {1573-7527},
	url = {https://doi.org/10.1007/s10514-010-9213-0},
	doi = {10.1007/s10514-010-9213-0},
	abstract = {GP-BayesFilters are a general framework for integrating Gaussian process prediction and observation models into Bayesian filtering techniques, including particle filters and extended and unscented Kalman filters. GP-BayesFilters have been shown to be extremely well suited for systems for which accurate parametric models are difficult to obtain. GP-BayesFilters learn non-parametric models from training data containing sequences of control inputs, observations, and ground truth states. The need for ground truth states limits the applicability of GP-BayesFilters to systems for which the ground truth can be estimated without significant overhead. In this paper we introduce GPBF-Learn, a framework for training GP-BayesFilters without ground truth states. Our approach extends Gaussian Process Latent Variable Models to the setting of dynamical robotics systems. We show how weak labels for the ground truth states can be incorporated into the GPBF-Learn framework. The approach is evaluated using a difficult tracking task, namely tracking a slotcar based on inertial measurement unit (IMU) observations only. We also show some special features enabled by this framework, including time alignment, and control replay for both the slotcar, and a robotic arm.},
	language = {en},
	number = {1},
	urldate = {2025-06-11},
	journal = {Autonomous Robots},
	author = {Ko, Jonathan and Fox, Dieter},
	month = jan,
	year = {2011},
	keywords = {Bayesian Inference, Bayesian Network, Bayesian filtering, Gaussian process, Learning Theory, Learning algorithms, Machine Learning, Machine learning, Statistical Learning, System control, System identification, Time alignment, zu lesen},
	pages = {3--23},
}

@inproceedings{kingma_auto-encoding_2014,
	address = {Banff, AB, Canada},
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Learning} {Representations}},
	author = {Kingma, Diederik P. and Welling, Max},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2014},
	keywords = {zu lesen},
}

@inproceedings{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	booktitle = {Proceedings of the {Third} {International} {Conference} on {Learning} {Representations}},
	author = {Kingma, Diederik and Ba, Jimmy},
	month = dec,
	year = {2014},
	keywords = {zu lesen},
}

@inproceedings{kim_reversible_2021,
	title = {Reversible {Instance} {Normalization} for {Accurate} {Time}-{Series} {Forecasting} against {Distribution} {Shift}},
	url = {https://openreview.net/forum?id=cGDAkQo1C0p},
	abstract = {Statistical properties such as mean and variance often change over time in time series, i.e., time-series data suffer from a distribution shift problem. This change in temporal distribution is one of the main challenges that prevent accurate time-series forecasting. To address this issue, we propose a simple yet effective normalization method called reversible instance normalization (RevIN), a generally-applicable normalization-and-denormalization method with learnable affine transformation. The proposed method is symmetrically structured to remove and restore the statistical information of a time-series instance, leading to significant performance improvements in time-series forecasting, as shown in Fig. 1. We demonstrate the effectiveness of RevIN via extensive quantitative and qualitative analyses on various real-world datasets, addressing the distribution shift problem.},
	language = {en},
	urldate = {2024-11-18},
	booktitle = {Proceedings of the {Tenth} {International} {Conference} on {Learning} {Representations}},
	author = {Kim, Taesung and Kim, Jinhee and Tae, Yunwon and Park, Cheonbok and Choi, Jang-Ho and Choo, Jaegul},
	month = oct,
	year = {2021},
	keywords = {zu lesen},
}

@inproceedings{kan_multivariate_2022,
	title = {Multivariate {Quantile} {Function} {Forecaster}},
	url = {https://proceedings.mlr.press/v151/kan22a.html},
	abstract = {We propose Multivariate Quantile Function Forecaster (MQF2), a global probabilistic forecasting method constructed using a multivariate quantile function and investigate its application to multi-horizon forecasting. Prior approaches are either autoregressive, implicitly capturing the dependency structure across time but exhibiting error accumulation with increasing forecast horizons, or multi-horizon sequence-to-sequence models, which do not exhibit error accumulation, but also do typically not model the dependency structure across time steps. MQF2 combines the benefits of both approaches, by directly making predictions in the form of a multivariate quantile function, defined as the gradient of a convex function which we parametrize using input-convex neural networks. By design, the quantile function is monotone with respect to the input quantile levels and hence avoids quantile crossing. We provide two options to train MQF2: with energy score or with maximum likelihood. Experimental results on real-world and synthetic datasets show that our model has comparable performance with state-of-the-art methods in terms of single time step metrics while capturing the time dependency structure.},
	language = {en},
	urldate = {2025-06-06},
	booktitle = {Proceedings of {The} 25th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Kan, Kelvin and Aubet, François-Xavier and Januschowski, Tim and Park, Youngsuk and Benidis, Konstantinos and Ruthotto, Lars and Gasthaus, Jan},
	month = may,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {10603--10621},
}

@article{kalman_new_1960,
	title = {A {New} {Approach} to {Linear} {Filtering} and {Prediction} {Problems}},
	volume = {82},
	issn = {0021-9223},
	url = {https://doi.org/10.1115/1.3662552},
	doi = {10.1115/1.3662552},
	abstract = {The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
	number = {1},
	urldate = {2025-06-11},
	journal = {Journal of Basic Engineering},
	author = {Kalman, R. E.},
	month = mar,
	year = {1960},
	keywords = {zu lesen},
	pages = {35--45},
}

@misc{kalchbrenner_neural_2017,
	title = {Neural {Machine} {Translation} in {Linear} {Time}},
	url = {http://arxiv.org/abs/1610.10099},
	doi = {10.48550/arXiv.1610.10099},
	abstract = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.},
	urldate = {2025-04-18},
	publisher = {arXiv},
	author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray},
	month = mar,
	year = {2017},
	note = {arXiv:1610.10099},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, zu lesen},
}

@article{julier_unscented_2004,
	title = {Unscented filtering and nonlinear estimation},
	volume = {92},
	issn = {1558-2256},
	url = {https://ieeexplore.ieee.org/abstract/document/1271397},
	doi = {10.1109/JPROC.2003.823141},
	abstract = {The extended Kalman filter (EKF) is probably the most widely used estimation algorithm for nonlinear systems. However, more than 35 years of experience in the estimation community has shown that is difficult to implement, difficult to tune, and only reliable for systems that are almost linear on the time scale of the updates. Many of these difficulties arise from its use of linearization. To overcome this limitation, the unscented transformation (UT) was developed as a method to propagate mean and covariance information through nonlinear transformations. It is more accurate, easier to implement, and uses the same order of calculations as linearization. This paper reviews the motivation, development, use, and implications of the UT.},
	number = {3},
	urldate = {2025-06-11},
	journal = {Proceedings of the IEEE},
	author = {Julier, S.J. and Uhlmann, J.K.},
	month = mar,
	year = {2004},
	keywords = {Chemical processes, Control systems, Filtering, Kalman filters, Navigation, Nonlinear control systems, Nonlinear systems, Particle tracking, Target tracking, Vehicles, zu lesen},
	pages = {401--422},
}

@inproceedings{jin_time-llm_2023,
	title = {Time-{LLM}: {Time} {Series} {Forecasting} by {Reprogramming} {Large} {Language} {Models}},
	shorttitle = {Time-{LLM}},
	url = {https://openreview.net/forum?id=Unb5CVPtae},
	abstract = {Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that {\textbackslash}method is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios. The code is made available at https://github.com/KimMeen/Time-LLM.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y. and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and Wen, Qingsong},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
}

@inproceedings{jiang_transgan_2021,
	title = {{TransGAN}: {Two} {Pure} {Transformers} {Can} {Make} {One} {Strong} {GAN}, and {That} {Can} {Scale} {Up}},
	volume = {34},
	shorttitle = {{TransGAN}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/7c220a2091c26a7f5e9f1cfb099511e3-Abstract.html},
	urldate = {2025-06-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jiang, Yifan and Chang, Shiyu and Wang, Zhangyang},
	year = {2021},
	keywords = {zu lesen},
	pages = {14745--14758},
}

@inproceedings{jiang_approximation_2021,
	title = {Approximation {Theory} of {Convolutional} {Architectures} for {Time} {Series} {Modelling}},
	url = {https://proceedings.mlr.press/v139/jiang21d.html},
	abstract = {We study the approximation properties of convolutional architectures applied to time series modelling, which can be formulated mathematically as a functional approximation problem. In the recurrent setting, recent results reveal an intricate connection between approximation efficiency and memory structures in the data generation process. In this paper, we derive parallel results for convolutional architectures, with WaveNet being a prime example. Our results reveal that in this new setting, approximation efficiency is not only characterised by memory, but also additional fine structures in the target relationship. This leads to a novel definition of spectrum-based regularity that measures the complexity of temporal relationships under the convolutional approximation scheme. These analyses provide a foundation to understand the differences between architectural choices for time series modelling and can give theoretically grounded guidance for practical applications.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jiang, Haotian and Li, Zhong and Li, Qianxiao},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {4961--4970},
}

@article{jayanthi_traffic_2021,
	title = {Traffic time series forecasting on highways - a contemporary survey of models, methods and techniques},
	volume = {39},
	issn = {1742-7967},
	url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJLSM.2021.115068},
	doi = {10.1504/IJLSM.2021.115068},
	abstract = {Transportation research is dynamic and essential engineering prospect of all nations across the globe. Recent developments in intelligent transport systems (ITS) have established software system-enabled transportation infrastructure to the public using which traveller information service and hassle free transport have become the prime objective of the transport industry. At present, innovation in technology driven infrastructure planning in transportation management is highly demanded research prospect in the area of intelligent transportation systems and services. Research effort towards development of ITS with statistical and machine learning (ML) approaches applied in time series analysis for traffic forecasting is enormous. But, the outcome of such researches is still under refinement considering various practical difficulties. Hence, the objective of this survey is to present a detailed insight on evolution of traffic time series forecasting with broad classification of methods and detailed summary of their results. Finally, comprehensive review results are presented with directions to address the research challenges.},
	number = {1},
	urldate = {2025-06-19},
	journal = {International Journal of Logistics Systems and Management},
	author = {Jayanthi, G. and Jothilakshmi, P.},
	month = jan,
	year = {2021},
	note = {Publisher: Inderscience Publishers},
	keywords = {data mining, machine learning technique, non-parametric, parametric, short-term traffic prediction, traffic operations, zu lesen},
	pages = {77--110},
}

@inproceedings{jawed_gqformer_2022,
	title = {{GQFormer}: {A} {Multi}-{Quantile} {Generative} {Transformer} for {Time} {Series} {Forecasting}},
	shorttitle = {{GQFormer}},
	url = {https://ieeexplore.ieee.org/document/10020927},
	doi = {10.1109/BigData55660.2022.10020927},
	abstract = {We propose GQFormer, a probabilistic time series forecasting method that models the quantile function of the forecast distribution. Our methodology is rooted in the Implicit Quantile modeling approach, where samples from the Uniform distribution {\textbackslash}mathcalUłeft( 0,1 {\textbackslash}right) are reparameterized to quantile values of the target distribution. This allows implicit generative quantile modeling without any prior assumptions on the data distribution like Gaussianity, common in prior works. Our work is distinguished from prior quantile forecasting methods by novel methodological advances that relate to directly modeling the correlations among multiple quantile estimations at each forecasting horizon. To this end, we firstly develop a parameters haring architecture that implicitly models multiple quantile estimations efficiently and secondly regularize these through a novel multi-task loss function formulation that optimizes for quantile estimations to be sharper estimations individually and on the whole be spread maximally apart to capture the various modes of the underlying distribution. We experimentally validate the superiority of the method to state-of-the-art probabilistic forecasting baselines and ablations to the loss formulation.},
	urldate = {2024-12-24},
	booktitle = {2022 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Jawed, Shayan and Schmidt-Thieme, Lars},
	month = dec,
	year = {2022},
	keywords = {Big Data, Correlation, Estimation, Implicit Quantile Networks, Multi-task Learning, Multitasking, Predictive models, Probabilistic Forecasting, Probabilistic logic, Sparse Attention Transformer, Time series analysis, zu lesen},
	pages = {992--1001},
}

@article{januschowski_criteria_2020,
	series = {M4 {Competition}},
	title = {Criteria for classifying forecasting methods},
	volume = {36},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207019301529},
	doi = {10.1016/j.ijforecast.2019.05.008},
	abstract = {Classifying forecasting methods as being either of a “machine learning” or “statistical” nature has become commonplace in parts of the forecasting literature and community, as exemplified by the M4 competition and the conclusion drawn by the organizers. We argue that this distinction does not stem from fundamental differences in the methods assigned to either class. Instead, this distinction is probably of a tribal nature, which limits the insights into the appropriateness and effectiveness of different forecasting methods. We provide alternative characteristics of forecasting methods which, in our view, allow to draw meaningful conclusions. Further, we discuss areas of forecasting which could benefit most from cross-pollination between the ML and the statistics communities.},
	number = {1},
	urldate = {2025-05-28},
	journal = {International Journal of Forecasting},
	author = {Januschowski, Tim and Gasthaus, Jan and Wang, Yuyang and Salinas, David and Flunkert, Valentin and Bohlke-Schneider, Michael and Callot, Laurent},
	month = jan,
	year = {2020},
	keywords = {zu lesen},
	pages = {167--177},
}

@article{izakian_fuzzy_2015,
	title = {Fuzzy clustering of time series data using dynamic time warping distance},
	volume = {39},
	issn = {0952-1976},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197614003078},
	doi = {https://doi.org/10.1016/j.engappai.2014.12.015},
	abstract = {Clustering is a powerful vehicle to reveal and visualize structure of data. When dealing with time series, selecting a suitable measure to evaluate the similarities/dissimilarities within the data becomes necessary and subsequently it exhibits a significant impact on the results of clustering. This selection should be based upon the nature of time series and the application itself. When grouping time series based on their shape information is of interest (shape-based clustering), using a Dynamic Time Warping (DTW) distance is a desirable choice. Using stretching or compressing segments of temporal data, DTW determines an optimal match between any two time series. In this way, time series exhibiting similar patterns occurring at different time periods, are considered as being similar. Although DTW is a suitable choice for comparing data with respect to their shape information, calculating the average of a collection of time series (which is required in clustering methods) based on this distance becomes a challenging problem. As the result, employing clustering techniques like K-Means and Fuzzy C-Means (where the cluster centers – prototypes are calculated through averaging the data) along with the DTW distance is a challenging task and may produce unsatisfactory results. In this study, three alternatives for fuzzy clustering of time series using DTW distance are proposed. In the first method, a DTW-based averaging technique proposed in the literature, has been applied to the Fuzzy C-Means clustering. The second method considers a Fuzzy C-Medoids clustering, while the third alternative comes as a hybrid technique, which exploits the advantages of both the Fuzzy C-Means and Fuzzy C-Medoids when clustering time series. Experimental studies are reported over a set of time series coming from the UCR time series database.},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Izakian, Hesam and Pedrycz, Witold and Jamal, Iqbal},
	year = {2015},
	keywords = {Clustering time series, Dynamic Time Warping (DTW), Fuzzy clustering, Hybrid approach, zu lesen},
	pages = {235--244},
}

@article{ing_accumulated_2007,
	title = {Accumulated prediction errors, information criteria and optimal forecasting for autoregressive time series},
	volume = {35},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-35/issue-3/Accumulated-prediction-errors-information-criteria-and-optimal-forecasting-for-autoregressive/10.1214/009053606000001550.full},
	doi = {10.1214/009053606000001550},
	abstract = {The predictive capability of a modification of Rissanen’s accumulated prediction error (APE) criterion, APEδn, is investigated in infinite-order autoregressive (AR(∞)) models. Instead of accumulating squares of sequential prediction errors from the beginning, APEδn is obtained by summing these squared errors from stage nδn, where n is the sample size and 1/n≤δn≤1−(1/n) may depend on n. Under certain regularity conditions, an asymptotic expression is derived for the mean-squared prediction error (MSPE) of an AR predictor with order determined by APEδn. This expression shows that the prediction performance of APEδn can vary dramatically depending on the choice of δn. Another interesting finding is that when δn approaches 1 at a certain rate, APEδn can achieve asymptotic efficiency in most practical situations. An asymptotic equivalence between APEδn and an information criterion with a suitable penalty term is also established from the MSPE point of view. This offers new perspectives for understanding the information and prediction-based model selection criteria. Finally, we provide the first asymptotic efficiency result for the case when the underlying AR(∞) model is allowed to degenerate to a finite autoregression.},
	number = {3},
	urldate = {2025-02-04},
	journal = {The Annals of Statistics},
	author = {Ing, Ching-Kang},
	month = jul,
	year = {2007},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {60M20, 62F12, 62M10, Accumulated prediction errors, Asymptotic efficiency, information criterion, optimal forecasting, order selection, zu lesen, ‎asymptotic ‎equivalence},
	pages = {1238--1277},
}

@inproceedings{ilbert_samformer_2024,
	title = {{SAMformer}: {Unlocking} the {Potential} of {Transformers} in {Time} {Series} {Forecasting} with {Sharpness}-{Aware} {Minimization} and {Channel}-{Wise} {Attention}},
	shorttitle = {{SAMformer}},
	url = {https://proceedings.mlr.press/v235/ilbert24a.html},
	abstract = {Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses current state-of-the-art methods and is on par with the biggest foundation model MOIRAI while having significantly fewer parameters. The code is available at https://github.com/romilbert/samformer.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ilbert, Romain and Odonnat, Ambroise and Feofanov, Vasilii and Virmaux, Aladin and Paolo, Giuseppe and Palpanas, Themis and Redko, Ievgen},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {20924--20954},
}

@article{hyndman_automatic_2008,
	title = {Automatic {Time} {Series} {Forecasting}: {The} forecast {Package} for {R}},
	volume = {27},
	copyright = {Copyright (c) 2007 Rob J. Hyndman, Yeasmin Khandakar},
	issn = {1548-7660},
	shorttitle = {Automatic {Time} {Series} {Forecasting}},
	url = {https://doi.org/10.18637/jss.v027.i03},
	doi = {10.18637/jss.v027.i03},
	abstract = {Automatic forecasts of large numbers of univariate time series are often needed in business and other contexts. We describe two automatic forecasting algorithms that have been implemented in the forecast package for R. The first is based on innovations state space models that underly exponential smoothing methods. The second is a step-wise algorithm for forecasting with ARIMA models. The algorithms are applicable to both seasonal and non-seasonal data, and are compared and illustrated using four real time series. We also briefly describe some of the other functionality available in the forecast package.},
	language = {en},
	urldate = {2025-05-28},
	journal = {Journal of Statistical Software},
	author = {Hyndman, Rob J. and Khandakar, Yeasmin},
	month = jul,
	year = {2008},
	keywords = {zu lesen},
	pages = {1--22},
}

@article{hyndman_density_2010,
	title = {Density {Forecasting} for {Long}-{Term} {Peak} {Electricity} {Demand}},
	volume = {25},
	doi = {10.1109/TPWRS.2009.2036017},
	number = {2},
	journal = {IEEE Transactions on Power Systems},
	author = {Hyndman, Rob J. and Fan, Shu},
	year = {2010},
	keywords = {Calendars, Demand forecasting, Demography, Density forecast, Economic forecasting, Power generation, Probability distribution, Temperature, Timing, Uncertainty, Weather forecasting, long-term demand forecasting, simulation, time series, zu lesen},
	pages = {1142--1153},
}

@article{hyndman_state_2002,
	title = {A state space framework for automatic forecasting using exponential smoothing methods},
	volume = {18},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207001001108},
	doi = {10.1016/S0169-2070(01)00110-8},
	abstract = {We provide a new approach to automatic forecasting based on an extended range of exponential smoothing methods. Each method in our taxonomy of exponential smoothing methods provides forecasts that are equivalent to forecasts from a state space model. This equivalence allows: (1) easy calculation of the likelihood, the AIC and other model selection criteria; (2) computation of prediction intervals for each method; and (3) random simulation from the underlying state space model. We demonstrate the methods by applying them to the data from the M-competition and the M3-competition. The method provides forecast accuracy comparable to the best methods in the competitions; it is particularly good for short forecast horizons with seasonal data.},
	number = {3},
	urldate = {2025-05-28},
	journal = {International Journal of Forecasting},
	author = {Hyndman, Rob J and Koehler, Anne B and Snyder, Ralph D and Grose, Simone},
	month = jul,
	year = {2002},
	keywords = {Automatic forecasting, Exponential smoothing, Prediction intervals, State space models, zu lesen},
	pages = {439--454},
}

@book{huber_robust_2011,
	edition = {2},
	title = {Robust statistics},
	isbn = {978-1-118-21033-8},
	author = {Huber, Peter J and Ronchetti, Elvezio. M},
	month = sep,
	year = {2011},
	keywords = {zu lesen},
}

@article{hyvarinen_estimation_2005,
	title = {Estimation of {Non}-{Normalized} {Statistical} {Models} by {Score} {Matching}},
	volume = {6},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v6/hyvarinen05a.html},
	abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.},
	number = {24},
	urldate = {2025-06-09},
	journal = {Journal of Machine Learning Research},
	author = {Hyvärinen, Aapo},
	year = {2005},
	keywords = {zu lesen},
	pages = {695--709},
}

@inproceedings{huang_leret_2024,
	title = {{LeRet}: {Language}-{Empowered} {Retentive} {Network} for {Time} {Series} {Forecasting}},
	volume = {5},
	shorttitle = {{LeRet}},
	url = {https://www.ijcai.org/proceedings/2024/460},
	doi = {10.24963/ijcai.2024/460},
	abstract = {Electronic proceedings of IJCAI 2024},
	language = {en},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the {Thirty}-{Third} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Huang, Qihe and Zhou, Zhengyang and Yang, Kuo and Lin, Gengyu and Yi, Zhongchao and Wang, Yang},
	month = aug,
	year = {2024},
	note = {ISSN: 1045-0823},
	keywords = {zu lesen},
	pages = {4165--4173},
}

@inproceedings{huang_neural_2018,
	title = {Neural {Autoregressive} {Flows}},
	url = {https://proceedings.mlr.press/v80/huang18d.html},
	abstract = {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) (Papamakarios et al., 2017), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time (Oord et al., 2017), via Inverse Autoregressive Flows (IAF) (Kingma et al., 2016). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.},
	language = {en},
	urldate = {2025-06-05},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {2078--2087},
}

@inproceedings{huang_convex_2020,
	title = {Convex {Potential} {Flows}: {Universal} {Probability} {Distributions} with {Optimal} {Transport} and {Convex} {Optimization}},
	shorttitle = {Convex {Potential} {Flows}},
	url = {https://openreview.net/forum?id=te7PVH1sPxJ},
	abstract = {Flow-based models are powerful tools for designing probabilistic models with tractable density. This paper introduces Convex Potential Flows (CP-Flow), a natural and efficient parameterization of invertible models inspired by the optimal transport (OT) theory. CP-Flows are the gradient map of a strongly convex neural potential function. The convexity implies invertibility and allows us to resort to convex optimization to solve the convex conjugate for efficient inversion. To enable maximum likelihood training, we derive a new gradient estimator of the log-determinant of the Jacobian, which involves solving an inverse-Hessian vector product using the conjugate gradient method. The gradient estimator has constant-memory cost, and can be made effectively unbiased by reducing the error tolerance level of the convex optimization routine. Theoretically, we prove that CP-Flows are universal density approximators and are optimal in the OT sense. Our empirical results show that CP-Flow performs competitively on standard benchmarks of density estimation and variational inference.},
	language = {en},
	urldate = {2025-06-06},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Learning} {Representations} ({ICLR} 2020)},
	author = {Huang, Chin-Wei and Chen, Ricky T. Q. and Tsirigotis, Christos and Courville, Aaron},
	month = oct,
	year = {2020},
	keywords = {zu lesen},
}

@article{horowitz_identification_2006,
	title = {Identification and estimation of statistical functionals using incomplete data},
	volume = {132},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S030440760500062X},
	doi = {10.1016/j.jeconom.2005.02.007},
	abstract = {Incomplete data, due to missing observations or interval measurement of variables, usually cause parameters of interest in applications to be unidentified except under untestable and often controversial assumptions. However, it is often possible to identify sharp bounds on parameters without making untestable assumptions about the process through which data become incomplete. The bounds contain all logically possible values of the parameters and can be estimated consistently by replacing the population distribution of the data with the empirical distribution. This is straightforward in some circumstances but computationally burdensome in others. This paper describes the general problem and presents an empirical illustration.},
	number = {2},
	urldate = {2025-04-03},
	journal = {Journal of Econometrics},
	author = {Horowitz, Joel L. and Manski, Charles F.},
	month = jun,
	year = {2006},
	keywords = {Bounds, Missing data, Non-linear programming, zu lesen},
	pages = {445--459},
}

@incollection{horn_chapter_2012,
	edition = {2},
	title = {Chapter 7: {Positive} {Definite} and {Semidefinite} {Matrices}},
	booktitle = {Matrix {Analysis}},
	publisher = {Cambridge University Press},
	author = {Horn, Roger A. and Johnson, Charles R.},
	year = {2012},
	keywords = {zu lesen},
	pages = {425--516},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2025-04-16},
	journal = {Neural Computing},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	keywords = {zu lesen},
	pages = {1735--1780},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2025-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	keywords = {zu lesen},
	pages = {6840--6851},
}

@article{hewamalage_recurrent_2021,
	title = {Recurrent {Neural} {Networks} for {Time} {Series} {Forecasting}: {Current} {Status} and {Future} {Directions}},
	volume = {37},
	issn = {01692070},
	shorttitle = {Recurrent {Neural} {Networks} for {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/1909.00590},
	doi = {10.1016/j.ijforecast.2020.06.008},
	abstract = {Recurrent Neural Networks (RNN) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as ETS and ARIMA gain their popularity not only from their high accuracy, but they are also suitable for non-expert users as they are robust, eﬃcient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, that allow us to develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns, otherwise we recommend a deseasonalization step. Comparisons against ETS and ARIMA demonstrate that the implemented (semi-)automatic RNN models are no silver bullets, but they are competitive alternatives in many situations.},
	language = {en},
	number = {1},
	urldate = {2023-06-01},
	journal = {International Journal of Forecasting},
	author = {Hewamalage, Hansika and Bergmeir, Christoph and Bandara, Kasun},
	month = jan,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, zu lesen},
	pages = {388--427},
}

@article{han_capacity_2024,
	title = {The {Capacity} and {Robustness} {Trade}-{Off}: {Revisiting} the {Channel} {Independent} {Strategy} for {Multivariate} {Time} {Series} {Forecasting}},
	volume = {36},
	issn = {1558-2191},
	shorttitle = {The {Capacity} and {Robustness} {Trade}-{Off}},
	url = {https://ieeexplore.ieee.org/document/10529618},
	doi = {10.1109/TKDE.2024.3400008},
	abstract = {Multivariate time series data comprises various channels of variables. The multivariate forecasting models need to capture the relationship between the channels to accurately predict future values. However, recently, there has been an emergence of methods that employ the Channel Independent (CI) strategy. These methods view multivariate time series data as separate univariate time series and disregard the correlation between channels. Surprisingly, our empirical results have shown that models trained with the CI strategy outperform those trained with the Channel Dependent (CD) strategy, usually by a significant margin. Nevertheless, the reasons behind this phenomenon have not yet been thoroughly explored in the literature. This paper provides comprehensive empirical and theoretical analyses of the characteristics of multivariate time series datasets and the CI/CD strategy. Our results conclude that the CD approach has higher capacity but often lacks robustness to accurately predict distributionally drifted time series. In contrast, the CI approach trades capacity for robust prediction. Practical measures inspired by these analyses are proposed to address the capacity and robustness dilemma, including a modified CD method called Predict Residuals with Regularization (PRReg) that can surpass the CI strategy. We hope our findings can raise awareness among researchers about the characteristics of multivariate time series and inspire the construction of better forecasting models.},
	number = {11},
	urldate = {2024-10-26},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Han, Lu and Ye, Han-Jia and Zhan, De-Chuan},
	month = nov,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Biological system modeling, Channel independence, Forecasting, Predictive models, Robustness, Time series analysis, Training, Transformers, forecasting, multivariate time series, robustness, zu lesen},
	pages = {7129--7142},
}

@article{harsha_prescriptive_2021,
	title = {A {Prescriptive} {Machine}-{Learning} {Framework} to the {Price}-{Setting} {Newsvendor} {Problem}},
	volume = {3},
	issn = {2575-1484},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/ijoo.2019.0046},
	doi = {10.1287/ijoo.2019.0046},
	abstract = {We develop a practical procedure for solving the price-setting newsvendor problem employing (a) statistical estimation methods to recover only three distinct aspects of the demand distribution: the mean, quantile and superquantile, and (b) price optimization methods to estimate the optimal solution. This procedure is asymptotically optimal under mild conditions when the estimators are consistent and the price optimization has a unique global maximum. To estimate the quantities of interest in a data-driven, distribution-free fashion with multidimensional datasets, we investigate estimators based on generalized linear regression (GLR), mixed-quantile regression (MQR), and superquantile regression (SQR). We provide two extensions to these estimators that are of independent interest. First, we propose a novel and exact large-scale decomposition method that is computationally efficient for SQR, and second, we extend the MQR estimation method by relaxing its implicit assumptions of homoscedasticity. Our computational experiments indicate the importance of flexible estimation methods that inherently model heteroscedasticity (with improvements in absolute error of resultant profit as high as 90\%), and suggest that quantile-based methods such as MQR and SQR provide better solutions for a wide range of demand distributions, although for certain location-scale demand distributions similar to the Normal distribution, GLR may be preferable.},
	number = {3},
	urldate = {2025-06-19},
	journal = {INFORMS Journal on Optimization},
	author = {Harsha, Pavithra and Natarajan, Ramesh and Subramanian, Dharmashankar},
	month = jul,
	year = {2021},
	note = {Publisher: INFORMS},
	keywords = {conditional value at risk, decomposition algorithm, estimation, heteroscedastic least squares, newsvendor, pricing, quantile regression, superquantile regression, zu lesen},
	pages = {227--253},
}

@article{han_mcformer_2024,
	title = {{MCformer}: {Multivariate} {Time} {Series} {Forecasting} {With} {Mixed}-{Channels} {Transformer}},
	volume = {11},
	doi = {10.1109/JIOT.2024.3401697},
	number = {17},
	journal = {IEEE Internet of Things Journal},
	author = {Han, Wenyong and Zhu, Tao and Chen, Liming and Ning, Huansheng and Luo, Yang and Wan, Yaping},
	year = {2024},
	keywords = {Correlation, Data models, Forecasting, Internet of Things, Long time series, Predictive models, Time series analysis, Transformers, multivariate time series, self-attention, time series forecasting, zu lesen},
	pages = {28320--28329},
}

@article{groser_copulae_2022,
	title = {Copulae: {An} overview and recent developments},
	volume = {14},
	copyright = {© 2021 The Authors. WIREs Computational Statistics published by Wiley Periodicals LLC.},
	issn = {1939-0068},
	shorttitle = {Copulae},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1557},
	doi = {10.1002/wics.1557},
	abstract = {Over the decades that have passed since they were introduced, copulae still remain a very powerful tool for modeling and estimating multivariate distributions. This work gives an overview of copula theory and it also summarizes the latest results. This article recalls the basic definition, the most important cases of bivariate copulae, and it then proceeds to a sketch of how multivariate copulae are developed both from bivariate copulae and from scratch. Regarding higher dimensions, the focus is on hierarchical Archimedean, vine, and factor copulae, which are the most often used and most flexible ways to introduce copulae to multivariate distributions. We also provide an overview of how copulae can be used in various fields of data science, including recent results. These fields include but are not limited to time series and machine learning. Finally, we describe estimation and testing methods for copulae in general, their application to the presented copula structures, and we give some specific testing and estimation procedures for those specific copulae. This article is categorized under: Statistical Models {\textgreater} Multivariate Models Statistical Models {\textgreater} Semiparametric Models Statistical and Graphical Methods of Data Analysis {\textgreater} Multivariate Analysis},
	language = {en},
	number = {3},
	urldate = {2025-06-16},
	journal = {WIREs Computational Statistics},
	author = {Größer, Joshua and Okhrin, Ostap},
	year = {2022},
	note = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1557},
	keywords = {copula, dependence modeling, multivariate distribution, zu lesen},
	pages = {e1557},
}

@misc{graves_generating_2014,
	title = {Generating {Sequences} {With} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1308.0850},
	doi = {10.48550/arXiv.1308.0850},
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Graves, Alex},
	month = jun,
	year = {2014},
	note = {arXiv:1308.0850},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, zu lesen},
}

@inproceedings{gouttes_probabilistic_2021,
	title = {Probabilistic {Time} {Series} {Forecasting} with {Implicit} {Quantile} {Networks}},
	abstract = {Here, we propose a general method for probabilistic time series forecasting. We combine an autoregressive recurrent neural network to model temporal dynamics with Implicit Quantile Networks to learn a large class of distributions over a time-series target. When compared to other probabilistic neural forecasting models on real- and simulated data, our approach is favorable in terms of point-wise prediction accuracy as well as on estimating the underlying temporal distribution.},
	booktitle = {Proceedings of the {Time} {Series} {Workshop} at 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gouttes, Adèle and Rasul, Kashif and Koren, Mateusz and Stephan, Johannes and Naghibi, Tofigh},
	year = {2021},
	note = {Workshop Paper},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, zu lesen},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	isbn = {0-262-03561-8},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {zu lesen},
}

@article{good_rational_1952,
	title = {Rational {Decisions}},
	volume = {14},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984087},
	abstract = {This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
	number = {1},
	urldate = {2025-03-18},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Good, I. J.},
	year = {1952},
	note = {Publisher: [Royal Statistical Society, Oxford University Press]},
	keywords = {zu lesen},
	pages = {107--114},
}

@inproceedings{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/f033ed80deb0234979a61f95710dbe25-Abstract.html},
	abstract = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
	urldate = {2025-06-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2014},
	keywords = {zu lesen},
}

@article{gneiting_model_2023,
	title = {Model {Diagnostics} and {Forecast} {Evaluation} for {Quantiles}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2326-8298, 2326-831X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-032921-020240},
	doi = {10.1146/annurev-statistics-032921-020240},
	abstract = {Model diagnostics and forecast evaluation are closely related tasks, with the former concerning in-sample goodness (or lack) of fit and the latter addressing predictive performance out-of-sample. We review the ubiquitous setting in which forecasts are cast in the form of quantiles or quantile-bounded prediction intervals. We distinguish unconditional calibration, which corresponds to classical coverage criteria, from the stronger notion of conditional calibration, as can be visualized in quantile reliability diagrams. Consistent scoring functions—including, but not limited to, the widely used asymmetricpiecewise linear score or pinball loss—provide for comparative assessment and ranking, and link to the coefficient of determination and skill scores. We illustrate the use of these tools on Engel's food expenditure data, the Global Energy Forecasting Competition 2014, and the US COVID-19 Forecast Hub.},
	language = {en},
	number = {1},
	urldate = {2025-03-18},
	journal = {Annual Review of Statistics and Its Application},
	author = {Gneiting, Tilmann and Wolffram, Daniel and Resin, Johannes and Kraus, Kristof and Bracher, Johannes and Dimitriadis, Timo and Hagenmeyer, Veit and Jordan, Alexander I. and Lerch, Sebastian and Phipps, Kaleb and Schienle, Melanie},
	month = mar,
	year = {2023},
	keywords = {zu lesen},
	pages = {597--621},
}

@article{gneiting_comparing_2011,
	title = {Comparing {Density} {Forecasts} {Using} {Threshold}- and {Quantile}-{Weighted} {Scoring} {Rules}},
	volume = {29},
	issn = {0735-0015},
	url = {https://doi.org/10.1198/jbes.2010.08110},
	doi = {10.1198/jbes.2010.08110},
	abstract = {We propose a method for comparing density forecasts that is based on weighted versions of the continuous ranked probability score. The weighting emphasizes regions of interest, such as the tails or the center of a variable’s range, while retaining propriety, as opposed to a recently developed weighted likelihood ratio test, which can be hedged. Threshold- and quantile-based decompositions of the continuous ranked probability score can be illustrated graphically and provide insight into the strengths and deficiencies of a forecasting method. We illustrate the use of the test and graphical tools in case studies on the Bank of England’s density forecasts of quarterly inflation rates in the United Kingdom, and probabilistic predictions of wind resources in the Pacific Northwest.},
	number = {3},
	urldate = {2025-03-25},
	journal = {Journal of Business \& Economic Statistics},
	author = {Gneiting, Tilmann and Ranjan, Roopesh},
	month = jul,
	year = {2011},
	note = {https://doi.org/10.1198/jbes.2010.08110},
	keywords = {Continuous ranked probability score, Predictive ability testing, Probabilistic forecast, Proper scoring rule, Quantile, Weighted likelihood ratio test, zu lesen},
	pages = {411--422},
}

@inproceedings{goerg_forecastable_2013,
	address = {Atlanta, Georgia, USA},
	title = {Forecastable {Component} {Analysis}},
	url = {https://proceedings.mlr.press/v28/goerg13.html},
	abstract = {I introduce Forecastable Component Analysis (ForeCA), a novel dimension reduction technique for temporally dependent signals. Based on a new forecastability measure, ForeCA finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space. I present a converging algorithm with a fast eigenvector solution. Applications to financial and macro-economic time series show that ForeCA can successfully discover informative structure, which can be used for forecasting as well as classification. The R package ForeCA accompanies this work and is publicly available on CRAN.},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Goerg, Georg},
	editor = {Dasgupta, Sanjoy and McAllester, David},
	month = jun,
	year = {2013},
	keywords = {zu lesen},
	pages = {64--72},
}

@article{gneiting_strictly_2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	language = {en},
	number = {477},
	urldate = {2025-03-18},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	month = mar,
	year = {2007},
	keywords = {zu lesen},
	pages = {359--378},
}

@article{gneiting_probabilistic_2014,
	title = {Probabilistic {Forecasting}},
	volume = {1},
	issn = {2326-8298, 2326-831X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-062713-085831},
	doi = {10.1146/annurev-statistics-062713-085831},
	abstract = {A probabilistic forecast takes the form of a predictive probability distribution over future quantities or events of interest. Probabilistic forecasting aims to maximize the sharpness of the predictive distributions, subject to calibration, on the basis of the available information set. We formalize and study notions of calibration in a prediction space setting. In practice, probabilistic calibration can be checked by examining probability integral transform (PIT) histograms. Proper scoring rules such as the logarithmic score and the continuous ranked probability score serve to assess calibration and sharpness simultaneously. As a special case, consistent scoring functions provide decision-theoretically coherent tools for evaluating point forecasts. We emphasize methodological links to parametric and nonparametric distributional regression techniques, which attempt to model and to estimate conditional distribution functions; we use the context of statistically postprocessed ensemble forecasts in numerical weather prediction as an example. Throughout, we illustrate concepts and methodologies in data examples.},
	language = {en},
	number = {1},
	urldate = {2025-03-18},
	journal = {Annual Review of Statistics and Its Application},
	author = {Gneiting, Tilmann and Katzfuss, Matthias},
	month = jan,
	year = {2014},
	keywords = {zu lesen},
	pages = {125--151},
}

@article{gneiting_probabilistic_2007,
	title = {Probabilistic {Forecasts}, {Calibration} and {Sharpness}},
	volume = {69},
	issn = {1369-7412},
	url = {https://doi.org/10.1111/j.1467-9868.2007.00587.x},
	doi = {10.1111/j.1467-9868.2007.00587.x},
	abstract = {Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
	number = {2},
	urldate = {2024-12-22},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
	month = apr,
	year = {2007},
	keywords = {zu lesen},
	pages = {243--268},
}

@article{gneiting_making_2011,
	title = {Making and {Evaluating} {Point} {Forecasts}},
	volume = {106},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/jasa.2011.r10138},
	doi = {10.1198/jasa.2011.r10138},
	abstract = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.},
	number = {494},
	urldate = {2024-12-22},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann},
	month = jun,
	year = {2011},
	note = {https://doi.org/10.1198/jasa.2011.r10138},
	keywords = {Bayes rule, Bregman function, Conditional value-at-risk (CVaR), Decision theory, Elicitability, Expectile, Mean, Median, Mode, Proper scoring rule, Quantile, Statistical functional, zu lesen},
	pages = {746--762},
}

@inproceedings{ghimire_reliable_2021,
	title = {Reliable {Estimation} of {KL} {Divergence} using a {Discriminator} in {Reproducing} {Kernel} {Hilbert} {Space}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/54a367d629152b720749e187b3eaa11b-Abstract.html},
	abstract = {Estimating Kullback–Leibler (KL) divergence from samples of two distributions is essential in many machine learning problems. Variational methods using neural network discriminator have been proposed to achieve this task in a scalable manner. However, we noticed that most of these methods using neural network discriminators suffer from high fluctuations (variance) in estimates and instability in training. In this paper, we look at this issue from statistical learning theory and function space complexity perspective to understand why this happens and how to solve it. We argue that the cause of these pathologies is lack of control over the complexity of the neural network discriminator function and could be mitigated by controlling it. To achieve this objective, we 1) present a novel construction of the discriminator in the Reproducing Kernel Hilbert Space (RKHS), 2) theoretically relate the error probability bound of the KL estimates to the complexity of the discriminator in the RKHS space, 3) present a scalable way to control the complexity (RKHS norm) of the discriminator for a reliable estimation of KL divergence, and 4) prove the consistency of the proposed estimator. In three different applications of KL divergence -- estimation of KL, estimation of mutual information and Variational Bayes -- we show that by controlling the complexity as developed in the theory, we are able to reduce the variance of KL estimates and stabilize the training.},
	urldate = {2025-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ghimire, Sandesh and Masoomi, Aria and Dy, Jennifer},
	year = {2021},
	keywords = {zu lesen},
	pages = {10221--10233},
}

@inproceedings{gasthaus_probabilistic_2019,
	title = {Probabilistic {Forecasting} with {Spline} {Quantile} {Function} {RNNs}},
	url = {https://proceedings.mlr.press/v89/gasthaus19a.html},
	abstract = {In this paper, we propose a flexible method for probabilistic modeling with conditional quantile functions using monotonic regression splines. The shape of the spline is parameterized by a neural network whose parameters are learned by minimizing the continuous ranked probability score. Within this framework, we propose a method for probabilistic time series forecasting, which combines the modeling capacity of recurrent neural networks with the flexibility of a spline-based representation of the output distribution. Unlike methods based on parametric probability density functions and maximum likelihood estimation, the proposed method can flexibly adapt to different output distributions without manual intervention. We empirically demonstrate the effectiveness of the approach  on synthetic and real-world data sets.},
	language = {en},
	urldate = {2025-06-06},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Gasthaus, Jan and Benidis, Konstantinos and Wang, Yuyang and Rangapuram, Syama Sundar and Salinas, David and Flunkert, Valentin and Januschowski, Tim},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {1901--1910},
}

@misc{gao_client_2023,
	title = {Client: {Cross}-variable {Linear} {Integrated} {Enhanced} {Transformer} for {Multivariate} {Long}-{Term} {Time} {Series} {Forecasting}},
	shorttitle = {Client},
	url = {http://arxiv.org/abs/2305.18838},
	doi = {10.48550/arXiv.2305.18838},
	abstract = {Long-term time series forecasting (LTSF) is a crucial aspect of modern society, playing a pivotal role in facilitating long-term planning and developing early warning systems. While many Transformer-based models have recently been introduced for LTSF, a doubt have been raised regarding the effectiveness of attention modules in capturing cross-time dependencies. In this study, we design a mask-series experiment to validate this assumption and subsequently propose the "Cross-variable Linear Integrated ENhanced Transformer for Multivariate Long-Term Time Series Forecasting" (Client), an advanced model that outperforms both traditional Transformer-based models and linear models. Client employs linear modules to learn trend information and attention modules to capture cross-variable dependencies. Meanwhile, it simplifies the embedding and position encoding layers and replaces the decoder module with a projection layer. Essentially, Client incorporates non-linearity and cross-variable dependencies, which sets it apart from conventional linear models and Transformer-based models. Extensive experiments with nine real-world datasets have confirmed the SOTA performance of Client with the least computation time and memory consumption compared with the previous Transformer-based models. Our code is available at https://github.com/daxin007/Client.},
	urldate = {2025-04-24},
	publisher = {arXiv},
	author = {Gao, Jiaxin and Hu, Wenbo and Chen, Yuntian},
	month = may,
	year = {2023},
	note = {arXiv:2305.18838},
	keywords = {Computer Science - Machine Learning, zu lesen},
}

@inproceedings{feng_hypergraph_2019,
	address = {Honolulu, Hawaii, USA},
	series = {{AAAI}'19},
	title = {Hypergraph neural networks},
	volume = {33},
	isbn = {978-1-57735-809-1},
	url = {https://dl.acm.org/doi/10.1609/aaai.v33i01.33013558},
	doi = {10.1609/aaai.v33i01.33013558},
	abstract = {In this paper, we present a hypergraph neural networks (HGNN) framework for data representation learning, which can encode high-order data correlation in a hypergraph structure. Confronting the challenges of learning representation for complex data in real practice, we propose to incorporate such data structure in a hypergraph, which is more flexible on data modeling, especially when dealing with complex data. In this method, a hyperedge convolution operation is designed to handle the data correlation during representation learning. In this way, traditional hypergraph learning procedure can be conducted using hyperedge convolution operations efficiently. HGNN is able to learn the hidden layer representation considering the high-order data structure, which is a general framework considering the complex data correlations. We have conducted experiments on citation network classification and visual object recognition tasks and compared HGNN with graph convolutional networks and other traditional methods. Experimental results demonstrate that the proposed HGNN method outperforms recent state-of-the-art methods. We can also reveal from the results that the proposed HGNN is superior when dealing with multi-modal data compared with existing methods.},
	urldate = {2025-05-06},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Feng, Yifan and You, Haoxuan and Zhang, Zizhao and Ji, Rongrong and Gao, Yue},
	month = jan,
	year = {2019},
	keywords = {zu lesen},
	pages = {3558--3565},
}

@article{feng_multi-scale_2024,
	title = {Multi-{Scale} {Attention} {Flow} for {Probabilistic} {Time} {Series} {Forecasting}},
	volume = {36},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/abstract/document/10265130?casa_token=PdK0TC8e6WsAAAAA:ZyvA8BnPuDX2U6iPYQeHdyTbRUWSKXaPKIw7Hk1JDjVGn256Lp0hChY-YYVSSoIH8bdrlWZ6cOr2},
	doi = {10.1109/TKDE.2023.3319672},
	abstract = {The probability prediction of multivariate time series is a notoriously challenging but practical task. On the one hand, the challenge is how to effectively capture the cross-series correlations between interacting time series, to achieve accurate distribution modeling. On the other hand, we should consider how to capture the contextual information within time series more accurately to model multivariate temporal dynamics of time series. In this work, we proposed a novel non-autoregressive deep learning model, called Multi-scale Attention Normalizing Flow(MANF), where we combine multi-scale attention with relative position information and the multivariate data distribution is represented by the conditioned normalizing flow. Additionally, compared with autoregressive modeling methods, our model avoids the influence of cumulative error and does not increase the time complexity. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multivariate datasets.},
	number = {5},
	urldate = {2024-12-21},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Feng, Shibo and Miao, Chunyan and Xu, Ke and Wu, Jiaxiang and Wu, Pengcheng and Zhang, Yang and Zhao, Peilin},
	month = may,
	year = {2024},
	keywords = {Correlation, Data models, Forecasting, Multivariate time series, Predictive models, Probabilistic logic, Task analysis, Time series analysis, generative model, multi-scale attention, normalizing flow, zu lesen},
	pages = {2056--2068},
}

@inproceedings{ekambaram_tsmixer_2023,
	address = {New York, NY, USA},
	series = {{KDD} '23},
	title = {{TSMixer}: {Lightweight} {MLP}-{Mixer} {Model} for {Multivariate} {Time} {Series} {Forecasting}},
	isbn = {979-8-4007-0103-0},
	shorttitle = {{TSMixer}},
	url = {https://dl.acm.org/doi/10.1145/3580305.3599533},
	doi = {10.1145/3580305.3599533},
	abstract = {Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their memory and compute-intensive requirements pose a critical bottleneck for long-term forecasting, despite numerous advancements in compute-aware self-attention modules. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60\%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2\%) with a significant reduction in memory and runtime (2-3X).},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the 29th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Ekambaram, Vijay and Jati, Arindam and Nguyen, Nam and Sinthong, Phanwadee and Kalagnanam, Jayant},
	month = aug,
	year = {2023},
	keywords = {zu lesen},
	pages = {459--469},
}

@inproceedings{du_implicit_2019,
	title = {Implicit {Generation} and {Modeling} with {Energy} {Based} {Models}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/378a063b8fdb1db941e34f4bde584c7d-Abstract.html},
	abstract = {Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.},
	urldate = {2025-06-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Du, Yilun and Mordatch, Igor},
	year = {2019},
	keywords = {zu lesen},
}

@inproceedings{donghao_moderntcn_2023,
	title = {{ModernTCN}: {A} {Modern} {Pure} {Convolution} {Structure} for {General} {Time} {Series} {Analysis}},
	shorttitle = {{ModernTCN}},
	url = {https://openreview.net/forum?id=vpJMJerXHU},
	abstract = {Recently, Transformer-based and MLP-based models have emerged rapidly and won dominance in time series analysis. In contrast, convolution is losing steam in time series tasks nowadays for inferior performance. This paper studies the open question of how to better use convolution in time series analysis and makes efforts to bring convolution back to the arena of time series analysis. To this end, we modernize the traditional TCN and conduct time series related modifications to make it more suitable for time series tasks. As the outcome, we propose ModernTCN and successfully solve this open question through a seldom-explored way in time series community. As a pure convolution structure, ModernTCN still achieves the consistent state-of-the-art performance on five mainstream time series analysis tasks while maintaining the efficiency advantage of convolution-based models, therefore providing a better balance of efficiency and performance than state-of-the-art Transformer-based and MLP-based models. Our study further reveals that, compared with previous convolution-based models, our ModernTCN has much larger effective receptive fields (ERFs), therefore can better unleash the potential of convolution in time series analysis. Code is available at this repository: https://github.com/luodhhh/ModernTCN.},
	language = {en},
	urldate = {2024-10-26},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Learning} {Representations} ({ICLR} 2024)},
	author = {Donghao, Luo and Xue, Wang},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
}

@inproceedings{donahue_adversarial_2018,
	title = {Adversarial {Audio} {Synthesis}},
	url = {https://openreview.net/forum?id=ByMVTsR5KQ},
	abstract = {Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that—without labels—WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.},
	language = {en},
	urldate = {2025-06-11},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Learning} {Representations} ({ICLR} 2019)},
	author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
	month = sep,
	year = {2018},
	keywords = {zu lesen},
}

@article{du_probabilistic_2023,
	title = {Probabilistic time series forecasting with deep non-linear state space models},
	volume = {8},
	copyright = {© 2022 The Authors. CAAI Transactions on Intelligence Technology published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology and Chongqing University of Technology.},
	issn = {2468-2322},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/cit2.12085},
	doi = {10.1049/cit2.12085},
	abstract = {Probabilistic time series forecasting aims at estimating future probabilistic distributions based on given time series observations. It is a widespread challenge in various tasks, such as risk management and decision making. To investigate temporal patterns in time series data and predict subsequent probabilities, the state space model (SSM) provides a general framework. Variants of SSM achieve considerable success in many fields, such as engineering and statistics. However, since underlying processes in real-world scenarios are usually unknown and complicated, actual time series observations are always irregular and noisy. Therefore, it is very difficult to determinate an SSM for classical statistical approaches. In this paper, a general time series forecasting framework, called Deep Non-linear State Space Model (DNLSSM), is proposed to predict the probabilistic distribution based on estimated underlying unknown processes from historical time series data. We fuse deep neural networks and statistical methods to iteratively estimate states and network parameters and thus exploit intricate temporal patterns of time series data. In particular, the unscented Kalman filter (UKF) is adopted to calculate marginal likelihoods and update distributions recursively for non-linear functions. After that, a non-linear Joseph form covariance update is developed to ensure that calculated covariance matrices in UKF updates are symmetric and positive definitive. Therefore, the authors enhance the tolerance of UKF to round-off errors and manage to combine UKF and deep neural networks. In this manner, the DNLSSM effectively models non-linear correlations between observed time series data and underlying dynamic processes. Experiments in both synthetic and real-world datasets demonstrate that the DNLSSM consistently improves the accuracy of probability forecasts compared to the baseline methods.},
	language = {en},
	number = {1},
	urldate = {2024-12-21},
	journal = {CAAI Transactions on Intelligence Technology},
	author = {Du, Heming and Du, Shouguo and Li, Wen},
	year = {2023},
	note = {https://onlinelibrary.wiley.com/doi/pdf/10.1049/cit2.12085},
	keywords = {Artificial Intelligence, machine learning, time series, zu lesen},
	pages = {3--13},
}

@inproceedings{drouin_tactis_2022,
	title = {{TACTiS}: {Transformer}-{Attentional} {Copulas} for {Time} {Series}},
	shorttitle = {{TACTiS}},
	url = {https://proceedings.mlr.press/v162/drouin22a.html},
	abstract = {The estimation of time-varying quantities is a fundamental component of decision making in fields such as healthcare and finance. However, the practical utility of such estimates is limited by how accurately they quantify predictive uncertainty. In this work, we address the problem of estimating the joint predictive distribution of high-dimensional multivariate time series. We propose a versatile method, based on the transformer architecture, that estimates joint distributions using an attention-based decoder that provably learns to mimic the properties of non-parametric copulas. The resulting model has several desirable properties: it can scale to hundreds of time series, supports both forecasting and interpolation, can handle unaligned and non-uniformly sampled data, and can seamlessly adapt to missing data during training. We demonstrate these properties empirically and show that our model produces state-of-the-art predictions on multiple real-world datasets.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Drouin, Alexandre and Marcotte, Étienne and Chapados, Nicolas},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {5447--5493},
}

@inproceedings{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
	urldate = {2025-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dhariwal, Prafulla and Nichol, Alexander},
	year = {2021},
	keywords = {zu lesen},
	pages = {8780--8794},
}

@inproceedings{doerr_probabilistic_2018,
	title = {Probabilistic {Recurrent} {State}-{Space} {Models}},
	url = {https://proceedings.mlr.press/v80/doerr18a.html},
	abstract = {State-space models (SSMs) are a highly expressive model class for learning patterns in time series data and for system identification. Deterministic versions of SSMs (e.g., LSTMs) proved extremely successful in modeling complex time series data. Fully probabilistic SSMs, however, are often found hard to train, even for smaller problems. We propose a novel model formulation and a scalable training algorithm based on doubly stochastic variational inference and Gaussian processes. This combination allows efficient incorporation of latent state temporal correlations, which we found to be key to robust training. The effectiveness of the proposed PR-SSM is evaluated on a set of real-world benchmark datasets in comparison to state-of-the-art probabilistic model learning methods. Scalability and robustness are demonstrated on a high dimensional problem.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Doerr, Andreas and Daniel, Christian and Schiegg, Martin and Duy, Nguyen-Tuong and Schaal, Stefan and Toussaint, Marc and Sebastian, Trimpe},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {1280--1289},
}

@inproceedings{dinh_density_2017,
	title = {Density estimation using {Real} {NVP}},
	url = {https://openreview.net/forum?id=HkpbnH9lx},
	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
	language = {en},
	urldate = {2025-06-05},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Learning} {Representations} ({ICLR} 2017)},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	month = feb,
	year = {2017},
	keywords = {zu lesen},
}

@inproceedings{dheur_large-scale_2023,
	title = {A {Large}-{Scale} {Study} of {Probabilistic} {Calibration} in {Neural} {Network} {Regression}},
	url = {https://proceedings.mlr.press/v202/dheur23a.html},
	abstract = {Accurate probabilistic predictions are essential for optimal decision making. While neural network miscalibration has been studied primarily in classification, we investigate this in the less-explored domain of regression. We conduct the largest empirical study to date to assess the probabilistic calibration of neural networks. We also analyze the performance of recalibration, conformal, and regularization methods to enhance probabilistic calibration. Additionally, we introduce novel differentiable recalibration and regularization methods, uncovering new insights into their effectiveness. Our findings reveal that regularization methods offer a favorable tradeoff between calibration and sharpness. Post-hoc methods exhibit superior probabilistic calibration, which we attribute to the finite-sample coverage guarantee of conformal prediction. Furthermore, we demonstrate that quantile recalibration can be considered as a specific case of conformal prediction. Our study is fully reproducible and implemented in a common code base for fair comparisons.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dheur, Victor and Taieb, Souhaib Ben},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {7813--7836},
}

@misc{dewancker_bayesian_2016,
	title = {Bayesian {Optimization} for {Machine} {Learning} : {A} {Practical} {Guidebook}},
	shorttitle = {Bayesian {Optimization} for {Machine} {Learning}},
	url = {http://arxiv.org/abs/1612.04858},
	doi = {10.48550/arXiv.1612.04858},
	abstract = {The engineering of machine learning systems is still a nascent field; relying on a seemingly daunting collection of quickly evolving tools and best practices. It is our hope that this guidebook will serve as a useful resource for machine learning practitioners looking to take advantage of Bayesian optimization techniques. We outline four example machine learning problems that can be solved using open source machine learning libraries, and highlight the benefits of using Bayesian optimization in the context of these common machine learning applications.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Dewancker, Ian and McCourt, Michael and Clark, Scott},
	month = dec,
	year = {2016},
	note = {arXiv:1612.04858},
	keywords = {Computer Science - Machine Learning, zu lesen},
}

@inproceedings{desai_keeping_2024,
	title = {Keeping {GPUs} {Cool}: {GPU} {Temperature} {Prediction} {Using} {LSTM}},
	doi = {10.1109/HiPCW63042.2024.00033},
	booktitle = {2024 {IEEE} 31st {International} {Conference} on {High} {Performance} {Computing}, {Data} and {Analytics} {Workshop} ({HiPCW})},
	author = {Desai, Tanish and Shah, Jainam and Prabhu, Gargi Alavani},
	year = {2024},
	keywords = {Accuracy, Computational modeling, Cooling Optimization, GPU Utilization, Graphics processing units, High performance computing, High-Performance Computing, Long Short-Term Memory (LSTM), Long short term memory, Predictive models, Resource management, Temperature distribution, Thermal degradation, Thermal management, zu lesen},
	pages = {111--112},
}

@book{devaney_introduction_2018,
	address = {Boca Raton},
	edition = {2},
	title = {An introduction to chaotic dynamical systems},
	isbn = {978-0-429-50230-9},
	publisher = {CRC press},
	author = {Devaney, Robert},
	month = mar,
	year = {2018},
	keywords = {zu lesen},
}

@inproceedings{deng_parsimony_2024,
	title = {Parsimony or {Capability}? {Decomposition} {Delivers} {Both} in {Long}-term {Time} {Series} {Forecasting}},
	volume = {37},
	shorttitle = {Parsimony or {Capability}?},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/7b122d0a0dcb1a86ffa25ccba154652b-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Deng, Jinliang and Ye, Feiyang and Yin, Du and Song, Xuan and Tsang, Ivor and Xiong, Hui},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {66687--66712},
}

@article{deliu_alternative_2024,
	title = {Alternative {Approaches} for {Estimating} {Highest}-{Density} {Regions}},
	issn = {0306-7734, 1751-5823},
	doi = {10.1111/insr.12592},
	abstract = {Among the variety of statistical intervals, highest-density regions (HDRs) stand out for their ability to effectively summarize a distribution or sample, unveiling its distinctive and salient features. An HDR represents the minimum size set that satisfies a certain probability coverage, and current methods for their computation require knowledge or estimation of the underlying probability distribution or density \$f\$. In this work, we illustrate a broader framework for computing HDRs, which generalizes the classical density quantile method introduced in the seminal paper of Hyndman (1996). The framework is based on neighbourhood measures, i.e., measures that preserve the order induced in the sample by \$f\$, and include the density \$f\$ as a special case. We explore a number of suitable distance-based measures, such as the \$k\$-nearest neighborhood distance, and some probabilistic variants based on copula models. An extensive comparison is provided, showing the advantages of the copula-based strategy, especially in those scenarios that exhibit complex structures (e.g., multimodalities or particular dependencies). Finally, we discuss the practical implications of our findings for estimating HDRs in real-world applications.},
	journal = {International Statistical Review},
	author = {Deliu, Nina and Liseo, Brunero},
	month = aug,
	year = {2024},
	note = {Publisher:  John Wiley \& Sons Ltd},
	keywords = {Statistics - Methodology, zu lesen},
	pages = {insr.12592},
}

@article{deisenroth_robust_2012,
	title = {Robust {Filtering} and {Smoothing} with {Gaussian} {Processes}},
	volume = {57},
	issn = {1558-2523},
	url = {https://ieeexplore.ieee.org/abstract/document/6099561},
	doi = {10.1109/TAC.2011.2179426},
	abstract = {We propose a principled algorithm for robust Bayesian filtering and smoothing in nonlinear stochastic dynamic systems when both the transition function and the measurement function are described by non-parametric Gaussian process (GP) models. GPs are gaining increasing importance in signal processing, machine learning, robotics, and control for representing unknown system functions by posterior probability distributions. This modern way of system identification is more robust than finding point estimates of a parametric function representation. Our principled filtering/smoothing approach for GP dynamic systems is based on analytic moment matching in the context of the forward-backward algorithm. Our numerical evaluations demonstrate the robustness of the proposed approach in situations where other state-of-the-art Gaussian filters and smoothers can fail.},
	number = {7},
	urldate = {2025-06-11},
	journal = {IEEE Transactions on Automatic Control},
	author = {Deisenroth, Marc Peter and Turner, Ryan Darby and Huber, Marco F. and Hanebeck, Uwe D. and Rasmussen, Carl Edward},
	month = jul,
	year = {2012},
	keywords = {Approximation methods, Bayesian inference, Covariance matrix, Gaussian processes, Noise, Robustness, Smoothing methods, Time measurement, Training, filtering, machine learning, nonlinear systems, smoothing, zu lesen},
	pages = {1865--1871},
}

@article{dawid_theory_2014,
	title = {Theory and applications of proper scoring rules},
	volume = {72},
	issn = {2281-695X},
	url = {https://doi.org/10.1007/s40300-014-0039-y},
	doi = {10.1007/s40300-014-0039-y},
	abstract = {A scoring rule \$\$S(x; q)\$\$provides a way of judging the quality of a quoted probability density \$\$q\$\$for a random variable \$\$X\$\$in the light of its outcome \$\$x\$\$. It is called proper if honesty is your best policy, i.e., when you believe \$\$X\$\$has density \$\$p\$\$, your expected score is optimised by the choice \$\$q = p\$\$. The most celebrated proper scoring rule is the logarithmic score, \$\$S(x; q) = -{\textbackslash}log \{q(x)\}\$\$: this is the only proper scoring rule that is local, in the sense of depending on the density function \$\$q\$\$only through its value at the observed value \$\$x\$\$. It is closely connected with likelihood inference, with communication theory, and with minimum description length model selection. However, every statistical decision problem induces a proper scoring rule, so there is a very wide variety of these. Many of them have additional interesting structure and properties. At a theoretical level, any proper scoring rule can be used as a foundational basis for the theory of subjective probability. At an applied level a proper scoring can be used to compare and improve probability forecasts, and, in a parametric setting, as an alternative tool for inference. In this article we give an overview of some uses of proper scoring rules in statistical inference, including frequentist estimation theory and Bayesian model selection with improper priors.},
	language = {en},
	number = {2},
	urldate = {2025-03-18},
	journal = {METRON},
	author = {Dawid, Alexander Philip and Musio, Monica},
	month = aug,
	year = {2014},
	keywords = {Bayesian model selection, Bregman score, Composite score, Homogeneous score, Hyvärinen score, Log score, Robust estimation, Survival score, zu lesen},
	pages = {169--183},
}

@article{davies_cluster_1979,
	title = {A {Cluster} {Separation} {Measure}},
	volume = {PAMI-1},
	doi = {10.1109/TPAMI.1979.4766909},
	number = {2},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Davies, David L. and Bouldin, Donald W.},
	year = {1979},
	keywords = {Algorithm design and analysis, Cluster, Clustering algorithms, Data analysis, Density measurement, Dispersion, Humans, Missiles, Multidimensional systems, Partitioning algorithms, Performance analysis, data partitions, multidimensional data analysis, parametric clustering, partitions, similarity measure, zu lesen},
	pages = {224--227},
}

@article{de_gooijer_25_2006,
	series = {Twenty five years of forecasting},
	title = {25 years of time series forecasting},
	volume = {22},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207006000021},
	doi = {10.1016/j.ijforecast.2006.01.001},
	abstract = {We review the past 25 years of research into time series forecasting. In this silver jubilee issue, we naturally highlight results published in journals managed by the International Institute of Forecasters (Journal of Forecasting 1982–1985 and International Journal of Forecasting 1985–2005). During this period, over one third of all papers published in these journals concerned time series forecasting. We also review highly influential works on time series forecasting that have been published elsewhere during this period. Enormous progress has been made in many areas, but we find that there are a large number of topics in need of further development. We conclude with comments on possible future research directions in this field.},
	number = {3},
	urldate = {2025-04-17},
	journal = {International Journal of Forecasting},
	author = {De Gooijer, Jan G. and Hyndman, Rob J.},
	month = jan,
	year = {2006},
	keywords = {ARCH, ARIMA, Accuracy measures, Combining, Count data, Densities, Exponential smoothing, Kalman filter, Long memory, Multivariate, Neural nets, Nonlinearity, Prediction intervals, Regime-switching, Robustness, Seasonality, State space, Structural models, Transfer function, Univariate, VAR, zu lesen},
	pages = {443--473},
}

@inproceedings{de_bezenac_normalizing_2020,
	title = {Normalizing {Kalman} {Filters} for {Multivariate} {Time} {Series} {Analysis}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/1f47cef5e38c952f94c5d61726027439-Abstract.html},
	abstract = {This paper tackles the modelling of large, complex and multivariate time series panels in a probabilistic setting. To this extent, we present a novel approach reconciling classical state space models with deep learning methods. By augmenting state space models with normalizing flows, we mitigate imprecisions stemming from idealized assumptions in state space models. The resulting model is highly flexible while still retaining many of the attractive properties of state space models, e.g., uncertainty and observation errors are properly accounted for, inference is tractable, sampling is efficient, good generalization performance is observed, even in low data regimes. We demonstrate competitiveness against state-of-the-art deep learning methods on the tasks of forecasting real world data and handling varying levels of missing data.},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {de Bézenac, Emmanuel and Rangapuram, Syama Sundar and Benidis, Konstantinos and Bohlke-Schneider, Michael and Kurle, Richard and Stella, Lorenzo and Hasson, Hilaf and Gallinari, Patrick and Januschowski, Tim},
	year = {2020},
	keywords = {zu lesen},
	pages = {2995--3007},
}

@inproceedings{dasgupta_optimal_2014,
	title = {Optimal rates for k-{NN} density and mode estimation},
	volume = {27},
	url = {https://papers.nips.cc/paper_files/paper/2014/hash/a5549f3f66cedf4204ffe35552e5b59c-Abstract.html},
	urldate = {2025-06-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dasgupta, Sanjoy and Kpotufe, Samory},
	year = {2014},
	keywords = {zu lesen},
}

@article{das_long-term_2023,
	title = {Long-term {Forecasting} with {TiDE}: {Time}-series {Dense} {Encoder}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=pCbC3aQB5W},
	journal = {Transactions on Machine Learning Research},
	author = {Das, Abhimanyu and Kong, Weihao and Leach, Andrew and Mathur, Shaan K. and Sen, Rajat and Yu, Rose},
	year = {2023},
	keywords = {zu lesen},
}

@inproceedings{dabney_implicit_2018,
	title = {Implicit {Quantile} {Networks} for {Distributional} {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v80/dabney18a.html},
	abstract = {In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm’s implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.},
	language = {en},
	urldate = {2025-06-06},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Remi},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {1096--1105},
}

@article{cox_prediction_1961,
	title = {Prediction by {Exponentially} {Weighted} {Moving} {Averages} and {Related} {Methods}},
	volume = {23},
	copyright = {© 1961 The Authors},
	issn = {2517-6161},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1961.tb00424.x},
	doi = {10.1111/j.2517-6161.1961.tb00424.x},
	abstract = {The mean square error of prediction is calculated for an exponentially weighted moving average (e.w.m.a.), when the series predicted is a Markov series, or a Markov series with superimposed error. The best choice of damping constant is given; the choice is not critical. There is a value of the Markov correlation ρ0 below which it is impossible to predict, with an e.w.m.a., the local variations of the series. The mean square error of an e.w.m.a. is compared with the minimum possible value, namely that for the best linear predictor (Wiener). A modified e.w.m.a. is constructed having a mean square error approaching that of the Wiener predictor. This modification will be of value if the Markov correlation parameter is negative, and possibly also when the Markov parameter is near ρ0.},
	language = {en},
	number = {2},
	urldate = {2025-04-23},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Cox, D. R.},
	year = {1961},
	note = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1961.tb00424.x},
	keywords = {zu lesen},
	pages = {414--422},
}

@article{cleveland_stl_1990,
	title = {{STL}: {A} {Seasonal}-{Trend} {Decomposition}},
	volume = {6},
	number = {1},
	journal = {Journal of Official Statistics},
	author = {Cleveland, Robert B and Cleveland, William S and McRae, Jean E and Terpenning, Irma},
	year = {1990},
	keywords = {zu lesen},
	pages = {3--73},
}

@article{cressie_fitting_1985,
	title = {Fitting variogram models by weighted least squares},
	volume = {17},
	issn = {1573-8868},
	url = {https://doi.org/10.1007/BF01032109},
	doi = {10.1007/BF01032109},
	abstract = {The method of weighted least squares is shown to be an appropriate way of fitting variogram models. The weighting scheme automatically gives most weight to early lags and down-weights those lags with a small number of pairs. Although weights are derived assuming the data are Gaussian (normal), they are shown to be still appropriate in the setting where data are a (smooth) transform of the Gaussian case. The method of (iterated) generalized least squares, which takes into account correlation between variogram estimators at different lags, offer more statistical efficiency at the price of more complexity. Weighted least squares for the robust estimator, based on square root differences, is less of a compromise.},
	language = {en},
	number = {5},
	urldate = {2025-03-25},
	journal = {Journal of the International Association for Mathematical Geology},
	author = {Cressie, Noel},
	month = jul,
	year = {1985},
	keywords = {generalized least squares, kriging, median polish, robustness, stationarity, zu lesen},
	pages = {563--586},
}

@inproceedings{cirstea_triformer_2022,
	title = {Triformer: {Triangular}, {Variable}-{Specific} {Attentions} for {Long} {Sequence} {Multivariate} {Time} {Series} {Forecasting}},
	volume = {3},
	shorttitle = {Triformer},
	url = {https://www.ijcai.org/proceedings/2022/277},
	doi = {10.24963/ijcai.2022/277},
	abstract = {Electronic proceedings of IJCAI 2022},
	language = {en},
	urldate = {2025-01-18},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Cirstea, Razvan-Gabriel and Guo, Chenjuan and Yang, Bin and Kieu, Tung and Dong, Xuanyi and Pan, Shirui},
	month = jul,
	year = {2022},
	note = {ISSN: 1045-0823},
	keywords = {zu lesen},
	pages = {1994--2001},
}

@article{christoffersen_evaluating_1998,
	title = {Evaluating {Interval} {Forecasts}},
	volume = {39},
	issn = {0020-6598},
	url = {https://www.jstor.org/stable/2527341},
	doi = {10.2307/2527341},
	abstract = {A complete theory for evaluating interval forecasts has not been worked out to date. Most of the literature implicitly assumes homoskedastic errors even when this is clearly violated, and proceed by merely testing for correct unconditional coverage. Consequently, I set out to build a consistent framework for conditional interval forecast evaluation, which is crucial when higher-order moment dynamics are present. The new methodology is demonstrated in an application to the exchange rate forecasting procedures advocated in risk management.},
	number = {4},
	urldate = {2025-05-30},
	journal = {International Economic Review},
	author = {Christoffersen, Peter F.},
	year = {1998},
	note = {Publisher: [Economics Department of the University of Pennsylvania, Wiley, Institute of Social and Economic Research, Osaka University]},
	keywords = {zu lesen},
	pages = {841--862},
}

@incollection{chu_iterative_2002,
	series = {Proceedings},
	title = {Iterative {Deepening} {Dynamic} {Time} {Warping} for {Time} {Series}},
	isbn = {978-0-89871-517-0},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611972726.12},
	abstract = {1 Introduction
Time series are a ubiquitous form of data occurring in virtually every scientific discipline and business application. There has been much recent work on adapting data mining algorithms to time series databases. For example, Das et al. attempt to show how association rules can be learned from time series [7]. Debregeas and Hebrail [8] demonstrate a technique for scaling up time series clustering algorithms to massive datasets. Keogh and Pazzani introduced a new, scalable time series classification algorithm [16]. Almost all algorithms that operate on time series data need to compute the similarity between them. Euclidean distance, or some extension or modification thereof, is typically used. However as we will demonstrate in Section 2.1, Euclidean distance can be an extremely brittle distance measure.},
	urldate = {2025-07-01},
	booktitle = {Proceedings of the 2002 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Chu, Selina and Keogh, Eamonn and Hart, David and Pazzani, Michael},
	month = apr,
	year = {2002},
	doi = {10.1137/1.9781611972726.12},
	keywords = {zu lesen},
	pages = {195--212},
}

@inproceedings{cho_learning_2014,
	address = {Doha, Qatar},
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://aclweb.org/anthology/D/D14/D14-1179.pdf},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Kyunghyun and Merrienboer, Bart van and Gülçehre, Çaglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	year = {2014},
	keywords = {zu lesen},
	pages = {1724--1734},
}

@article{chevillon_non-parametric_2005,
	title = {Non-parametric direct multi-step estimation for forecasting economic processes},
	volume = {21},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S016920700400069X},
	doi = {10.1016/j.ijforecast.2004.08.004},
	abstract = {We evaluate the asymptotic and finite-sample properties of direct multi-step estimation (DMS) for forecasting at several horizons. For forecast accuracy gains from DMS in finite samples, mis-specification and non-stationarity of the DGP are necessary, but when a model is well-specified, iterating the one-step ahead forecasts may not be asymptotically preferable. If a model is mis-specified for a non-stationary DGP, in particular omitting either negative residual serial correlation or regime shifts, DMS can forecast more accurately. Monte Carlo simulations clarify the nonlinear dependence of the estimation and forecast biases on the parameters of the DGP, and explain existing results.},
	number = {2},
	urldate = {2025-04-18},
	journal = {International Journal of Forecasting},
	author = {Chevillon, Guillaume and Hendry, David F.},
	month = apr,
	year = {2005},
	keywords = {Adaptive estimation, Dynamic forecasts, Model mis-specification, Multi-step estimation, zu lesen},
	pages = {201--218},
}

@inproceedings{chen_sdformer_2024,
	title = {{SDformer}: {Similarity}-driven {Discrete} {Transformer} {For} {Time} {Series} {Generation}},
	volume = {37},
	shorttitle = {{SDformer}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/ee6c4b99b4c0d3d60efd22c1ecdd9891-Abstract-Conference.html},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Zhicheng and Feng, Shibo and Zhang, Zhong and Xiao, Xi and Gao, Xingyu and Zhao, Peilin},
	month = dec,
	year = {2024},
	keywords = {zu lesen},
	pages = {132179--132207},
}

@article{chevillon_direct_2007,
	title = {Direct {Multi}-{Step} {Estimation} and {Forecasting}},
	volume = {21},
	issn = {1467-6419},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-6419.2007.00518.x},
	doi = {10.1111/j.1467-6419.2007.00518.x},
	abstract = {This paper surveys the literature on multi-step forecasting when the model or the estimation method focuses directly on the link between the forecast origin and the horizon of interest. Among diverse contributions, we show how the current consensual concepts have emerged. We present an exhaustive overview of the existing results, including a conclusive review of the circumstances favourable to direct multi-step forecasting, namely different forms of non-stationarity and appropriate model design. We also provide a unifying framework which allows us to analyse the sources of forecast errors and hence of accuracy improvements from direct over iterated multi-step forecasting.},
	language = {en},
	number = {4},
	urldate = {2025-04-18},
	journal = {Journal of Economic Surveys},
	author = {Chevillon, Guillaume},
	year = {2007},
	note = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-6419.2007.00518.x},
	keywords = {Adaptive estimation, Multi-step forecasting, Non-stationarity, Structural breaks, Varying horizon, zu lesen},
	pages = {746--785},
}

@article{chen_evaluating_2022,
	title = {Evaluating quantile forecasts in the {M5} uncertainty competition},
	volume = {38},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207022000449},
	doi = {https://doi.org/10.1016/j.ijforecast.2022.03.004},
	abstract = {Probabilistic forecasts are necessary for robust decisions in the face of uncertainty. The M5 Uncertainty competition required participating teams to forecast nine quantiles for unit sales of various products at various aggregation levels and for different time horizons. This paper evaluates the forecasting performance of the quantile forecasts at different aggregation levels and at different quantile levels. We contrast this with some theoretical predictions, and discuss potential implications and promising future research directions for the practice of probabilistic forecasting.},
	number = {4},
	journal = {International Journal of Forecasting},
	author = {Chen, Zhi and Gaba, Anil and Tsetlin, Ilia and Winkler, Robert L.},
	year = {2022},
	keywords = {M competitions, Probabilistic forecasts, Quantile forecasts, Retail sales forecasting, Uncertainty, zu lesen},
	pages = {1531--1545},
}

@article{chen_tsmixer_2023,
	title = {{TSMixer}: {An} {All}-{MLP} {Architecture} for {Time} {Series} {Forecasting}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=wbpxTuXgm0},
	journal = {Transactions on Machine Learning Research},
	author = {Chen, Si-An and Li, Chun-Liang and Arik, Sercan O. and Yoder, Nathanael Christian and Pfister, Tomas},
	year = {2023},
	keywords = {zu lesen},
}

@inproceedings{chen_pathformer_2023,
	title = {Pathformer: {Multi}-scale {Transformers} with {Adaptive} {Pathways} for {Time} {Series} {Forecasting}},
	shorttitle = {Pathformer},
	url = {https://openreview.net/forum?id=lJkOCMP2aW},
	abstract = {Transformers for time series forecasting mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. We propose Pathformer, a multi-scale Transformer with adaptive pathways. It integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale Transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics of the input, improving the accuracy and generalization of Pathformer. Extensive experiments on eleven real-world datasets demonstrate that Pathformer not only achieves state-of-the-art performance by surpassing all current models but also exhibits stronger generalization abilities under various transfer scenarios. The code is made available at https://github.com/decisionintelligence/pathformer.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Chen, Peng and Zhang, Yingying and Cheng, Yunyao and Shu, Yang and Wang, Yihang and Wen, Qingsong and Yang, Bin and Guo, Chenjuan},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
}

@article{chen_multiple_2024,
	title = {A multiple kernel-based kernel density estimator for multimodal probability density functions},
	volume = {132},
	issn = {0952-1976},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197624001374},
	doi = {10.1016/j.engappai.2024.107979},
	abstract = {The performance of the single kernel-based kernel density estimator (SK-KDE) in fitting a unimodal probability density function (PDF) depends on the choice of kernel function and the corresponding selection of kernel bandwidth. Unlike unimodal PDFs, a multimodal PDF has several distinct features. First, it has multiple local maxima. Second, it is composed of various unimodal PDFs. Each of these unimodal PDFs corresponds to a different collection of random variables. Importantly, these variables are not independent and identically distributed. Because of the difficulty in quantifying multimodality among different modes, it is extremely difficult to select an appropriate kernel function and optimal kernel bandwidth for the multimodal PDF. Multimodal PDFs are frequently encountered in real-world applications. To address this, this paper proposes a novel multiple kernel-based kernel density estimator (MK-KDE). It constructs a flexible KDE by using the weighted average of multiple kernels with consideration of their kernel efficiencies. By integrating multiple kernels, MK-KDE leverages their complementary strengths to enhance the estimation of complex and multimodal PDFs. To achieve this, a novel efficient objective function is designed to obtain the optimized kernel weights and kernel bandwidths by minimizing both the global estimation error of MK-KDE and the local estimation errors of SK-KDEs. Moreover, a sophisticated k-nearest neighbor strategy is devised as a heuristic method to determine the unknown PDF values of given data points, thereby optimizing the aforementioned objective function. A series of extensive experiments was conducted to validate the feasibility, rationality, and effectiveness of MK-KDE for 10 multimodal PDFs. The experimental results show that (1) the kernel weights and bandwidths of MK-KDE converge as the iteration number of the optimization algorithm increases; (2) MK-KDE can fit multimodal PDFs by automatically selecting the kernel functions and bandwidths; and (3) MK-KDE obtains lower estimation errors on 10 multimodal PDFs in comparison to 10 existing PDF estimation methods, demonstrating that MK-KDE is a viable approach to estimate multimodal PDFs.},
	urldate = {2025-06-29},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Chen, Jia-Qi and He, Yu-Lin and Cheng, Ying-Chao and Fournier-Viger, Philippe and Huang, Joshua Zhexue},
	month = jun,
	year = {2024},
	keywords = {Heuristic -nearest neighbor strategy, Kernel bandwidth, Kernel density estimator, Kernel function, Multimodal probability density function, zu lesen},
	pages = {107979},
}

@inproceedings{chen_calibration_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Calibration of {Time}-{Series} {Forecasting}: {Detecting} and {Adapting} {Context}-{Driven} {Distribution} {Shift}},
	isbn = {979-8-4007-0490-1},
	shorttitle = {Calibration of {Time}-{Series} {Forecasting}},
	url = {https://dl.acm.org/doi/10.1145/3637528.3671926},
	doi = {10.1145/3637528.3671926},
	abstract = {Recent years have witnessed the success of introducing deep learning models to time series forecasting. From a data generation perspective, we illustrate that existing models are susceptible to distribution shifts driven by temporal contexts, whether observed or unobserved. Such context-driven distribution shift (CDS) introduces biases in predictions within specific contexts and poses challenges for conventional training paradigms. In this paper, we introduce a universal calibration methodology for the detection and adaptation of CDS with a trained model. To this end, we propose a novel CDS detector, termed the "residual-based CDS detector" or "Reconditionor", which quantifies the model's vulnerability to CDS by evaluating the mutual information between prediction residuals and their corresponding contexts. A high Reconditionor score indicates a severe susceptibility, thereby necessitating model adaptation. In this circumstance, we put forth a straightforward yet potent adapter framework for model calibration, termed the "sample-level contextualized adapter" or "SOLID". This framework involves the curation of a contextually similar dataset to the provided test sample and the subsequent fine-tuning of the model's prediction layer with a limited number of steps. Our theoretical analysis demonstrates that this adaptation strategy can achieve an optimal bias-variance trade-off. Notably, our proposed Reconditionor and SOLID are model-agnostic and readily adaptable to a wide range of models. Extensive experiments show that SOLID consistently enhances the performance of current forecasting models on real-world datasets, especially on cases with substantial CDS detected by the proposed Reconditionor, thus validating the effectiveness of the calibration approach.},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Mouxiang and Shen, Lefei and Fu, Han and Li, Zhuo and Sun, Jianling and Liu, Chenghao},
	month = aug,
	year = {2024},
	keywords = {zu lesen},
	pages = {341--352},
}

@inproceedings{challu_nhits_2023,
	title = {{NHITS}: {Neural} {Hierarchical} {Interpolation} for {Time} {Series} {Forecasting}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	shorttitle = {{NHITS}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/25854},
	doi = {10.1609/aaai.v37i6.25854},
	abstract = {Recent progress in neural forecasting accelerated improvements in the performance of large-scale forecasting systems. Yet, long-horizon forecasting remains a very difficult task. Two common challenges afflicting the task are the volatility of the predictions and their computational complexity. We introduce NHITS, a model which addresses both challenges by incorporating novel hierarchical interpolation and multi-rate data sampling techniques. These techniques enable the proposed method to assemble its predictions sequentially, emphasizing components with different frequencies and scales while decomposing the input signal and synthesizing the forecast. We prove that the hierarchical interpolation technique can efficiently approximate arbitrarily long horizons in the presence of smoothness. Additionally, we conduct extensive large-scale dataset experiments from the long-horizon forecasting literature, demonstrating the advantages of our method over the state-of-the-art methods, where NHITS provides an average accuracy improvement of almost 20\% over the latest Transformer architectures while reducing the computation time by an order of magnitude (50 times). Our code is available at https://github.com/Nixtla/neuralforecast.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Challu, Cristian and Olivares, Kin G. and Oreshkin, Boris N. and Ramirez, Federico Garza and Canseco, Max Mergenthaler and Dubrawski, Artur},
	month = jun,
	year = {2023},
	keywords = {ML: Deep Neural Architectures, zu lesen},
	pages = {6989--6997},
}

@article{cao_gpu_2017,
	title = {A {GPU} {Heterogeneous} {Cluster} {Scheduling} {Model} for {Preventing} {Temperature} {Heat} {Island}},
	volume = {11},
	doi = {10.1051/itmconf/20171107003},
	journal = {ITM Web of Conferences},
	author = {Cao, Yun-Peng and Wang, Hai-Feng},
	month = jan,
	year = {2017},
	keywords = {zu lesen},
	pages = {07003},
}

@inproceedings{cai_msgnet_2024,
	title = {{MSGNet}: {Learning} {Multi}-{Scale} {Inter}-series {Correlations} for {Multivariate} {Time} {Series} {Forecasting}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	shorttitle = {{MSGNet}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/28991},
	doi = {10.1609/aaai.v38i10.28991},
	abstract = {Multivariate time series forecasting poses an ongoing challenge across various disciplines. Time series data often exhibit diverse intra-series and inter-series correlations, contributing to intricate and interwoven dependencies that have been the focus of numerous studies. Nevertheless, a significant research gap remains in comprehending the varying inter-series correlations across different time scales among multiple time series, an area that has received limited attention in the literature. To bridge this gap, this paper introduces MSGNet, an advanced deep learning model designed to capture the varying inter-series correlations across multiple time scales using frequency domain analysis and adaptive graph convolution. By leveraging frequency domain analysis, MSGNet effectively extracts salient periodic patterns and decomposes the time series into distinct time scales. The model incorporates a self-attention mechanism to capture intra-series dependencies, while introducing an adaptive mixhop graph convolution layer to autonomously learn diverse inter-series correlations within each time scale. Extensive experiments are conducted on several real-world datasets to showcase the effectiveness of MSGNet. Furthermore, MSGNet possesses the ability to automatically learn explainable multi-scale inter-series correlations, exhibiting strong generalization capabilities even when applied to out-of-distribution samples.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Cai, Wanlin and Liang, Yuxuan and Liu, Xianggen and Feng, Jianshuai and Wu, Yuankai},
	month = mar,
	year = {2024},
	keywords = {ML: Applications, zu lesen},
	pages = {11141--11149},
}

@inproceedings{brown_language_2020,
	address = {Vancouver, Canada},
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	keywords = {zu lesen},
	pages = {1877--1901},
}

@article{bracher_evaluating_2021,
	title = {Evaluating epidemic forecasts in an interval format},
	volume = {17},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618},
	doi = {10.1371/journal.pcbi.1008618},
	abstract = {For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.},
	language = {en},
	number = {2},
	urldate = {2025-03-18},
	journal = {PLOS Computational Biology},
	author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
	month = dec,
	year = {2021},
	note = {Publisher: Public Library of Science},
	keywords = {Binomials, COVID 19, Epidemiological methods and statistics, Forecasting, Instrument calibration, Pandemics, Probability distribution, Public and occupational health, zu lesen},
	pages = {e1008618},
}

@article{cao_yun-peng_gpu_2017,
	title = {A {GPU} {Heterogeneous} {Cluster} {Scheduling} {Model} for {Preventing} {Temperature} {Heat} {Island}},
	volume = {11},
	url = {https://doi.org/10.1051/itmconf/20171107003},
	doi = {10.1051/itmconf/20171107003},
	journal = {ITM Web Conf.},
	author = {{Cao, Yun-Peng} and {Wang, Hai-Feng}},
	year = {2017},
	keywords = {zu lesen},
	pages = {07003},
}

@book{box_time_2015,
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Time series analysis: forecasting and control},
	isbn = {978-1-118-67492-5},
	publisher = {John Wiley \& Sons},
	author = {Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
	year = {2015},
	lccn = {2015016616},
	keywords = {zu lesen},
}

@article{bothwell_pattern-based_2022,
	title = {Pattern-{Based} {Clustering} of {Daily} {Weigh}-{In} {Trajectories} {Using} {Dynamic} {Time} {Warping}},
	volume = {79},
	issn = {0006-341X},
	url = {https://doi.org/10.1111/biom.13773},
	doi = {10.1111/biom.13773},
	abstract = {“Smart”-scales are a new tool for frequent monitoring of weight change as well as weigh-in behavior. These scales give researchers the opportunity to discover patterns in the frequency that individuals weigh themselves over time, and how these patterns are associated with overall weight loss. Our motivating data come from an 18-month behavioral weight loss study of 55 adults classified as overweight or obese who were instructed to weigh themselves daily. Adherence to daily weigh-in routines produces a binary times series for each subject, indicating whether a participant weighed in on a given day. To characterize weigh-in by time-invariant patterns rather than overall adherence, we propose using hierarchical clustering with dynamic time warping (DTW). We perform an extensive simulation study to evaluate the performance of DTW compared to Euclidean and Jaccard distances to recover underlying patterns in adherence time series. In addition, we compare cluster performance using cluster validation indices (CVIs) under the single, average, complete, and Ward linkages and evaluate how internal and external CVIs compare for clustering binary time series. We apply conclusions from the simulation to cluster our real data and summarize observed weigh-in patterns. Our analysis finds that the adherence trajectory pattern is significantly associated with weight loss.},
	number = {3},
	journal = {Biometrics},
	author = {Bothwell, Samantha and Kaizer, Alex and Peterson, Ryan and Ostendorf, Danielle and Catenacci, Victoria and Wrobel, Julia},
	month = oct,
	year = {2022},
	note = {https://academic.oup.com/biometrics/article-pdf/79/3/2719/56040291/biometrics\_79\_3\_2719.pdf},
	keywords = {zu lesen},
	pages = {2719--2731},
}

@misc{borovykh_conditional_2018,
	title = {Conditional {Time} {Series} {Forecasting} with {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.04691},
	doi = {10.48550/arXiv.1703.04691},
	abstract = {We present a method for conditional time series forecasting based on an adaptation of the recent deep convolutional WaveNet architecture. The proposed network contains stacks of dilated convolutions that allow it to access a broad range of history when forecasting, a ReLU activation function and conditioning is performed by applying multiple convolutional filters in parallel to separate time series which allows for the fast processing of data and the exploitation of the correlation structure between the multivariate time series. We test and analyze the performance of the convolutional network both unconditionally as well as conditionally for financial time series forecasting using the S\&P500, the volatility index, the CBOE interest rate and several exchange rates and extensively compare it to the performance of the well-known autoregressive model and a long-short term memory network. We show that a convolutional network is well-suited for regression-type problems and is able to effectively learn dependencies in and between the series without the need for long historical time series, is a time-efficient and easy to implement alternative to recurrent-type networks and tends to outperform linear and recurrent models.},
	urldate = {2025-04-17},
	publisher = {arXiv},
	author = {Borovykh, Anastasia and Bohte, Sander and Oosterlee, Cornelis W.},
	month = sep,
	year = {2018},
	note = {arXiv:1703.04691},
	keywords = {Statistics - Machine Learning, zu lesen},
}

@incollection{bontempi_machine_2013,
	address = {Berlin, Heidelberg},
	title = {Machine {Learning} {Strategies} for {Time} {Series} {Forecasting}},
	isbn = {978-3-642-36318-4},
	url = {https://doi.org/10.1007/978-3-642-36318-4_3},
	abstract = {The increasing availability of large amounts of historical data and the need of performing accurate forecasting of future behavior in several scientific and applied domains demands the definition of robust and efficient techniques able to infer from observations the stochastic dependency between past and future. The forecasting domain has been influenced, from the 1960s on, by linear statistical methods such as ARIMA models. More recently, machine learning models have drawn attention and have established themselves as serious contenders to classical statistical models in the forecasting community. This chapter presents an overview of machine learning techniques in time series forecasting by focusing on three aspects: the formalization of one-step forecasting problems as supervised learning tasks, the discussion of local learning techniques as an effective tool for dealing with temporal data and the role of the forecasting strategy when we move from one-step to multiple-step forecasting.},
	language = {en},
	urldate = {2025-04-18},
	booktitle = {Business {Intelligence}: {Second} {European} {Summer} {School}, {eBISS} 2012, {Brussels}, {Belgium}, {July} 15-21, 2012, {Tutorial} {Lectures}},
	publisher = {Springer},
	author = {Bontempi, Gianluca and Ben Taieb, Souhaib and Le Borgne, Yann-Aël},
	editor = {Aufaure, Marie-Aude and Zimányi, Esteban},
	year = {2013},
	doi = {10.1007/978-3-642-36318-4_3},
	keywords = {MIMO, Time series forecasting, lazy learning, local learning, machine learning, zu lesen},
	pages = {62--77},
}

@article{box_distribution_1970,
	title = {Distribution of {Residual} {Autocorrelations} in {Autoregressive}-{Integrated} {Moving} {Average} {Time} {Series} {Models}},
	volume = {65},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1970.10481180},
	doi = {10.1080/01621459.1970.10481180},
	abstract = {Many statistical models, and in particular autoregressive—moving average time series models, can be regarded as means of transforming the data to white noise, that is, to an uncorrected sequence of errors. If the parameters are known exactly, this random sequence can be computed directly from the observations; when this calculation is made with estimates substituted for the true parameter values, the resulting sequence is referred to as the “residuals,” which can be regarded as estimates of the errors. If the appropriate model has been chosen, there will be zero autocorrelation in the errors. In checking adequacy of fit it is therefore logical to study the sample autocorrelation function of the residuals. For large samples the residuals from a correctly fitted model resemble very closely the true errors of the process; however, care is needed in interpreting the serial correlations of the residuals. It is shown here that the residual autocorrelations are to a close approximation representable as a singular linear transformation of the autocorrelations of the errors so that they possess a singular normal distribution. Failing to allow for this results in a tendency to overlook evidence of lack of fit. Tests of fit and diagnostic checks are devised which take these facts into account.},
	number = {332},
	urldate = {2024-05-24},
	journal = {Journal of the American Statistical Association},
	author = {Box, G. E. P. and Pierce, David A.},
	month = dec,
	year = {1970},
	keywords = {zu lesen},
	pages = {1509--1526},
}

@book{bishop_pattern_2019,
	address = {New York, NY},
	series = {Information {Science} and {Statistics}},
	title = {Pattern recognition and machine learning},
	url = {https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf},
	abstract = {Probability Distributions -- Linear Models for Regression -- Linear Models for Classification -- Neural Networks -- Kernel Methods -- Sparse Kernel Machines -- Graphical Models -- Mixture Models and EM -- Approximate Inference -- Sampling Methods -- Continuous Latent Variables -- Sequential Data -- Combining Models},
	language = {eng},
	urldate = {2025-06-09},
	publisher = {Springer Science+Business Media, LLC},
	author = {Bishop, Christopher M.},
	year = {2019},
	note = {OCLC: 1334664824},
	keywords = {Artificial Intelligence (incl. Robotics), Artificial intelligence, Computer science, Image Processing and Computer Vision, Informatique, Intelligence artificielle, Lehrbuch, Machine learning, Maschinelles Lernen, Mustererkennung, Optical data processing, Pattern perception, Pattern recognition, Perception des structures, Statistics, Statistics for Engineering, Physics, Computer Science, Chemistry \& Geosciences, Statistique, Traitement optique de l'information, artificial intelligence, statistics, zu lesen},
}

@misc{biewald_experiment_2020,
	title = {Experiment {Tracking} with {Weights} and {Biases}},
	url = {https://www.wandb.com/},
	author = {Biewald, Lukas},
	year = {2020},
	note = {URL: https://www.wandb.com/},
	keywords = {zu lesen},
}

@inproceedings{bergsma_c2far_2022,
	title = {{C2FAR}: {Coarse}-to-{Fine} {Autoregressive} {Networks} for {Precise} {Probabilistic} {Forecasting}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/899511e37a8e01e1bd6f6f1d377cc250-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bergsma, Shane and Zeyl, Tim and Rahimipour Anaraki, Javad and Guo, Lei},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	keywords = {zu lesen},
	pages = {21900--21915},
}

@inproceedings{bergsma_sutranets_2023,
	title = {{SutraNets}: {Sub}-series {Autoregressive} {Networks} for {Long}-{Sequence}, {Probabilistic} {Forecasting}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6171c9e600432a42688ad61a525951bf-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bergsma, Shane and Zeyl, Tim and Guo, Lei},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	keywords = {zu lesen},
	pages = {30518--30533},
}

@article{benidis_deep_2022,
	title = {Deep {Learning} for {Time} {Series} {Forecasting}: {Tutorial} and {Literature} {Survey}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3533382},
	doi = {10.1145/3533382},
	abstract = {Deep learning based forecasting methods have become the methods of choice in many applications of time series prediction or forecasting often outperforming other approaches. Consequently, over the last years, these methods are now ubiquitous in large-scale industrial forecasting applications and have consistently ranked among the best entries in forecasting competitions (e.g., M4 and M5). This practical success has further increased the academic interest to understand and improve deep forecasting methods. In this article we provide an introduction and overview of the field: We present important building blocks for deep forecasting in some depth; using these building blocks, we then survey the breadth of the recent deep forecasting literature.},
	number = {6},
	journal = {ACM Computing Surveys},
	author = {Benidis, Konstantinos and Rangapuram, Syama Sundar and Flunkert, Valentin and Wang, Yuyang and Maddix, Danielle and Turkmen, Caner and Gasthaus, Jan and Bohlke-Schneider, Michael and Salinas, David and Stella, Lorenzo and Aubet, François-Xavier and Callot, Laurent and Januschowski, Tim},
	month = dec,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Time series, forecasting, neural networks, zu lesen},
}

@article{bernardo_expected_1979,
	title = {Expected {Information} as {Expected} {Utility}},
	volume = {7},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-3/Expected-Information-as-Expected-Utility/10.1214/aos/1176344689.full},
	doi = {10.1214/aos/1176344689},
	abstract = {The normative procedure for the design of an experiment is to select a utility function, assess the probabilities, and to choose that design of maximum expected utility. One difficulty with this view is that a scientist typically does not have, nor can be normally expected to have, a clear idea of the utility of his results. An alternative is to design an experiment to maximize the expected information to be gained from it. In this paper we show that the latter view is a special case of the former with an appropriate choice of the decision space and a reasonable constraint on the utility function. In particular, the Shannon concept of information is seen to play a more important role in experimental design than was hitherto thought possible.},
	number = {3},
	urldate = {2025-03-18},
	journal = {The Annals of Statistics},
	author = {Bernardo, Jose M.},
	month = may,
	year = {1979},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62A15, 62B10, 62B15, Bayesian statistics, Design of experiments, Information, Utility, decision theory, scientific inference, zu lesen},
	pages = {686--690},
}

@article{anh_efficient_2015,
	title = {An efficient implementation of k-means clustering for time series data with {DTW} distance},
	volume = {10},
	url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJBIDM.2015.071311},
	doi = {10.1504/IJBIDM.2015.071311},
	abstract = {Time series clustering is one of the crucial tasks in time series data mining. The most popular method in time series clustering is k-means algorithm due to its simplicity and flexibility. So far, k-means for time series clustering has been most used with Euclidean distance. Dynamic time warping (DTW) distance measure has increasingly been used as a similarity measurement for various data mining tasks in place of traditional Euclidean distance due to its superiority in sequence-alignment flexibility. However, there exist some difficulties in clustering with DTW distance, for example, the problem of shape averaging in DTW or the problem of speeding up DTW distance calculation. In this paper, we compare the performance of the three shape averaging methods in DTW: nonlinear alignment and averaging filter (NLAAF), prioritised shape averaging (PSA) and DTW barycenter averaging (DBA) and propose an efficient method to implement k-means clustering for time series data with DTW distance. In our method, we choose to use DBA method for shape-based time series averaging, apply early abandoning method for speeding up DTW distance calculation and median-based method for determining initial centroids for k-means clustering. The experimental results on benchmark datasets validate our proposed implementation method for time series k-means clustering with DTW.},
	number = {3},
	journal = {International Journal of Business Intelligence and Data Mining},
	author = {Anh, Duong Tuan and Thanh, Le Huu},
	year = {2015},
	keywords = {zu lesen},
	pages = {213--232},
}

@inproceedings{amos_input_2017,
	title = {Input {Convex} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v70/amos17b.html},
	abstract = {This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.},
	language = {en},
	urldate = {2025-06-06},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Amos, Brandon and Xu, Lei and Kolter, J. Zico},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	keywords = {zu lesen},
	pages = {146--155},
}

@article{bengio_learning_1994,
	title = {Learning long-term dependencies with gradient descent is difficult},
	volume = {5},
	issn = {1941-0093},
	url = {https://ieeexplore.ieee.org/document/279181},
	doi = {10.1109/72.279181},
	abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.{\textless}{\textgreater}},
	number = {2},
	urldate = {2025-04-16},
	journal = {IEEE Transactions on Neural Networks},
	author = {Bengio, Y. and Simard, P. and Frasconi, P.},
	month = mar,
	year = {1994},
	keywords = {Computer networks, Cost function, Delay effects, Discrete transforms, Displays, Intelligent networks, Neural networks, Neurofeedback, Production, Recurrent neural networks, zu lesen},
	pages = {157--166},
}

@misc{bai_empirical_2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	doi = {10.48550/arXiv.1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
	urldate = {2025-04-17},
	publisher = {arXiv},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = apr,
	year = {2018},
	note = {arXiv:1803.01271},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, zu lesen},
}

@inproceedings{ashok_tactis-2_2023,
	title = {{TACTiS}-2: {Better}, {Faster}, {Simpler} {Attentional} {Copulas} for {Multivariate} {Time} {Series}},
	shorttitle = {{TACTiS}-2},
	url = {https://openreview.net/forum?id=xtOydkE1Ku},
	abstract = {We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series. Code is made available at https://github.com/ServiceNow/TACTiS.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Ashok, Arjun and Marcotte, Étienne and Zantedeschi, Valentina and Chapados, Nicolas and Drouin, Alexandre},
	month = oct,
	year = {2023},
	keywords = {zu lesen},
}

@article{almeida_ggplot2_2018,
	title = {ggplot2 {Compatible} {Quantile}-{Quantile} {Plots} in {R}},
	url = {https://digitalcommons.unl.edu/r-journal/344},
	journal = {The R Journal},
	author = {Almeida, Alexandre and Loy, Adam and Hofmann, Heike},
	month = dec,
	year = {2018},
	keywords = {zu lesen},
}

@article{alexandrov_gluonts_2020,
	title = {{GluonTS}: {Probabilistic} and {Neural} {Time} {Series} {Modeling} in {Python}},
	volume = {21},
	issn = {1533-7928},
	shorttitle = {{GluonTS}},
	url = {http://jmlr.org/papers/v21/19-820.html},
	abstract = {We introduce the Gluon Time Series Toolkit (GluonTS), a Python library for deep learning based time series modeling for ubiquitous tasks, such as forecasting and anomaly detection. GluonTS simplifies the time series modeling pipeline by providing the necessary components and tools for quick model development, efficient experimentation and evaluation. In addition, it contains reference implementations of state-of-the-art time series models that enable simple benchmarking of new algorithms.},
	number = {116},
	urldate = {2025-06-05},
	journal = {Journal of Machine Learning Research},
	author = {Alexandrov, Alexander and Benidis, Konstantinos and Bohlke-Schneider, Michael and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Maddix, Danielle C. and Rangapuram, Syama and Salinas, David and Schulz, Jasper and Stella, Lorenzo and Türkmen, Ali Caner and Wang, Yuyang},
	year = {2020},
	keywords = {zu lesen},
	pages = {1--6},
}

@article{alcaraz_diffusion-based_2022,
	title = {Diffusion-based {Time} {Series} {Imputation} and {Forecasting} with {Structured} {State} {Space} {Models}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=hHiIbk7ApW},
	abstract = {The imputation of missing values represents a significant obstacle for many real-world data analysis pipelines. Here, we focus on time series data and put forward SSSD, an imputation model that relies on two emerging technologies, (conditional) diffusion models as state-ofthe-art generative models and structured state space models as internal model architecture, which are particularly suited to capture long-term dependencies in time series data. We demonstrate that SSSD matches or even exceeds state-of-the-art probabilistic imputation and forecasting performance on a broad range of data sets and different missingness scenarios, including the challenging blackout-missing scenarios, where prior approaches failed to provide meaningful results.},
	language = {en},
	urldate = {2025-01-18},
	journal = {Transactions on Machine Learning Research},
	author = {Alcaraz, Juan Lopez and Strodthoff, Nils},
	month = dec,
	year = {2022},
	keywords = {zu lesen},
}

@article{alexander_evaluating_2024,
	title = {Evaluating the discrimination ability of proper multi-variate scoring rules},
	volume = {334},
	issn = {1572-9338},
	url = {https://doi.org/10.1007/s10479-022-04611-9},
	doi = {10.1007/s10479-022-04611-9},
	abstract = {Proper scoring rules are commonly applied to quantify the accuracy of distribution forecasts. Given an observation they assign a scalar score to each distribution forecast, with the lowest expected score attributed to the true distribution. The energy and variogram scores are two rules that have recently gained some popularity in multivariate settings because their computation does not require a forecast to have parametric density function and so they are broadly applicable. Here we conduct a simulation study to compare the discrimination ability between the energy score and three variogram scores. Compared with other studies, our simulation design is more realistic because it is supported by a historical data set containing commodity prices, currencies and interest rates, and our data generating processes include a diverse selection of models with different marginal distributions, dependence structure, and calibration windows. This facilitates a comprehensive comparison of the performance of proper scoring rules in different settings. To compare the scores we use three metrics: the mean relative score, error rate and a generalized discrimination heuristic. Overall, we find that the variogram score with parameter \$\$p=0.5\$\$outperforms the energy score and the other two variogram scores.},
	language = {en},
	number = {1},
	urldate = {2025-03-20},
	journal = {Annals of Operations Research},
	author = {Alexander, C. and Coulon, M. and Han, Y. and Meng, X.},
	month = mar,
	year = {2024},
	keywords = {Discrimination heuristic, Energy score, Multivariate forecasting, Proper scoring rules, Variogram score, zu lesen},
	pages = {857--883},
}
